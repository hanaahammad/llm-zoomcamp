{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c50d34-d5bf-477d-8a31-78a528b91f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1830"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cfd91ec-8ec8-4570-bff7-228c5eb7327b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx = {d['id']: d for d in documents}\n",
    "doc_idx['5170565b']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8f7ec2-5528-4ce5-9205-4eed8b8124f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b32ba5-d80d-4ad1-830c-5d90fe25750e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'What if I miss a session?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'id': '5170565b'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx['5170565b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e6678220-4686-41d2-9d6b-878498728cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, sessions are recorded so you won't miss anything.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(ground_truth[10], model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfdc6e3c-a9d4-4ae8-8379-c9c90efc23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "pool = ThreadPoolExecutor(max_workers=3)\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "\n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9c725d15-c355-4978-b598-81526fe298ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(rec):\n",
    "    model = 'gpt-3.5-turbo'\n",
    "    answer_llm = rag(rec, model=model)\n",
    "    \n",
    "    doc_id = rec['document']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    answer_orig = original_doc['text']\n",
    "\n",
    "    return {\n",
    "        'answer_llm': answer_llm,\n",
    "        'answer_orig': answer_orig,\n",
    "        'document': doc_id,\n",
    "        'question': rec['question'],\n",
    "        'course': rec['course'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dfd12868-c71e-45da-9557-d22f720c04a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'No, sessions are recorded so if you miss one, you can still watch it later.',\n",
       " 'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       " 'document': '5170565b',\n",
       " 'question': 'Are sessions recorded if I miss one?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_record(ground_truth[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a0f73f6-7542-4351-9839-ba3129d0da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1954dfa89e47edb28925ced8e30b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_gpt35 = map_progress(pool, ground_truth, process_record)"
   ]
  },
  {
   "attachments": {
    "ac296b98-c68d-4f3a-9ad1-5523186778d3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b6623d42-b5f4-469e-8498-0e52684f0df6",
   "metadata": {},
   "source": [
    "![image.png](attachment:ac296b98-c68d-4f3a-9ad1-5523186778d3.png)"
   ]
  },
  {
   "attachments": {
    "4b7ab473-3178-4b91-991e-af76b2387399.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAygAAALyCAYAAAAxJvAMAAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AAAAudEVYdENyZWF0aW9uIFRpbWUAVGh1IDAxIEF1ZyAyMDI0IDEwOjIwOjM3IFBNIEVFU1RLiURuAAAgAElEQVR4nOzdd3xN9xvA8c/NvdlbEgkxEiJWjNixZ0WN0lZLaxal+LU1W3QoWqpFl1G1KWpvSa3EilhB7SDIEknI3rn398flchsjVJJLnvfrdV/uPed7vuc5R8J9zncp3NwrahBCCCGEEEKIfDAzteXixZNPLOPt7c2cOXNQqVQYGxvrXiqVSu+lVCpRKpUYGRk9eBXSdQghhBBCCCHEU0mCIoQQQgghhDAYkqAIIYQQQgghDIYkKEIIIYQQQgiDIQmKEEIIIYQQwmBIgiKEEEIIIYQwGIWSoPj7+3Hh/DnmzZ3z2DI7tm/jwvlz/DF/fmGEVKQsLS0ZMngwG9av4/ixo4ScPMGGDev58MNBmJmZFXV4QgghhBBCFBlVUQdQ3FSrVo15c+fg5OSkt71qlSpUrVKFLp0707dvP+Lv3CmwGHx8fFi0cAEAzZq3IC4ursDOJYQQQgghxLOQBKUQlSpVisWLFmJjYwOAv//fHDhwABNTE9q2aUPjxo2pWLEifyz4g+7d3yE3N7eIIxZCCCGEEKJwSYJSiL6YMAEbGxs0Gg2jx4xhx46dun2rVq1mxKef8uGHg6hapQodOviybdv2IoxWCCGEEEKIwmfQCYqpqSm9evXijS6dKVOmLGlpaZw6FcLSZcs5duyYXtmWLVvwQf/+VK5cGY1GQ1hYGIsWL2bXrt165czMzPhw0CA6deqEi4szERERrFy5CldXV/r160tYWBivd+ykK69QKOjWtSs9evSgUiUPsrOzCTl1itmz53DmzJl8X4urqyutWrUEwM/fXy85ue+XX3+lXbu2/HP2LJGRkbrtlpaWfNC/P6+/3oHSpUuTnJzM4aAg5s2bx7VrYXp1VKtWjWFDh+LlVR1raxsiIyPw8/dn8eIlpKWlsWjRQnwaNdKVP7A/EICPhg4lICAw39cjhBBCCCFEQTDYBMXIyIjf582jYcMGum3m5ma0adOGNm3aMOunn5g//w8A+vbtw+effaZ3fO3atfnl55/55ptJrP7rLwCUSiXz5s7Vq9Pd3Z0JE8aTlJz8yDi+nTKFbt266j6bmZnRvFkzfBo1Ytiw4Rw4eBAvr+qsXbMGgMDAQIZ8NDRPPY0b+6BQKAD488+VjzxXbm6uXnIEYG9vz7KlS/Dw8NBtc3BwoHOnTrRt05Zhw4cTFBQEaMeW/DH/d5RKpa6sh4cHwz086PrGG7zfq/cjzyuEEEIIIYShMNhphuvXr6dLJEaMGEndevXp8kZXdvr5odFoyMjIBLTjOsaMHg3A5s2befOtt+nbrx+hoaEA9P+gv67O93r21NW5bdt2evfpy4CBgzh5MgQba+s8MXTt+gbdunVFrVYz66efeO219rz1dneCgoIwNjbm22+nYGJikq/rqeBeQff+8uXL+b4PX34xAQ8PDzIzM5k8ZQrt2/sy6MMPuRkejrm5GTN+/AErKysAhn40BKVSybFjx2jVqjUNG/nw+bhx3L17l/T0dBQKBaNGjWbUvfsF8OZbb9OqVWuCgo7kOyYhhBBCCCEKisG2oDg7uwCg0Wi4dPkyaWlphIaGMmrUaP5c8ScnTp4EID09nS++/BKv6l5M+/57cnJyAFi6dBlTpkymjKsrCoUCjUbDW2+9BUBISAhjxo7VnevUqVPs3vU39vb2ejH079cPgGXLlutaawBGjBzF4UMHcXJywsfHh7i4ON2A9txc9SOvx8rKEoDMzEySH9Na829OTk60b98egNlz5rBy5SoAboaH8/HHH7Nxwwbs7e15/fUOrFmzVnfPom/d4nZsLGq1ms2bt3DhwkWio6N15717N0F3jtjYWJnFSwghhBBCGIzCSVA0mvwXRVv29OnT5ObmolQq2bJ5E6fPnOH06dMcPXqMgwcP6sonJCSwadNm9u7dx+uvv079enXxqlGDSve6RBkZGWFkZISxsTGenpUA8PPz1ztnWloa+/fv54033tBts7a2xtPTE4COHV+ndZvWj4zXw8ODwMBAvGrUfOJ1paSkAtpxNVZWVqSkpDz1XtSqVQsjI20jl/+/Yr506TLXrl2jYsWK1PH2Zs2atZw4cYKyZcvQpXNnGtRvQEjISU6eDCEgMDDfSZEQQgghhBBFqVC6eGVkZADkaaF4WIkSJbRl09MBuHHjBiNGjiQqKgqVSkXdOnX4oH9/5s2dw66//alZo4bu2GHDhnLwwH6+nzaVt99+m0oeHtz51zoiVlZWujEgsY9oMYiN1d9mb2ene+/k5ES5smX1XvcTB0tLi3zdg7DrDwaze1aqlK9jbG1tH8T3iJhvx8beK6eN9bupU9m2bTu5ubm4uDjToUMHJkwYj7/fTiZO/Fp3/UIIIYQQQhiqQmlBuXXrFp6enpQvXx5jY2Oys7P19js5Oem+jEffuqXbvmvXbnbv3oOnZyXq1q1LHe86tG7dilKlSjFt2lRe79iJt958k+HDhqHRaFi1ahX+f//NmTP/0LxZM376aZaurpSUFDQaDQqFAidHxzwxOv5rW0Jiou79wEGDOHTo8H+6B4cPB+nO//7773EyJCRPGaVSyeZNGzl95gx//bWGxIdicHJ05GZ4uF75+9eRmKjtspWcnMyYsWOZNHky9epp71eTpk2oWqUK777zDkePHn3k7GFCCCGEEEIYikJpQQk6oh2AbWtrS6/338+zf+jQj3TvjwYfBbRf1vv164uZmRmXLl1m5cpVjB4zhqnTpgHa2bdUKhUtW7YAICAgkEmTpxAcfJT09HRKliypd46MjAzdwPkOHXz19pmZmdGsWVO9bUlJSYSFaVs9unXtlifmz8aOpXOnTpiZmeXrHkRERLBvXwAAvr6+dPD1zVNm2LChVKxYkTe7dcOtfHnOnDmDWq0d09Lutdf0ynp4eFCxYkUAQkJOAVC5sievvdaO5ORk9u0LYMbMmbz11tvExMQAUOkRLTfSqiKEEEIIIQxJobSgrF+/gUEDB1KiRAnGjh1D1WpVCQ4+irGxMa1ataR5s2YAXL16lYDAQIyMjJg7dw7NmjalS+fO/PDjDEJCQrC1taVli5YAREZGkpOToxvb4eHhgYuzM/F37tCwYQOGDBmsO7+7uztXrlxh3br1jB8/jtq1azPjxx9YtfovjI2N+WjIYBwcHPLEvWz5Cr7+6ks6dnydO3fvsG7dejQaDX169+Ltt99GrVZz+a3LqFTG/LVaO4A9MHA/w4YPf+R9+Pa776hXry42NjbMmPEjbdu2Yf+BA5iYaFeSb968OaAdX7J9xw5yc3P5e9cufNu3Z9jQoSQlJXLw4CHKlSvL1199hUKhICEhge07duDj48Ps337DxMSY+fP/YM3atdy9e5cmjRvrus/duHEDgNSHxr907tSJwP37Sbh7l/h/dYsTQgghhBCisCnc3CvmfwT7f1C3bl3mzpmN9SOm8wW4dSuGgQMHcvXaNQDeeac7X335pd6aHvep1Wo+HTGCXbt24+1dm+XLlunK3e9GlZ6egbm5tnUjOTmZBg0boVQqWbhggd46KPclJydjbW3NtWthdOykXYtEoVAwc+YMfO/NpPVvP86YycKFC/O1Dsp91atXZ97cOXm6lN0XFhZGn779dDNrlShRgmVLl+haSx6WkZHB0GHadVBsbW1ZvHgRVatUeWS9Z/75h169epOdnY21tTUB+/ZiYfFg/MzMWbP4448Fj41bCCGEEEIIADNTWy5ePPnEMt7e3syZMweVSoWxsbHupVKp9F5KpRKlUqmb2MrIyAilnX2JiYVxIdHR0Wzbvh2lUomtrS0WFhZkZWVz/XoYa9etY9z48URHR+vKnzt3nt179mBsbIytrS1mZmbExcURHBzMZ5+P48iRYEA7vuVkSAjlypbD1taWjMxMDh06zOgxY7h+/TqOTo7ExcWxbr229cPP3x+NWkOpUi6Ym5sTERHB7/PnExMTQ40aNYiMjGTt2nW6OHbt2sWtWzE4OjpiY2NLenoaIadOMeXb79i4cSMAJUuW5J3u3QFtK8W27dsfex9iY2NZt249mVmZ2Ny7D2q1mqtXr7Fy5UomfPGF3tiT9PR0tmzdikajwcnREUtLSxITEwkICGDsZ5/rVrPPzMxk/foNxMfHY2VlhZWlJWq1hrCwayxespRJkyaTlZUFQFZWFqGhoVT08MDKygq1Ws2+gAD++efsC/rbFkIIIYQQryqVyoy4uOgnlilVqhQdO3bUJhz3kpB/JyIPvxQKxYNXYbWgGAJv79r4+vpy6OAh9h84oLdv9aqV1KpVi02bNjNu/PgiilAIIYQQQgjDVtAtKAa7UGNBGDN6NN7e3nTw9eWzz8dx/PhxbG1t6du3D7Vq1QJgw71WESGEEEIIIUThK1YJyqTJU1i8aCFOTk4sWph3vMXv8+dz7NixIohMCCGEEEIIAcUsQbl48SLvvPMu/T/oT5MmTXAuWZKsrCzOnT/PiuUr2LN3b1GHKIQQQgghRLFWrBIUgPCICCZNmlzUYQghhBBCCCEeocAXavzxhz/kz5foTyGEEEIIIYpSsZrFSwghhBBCCPHfFPQsXgXegiKEEEIIIYQQ+SUJihBCCCGEEMJgSIIihBBCCCGEMBiSoAghhBBCCCEMhiQoQgghhBBCCIMhCYoQQgghhBDCYEiCIoQQQgghhDAYkqAIIYQQQgghDIYkKEIIIYQQQgiDoSrsEyYnJ5GUmEhOTjYajSxi/ypSKBSoVMbY2NpibW1T1OEIIYQwIFZWdjg6lcbGpgSmpuYoFIqiDkm8YBqNhszMdJKS7hAXG0VKSsJ/q8/JhtwKLmhK2YOVORqjV+NnRqHWQEo6iui7KK/dQhGbVNQhGYxCTVAiI8LJzs4qzFOKIqDRaMjOziI+LpakxERcy5Qt6pCEEEIYADf3atjZORITc5PQyzfJyEiVh5WvIIVCgZmZJXb2jnhUqklCQhzXw84/V105DT1RV3Z9wREaBo2RAmws0NhYoK7sitGlSFTBl4s6LINQaAlKzK1oXXJia2uHpaUVxiYmL9WTE41GQ3ZWFqmpKSQm/renAcVFdnYWMbeicXYpVdShCCGEKEKVq9QlKzOd06cOSFLyitNoNKSnp5CensKt6Bu4uVWlcpW6XLp44pnqyWlbC3XpEgUUpeFRV3Ylx9oc1e7TRR1KkSuUMSiREeGkp6ehUhlTunQZ7Es4YGJq+lIlJ6B9ImBiaop9CQdKly6DSmVc1CG9FNLT04iMCC/qMIQQQhQRN/dqZGWmExZ2XpKTYkaj0RAWdp6szHTc3Kvl+7ichp7FKjm5T126BDkNPYs6jCJX4AlKcnKSruWkZElnTExNC/qUhcLE1JSSJZ2LOoyXRnZ2FsnJ0rdSCCGKGysrO+zsHLl+/UJRhyKK0PXrF7Czc8TKyu6pZTVONq9st678UFd2ReNUvMfwFniCkpSYCGi7db0qycl9Jqam2No+/RdNaN3/WRBCCFF8ODqVJibmprScFHMajYaYmJs4OpV+atncCi6FEJFhK+73oMATlJycbAAsLa0K+lRF4lW9roJw/2dBCCFE8WFjU4KEu3FFHYYwAAl347CxeXq3LU0p+0KIxrAV93tQ4AnK/ScmxiYmBX2qIvGqXldBkKdnQghR/JiampORkVrUYQgDkJGRiqmp+dMLWuWjzKuumN+DQluo8WUbEJ9fr+p1CSGEEC+CQqGQB1QC0D6ozM/3pldlnZP/orjfA1lJXgghhBBCCGEwCn0leSGEEEKIZ1WzZm3atfNlxoxpRR1KsVLGtTxVq9Yi+Oh+kpIMdA04yzasGNYNn0c9do/cRss//QhXF3ZQFvjUaU9Lywi2HDrGuUI//8vtpUhQckvakdGmFjnuLqDRoIyMx3zvaZQR/33QXVYtd9Q2FpgdOPcCIn0+bm4efPzxFwBMmzaO27ej83WcnV0JGjZszsWL/3DjxlUA3n9/MHXr+vD115+QnPz4WbMcHZ0ZP/57goL2sXbtUtzdPfH0rMbhw/ueeJwQQghR2GrWrM0P038iJuZWUYdSLFWtWgtX1/Js2LjccJMUgLRbnLydROZDmzLj48kokuRARfWqbRjocJyTQZKgPCuD7+KVU86JlMG+ZFcti1FcIsrYRHIqliLlg3bklC/5n+vPquVOZjOvFxDp8/P2bvjI909ja2tP+/ZdKV++om5bUlICt29Ho1Y/+TchNzeH27ejSUrSJiMVKlSiffuu2NjYPmP0QgghRMF5ODnp07dHoZ67SZPm+PsF0K/vgEI9ryGJiLxB8NH92NjY8Wa33tjYGPDyCvFBDF39C70eeg3YdYzYoo5LPDPDbkExUpD2ZmM0SiWWK/ZhfDkSgByPUqT0ak366/WwnrujiIP8bxQKBbVrNyAuLgYbGzu8vRvi77/puevbuvUvtm7966nl7t6NZ9q0cc99HiGEEKKgFWVyAtC6VRuysrJo1aotS5YuLPTzP07vXv2o7lWTzz8fWSjnCw4OBKBhg+a82a234bek/IuNZz/+frMeppf/5LUNQcRa1GZm/4G8oQ5i6NI1mL42nVme8QScTaKihztlTbK5cn0/U3dtI+BepxKnCr5806w5LZ1tIP0WQWe3MfXAKa7kgFPdjznSzp2gE3vIdGuOT84x/sxuwQdlAOoxZ3Q9rgTPoP2+sKK8DS8Vg05Qciq4oC5hjcnpMF1yAqC6Eo353ycxiksCBWBkRHrb2mTVqgBmxihvxmK+8wTKmLsA5Drbkf56fXJdHVBkZGF85jpm+86Q+n5Lcu4thJMwqRdme05jFvhPoV6jh0dVrK1tCQoKpGRJZ2rXboira3kiI2/oylSrVovOnd/FwaEksbG38PPbSEpKMv/733gAunZ9jy5dejB69Af07DmQ+vWbMmHCUIYO/Qx7e0e++upj1OpcAEaOnIidXQl++WUK48dPZ//+v0lNTaFDhzcBGDVqEqGh54mKukmLFr7MnDmRiIjrAHTp0oOWLX2ZPv0Lbt2KKNT7JIQQongp6uTEwsKSBg18WLJkAR9+OJSqVatz4ULRdQcvai9FkuLgw6x3PB908coKY8E2P4Iub+C7i57MquLLSM/L+Lt34w3rJPy3bMU/DboAGLnQ0hP8z+3B37Q673j5Msc8hy5/+hFeqhMLu/pSPScc/zOnyHCojW/DgSwwmUsX//s/E8b41PWFjHiuXL/Lpagggmx98DG/RcDZy5yMTCqSW/KyMuguXjmuDgAYX8r7Zdj08AVt0qKB9PZ1yWxSDeWtO5gcv0KuqwMp/duiMTcBBaT2aoXawRrToIsoI+PJbFqNrPqVMDlzHaO7KZCTi+mBc6hu3i7sS9R16Tpz5hghIUf1toE2ORk4cARKpYojRwJRqYzp1284trZ2HD9+GIDQ0PPs3bs9T92nTh3FwsKSChU8AbCxscPVtTynTx/X6wIWFhbK1asXATh69AAnTx7RxVKjRl1duerVvYmJiZLkRAghRIEq6uQEoGnT5mRmZrB5ywauXbtK61Zt9fbPnPEb7/Xsrftctmw5/P0CcHR0AsDIyIgPB33EurVbWLNmMz169GLmjN/o3asfAP36DmDatJl6da5etZE2rdsBYGlpyeeffcGG9dtZs2Yzw4Z+gqmpGZ+NnUCvXv3wrl0Hf78AOnbsUoB3QV9wcKBhd/eycMGnQnVa3n+5lUH7t5HEln1bCUhz4J32HzOtlgNJV7Yy9fzDSUMSm/+eydBd25i67VdGnEnC1NWHXq4WtPRuTHWTeJZvmslQvzWMXDWX2dHZlK3enDcsH9SQGb6N7vO+pv2m3aw7upWABCArgjW71zD7cnyh3oqXnUG3oGjMtIsgKlIyHl/G0ozMBp4Yh0ZhuXwvAKorUaT2akVmg8qYHg9FbWuJyekwzPadBrWGbE9XjEO1yU2WVzk05qaY7woplGt6mFKppGbNesTGxhAVFc7t27fIzMzA27sB27evRaPR4OvbjdTUZGbNmkh6ehqWltb06zec+PhYDh3aQ716jTl37hT79/+dp/5Tp47SsWN3vLy8uXLlAl5e3igUCk6fPqpX7sqVC5QvX4GKFatw4MAuIiNvAnDnTiw1atRh5871ODuXxsnJGT+/jYVyb4QQQhRPzs4u/DD9J4AiS04AWrdux6HDB8jJyWFfwB7eerM7c+f9+tQxnvf1ePd92rfvyO/zZxN7O4aePXtTqZInISHH83X8B/0/xKmkMyNGDsPWxpaPPvqY7t178MeCeaSkplDJw5MZM7/n7t07/+Uyn9nDLSlt23Zhw4ZlhXr+JwrfSKM/9zx6zEliEFODG+PTyh2nrMtM3RdE+MP71bcIuZ5270MaQRHhZNasjodzOZKcbCA1iICo7Htlwwm4Gc/IUmWp5mSMHwDZnLywn5OP/8oqnoFBt6AoMrIA0FiZPbZMbklbMFKgeqgLmPGVKFBryC1ljyI1A1V4HFm13Ekc9w6pvVqhsTQDA1gzqkqVGlhYWHLmzDEAcnKyOXv2JPb2jpQvXxEjIyWlSpUlNPQ86enaX5rU1GRmz55KePjT+zHGx8cSEXEdLy9vALy86pCcnMjVq5fyFd+pU8dwcXHF0dFZV8epU0efcpQQQgjx39yfret+a0NhK1HCgdq1arM/MACAgIA92NraUce77pMPvEehUNCtW3f+WDCXv//eScipk0z59ms0mvxP5VSmTFmuXLnMjRvXOfPPacaNH83q1Su4cyeepMREMjIzCQ+/SUpKyvNc4n/i6loeQK87uuGzoIy9DaYAKgcq2lrkKWGqfOh9vr4hZ+t9yszJfkw58awMOkFRRWqbw7KrlMmzL7NpNbIrl9GOQQF4wiq1lkt2YbEpCOOL4eSWKkFaNx8ym1YviJCfibd3IwDatOnEzJlLmDlzCXXrNtbtexGL1J86dZQSJZxwc/PAw6Mqp08fz/eKvqdOBQNQo0YdvLzq3Gvlyd8UyEIIIcTziIm5xZixnxITc4tevfoVSZLSskVrUlJSOH/hLCYmJiQk3OXSpQu0vtf96mkcSjhgY2PD6dMPemckJSVx8+bNfMewc+c2Onfqyrx5ixg29BPKlS1HTk7OM1/Li/bmm30o41qeCxdO61pTXgambp0YV9OBpBvnOJnlwDttOuFj8lABozK09HS498EGXw93TInnSsxNzsUmgaUnLUsba3erytKynANk3eJ87FOSEiOVNikSz8Sgu3iprt3CKD6ZLC83jC9GYHxWm6nneJQiva03ylt3sVq+F3LV5FQug+nRywBke5QGIwXK6LugNCKrviemwZcwOXkVlEYkft6dHLeSmB48h0IDGL2ATOAZmZiYUL16bZKTEwkLC9Xb5+lZndq167N580qio8OpVKk6FhaWpKWlYmVlw6BBI9iyZTU59zJ1pVL5qFMA2gSlU6d3ePPNXqhUqjzdu+67n7M8XFdExA3i4mJo2LA5Tk4u7Ny54T9etRBCCPF095OUH6b/RK97CcryFUsK7fytWrXF2tqGTRt36m0vV94NUxNTMrMyH3PkPY95wpjfB4QAAYF7OXf+LPXrN6SGV02mTv2RufN+Y9u2zfmu40V7ODnZtXtLkcXxWP8eJA8QH8TngYkMbOODR8Y5xu1YwMlqH7OlhQ8Tmxyjy77742ot8Gkzii0VrxFhWpaWrhZkhu9lRWQa4TmHOefpS++uIyl5JQIcqtOylDHhIfvZnMpjEpAckjKzwcyTYV37UedaIBNPyixe+WXQCQpqDRYbDpHaty2p7zRD2aQaCrWGnDKOKHJzMd9xDEVqBqZHL5PpU4WUvm1QxieTVctduz34Elm13En3rUtWnYoYX4ggt6QtGlNjVGExACjSMtGYqEh9txnGFyMwOV04PzzVqnljamrGrl1b2LtXf6rkt97qTZMmbahYsQo7d25g4MARjBr1DZcunaNSpWqUKOEIKHTNuk2atMbBoSTbt6/Lc547d+K4eTOMcuXcSUpK4Nq1y4+MJzU1GYA33niP0NDzurEmp04dpW3bzrr3QgghRGEoqiTF1bUMnp6VmT79W6KiHnQfNzUz47tvf6Bho8bs37/v3rhQK93+Ui6lde/j4+NITk6iVk1voqOjALCysqZcufIcPRoEQFp6OpYWD0ZYm5tbYGv7YC2yRo0aExwcxI4dW9mxYysxMbdo1rRFkSUoBp+cwL1B8i7620xvUKNxe3o7ZRO0ZyNrErPh2BoWeI5kWN13GHbxV64AqMPZfCKe6tWr094kmyvX/Ji6048raiB6GwM25PBNq+a09CqrnWY4eA0TA8+RBPcG4v9bGn4n99PduTl1POphm3aKqcBTUltxj2EnKIAqPA6r+X5ktK1FjpsLoEF1NVpvJXlzvxMoMrLIrO9JbvmSKG/cxmJLMIqMLG2rCQoyG1Ums0lVFKkZmO09jenhCwCYBl0kx9WB7KrlMEpOh0JKUOrUaYhGo+HkySN59h09epAmTdrg7d2QNWsWs2DBLLp06UG9ek2Ii4th6dLfdLNuHTq0l3r1GlO/fhP+/vvR/2idOhVMuXLuT+zedebMcerVa0z58h6YmprmSVDut6YIIYQQhaUokpTWrdoSFRXJnr278uw7diyY1q3asH//Ps6fP8vrr3chcP8+NBo1AwYM1pXTaDRs3LSegQMHk52Tze3bMfTs0Qsjowc968+fP8sH/QfSvFlLrlwNpX//Qbr9tja2jB0zgaPHjrB+/V8olUoaNPDhxAntg8KU1BTKlimHV/UaxMbF6sbsFBSDT05S99Br+p4nFNhBtf0PfcwJZ+bSEWjnUDPWTjNMDuePL2DkY6qJve7H0MV+j9534hcqnsi7PenaRrrPlsmFnofBJygAytsJWK58Qj9HjQazfWcw23fmkbtNTl7B5OSVR9cdFY/NL4X/y7Zo0S+P3RceHsbIkf10ny9cOMOFC4++tvXrl7F+/YMZNFatWsCqVQv0ygQE+BEQoP9LdedOnN450tPTmD17Wp76708heKT79ccAACAASURBVH88ihBCCFGYHk5S2rXzLfAEpVWrtuzek3dmTIBdu/0Y9/mXWFlZs37DGsqVc2P697O4cyeetetW8+kno3VlV69egaWlJUM/+p8uYbGwtCQzSzsB0NmzZ1i2bDHDh38KwIoVS6nhVQuAxKRERo36HwMGDGb69J/Izs5mf+A+3bXv3x9A27bt+f77Waxdt5olS/T/33+RyriWN+zkRLySXooERRSNDh3ewtu7IdnZ2Rw7drCowxFCCFFM3U9SCsMHA3o9dt/Bg/vp2OnBQPmp0ybp7d+5cxsACoURXbp0Y/v2LcyfPwfQjvHs0rkrkREPJrdduWo5K1ct133esvXB0/aw69f44svPHhlHfHwcw4YNeoaren5JyYls2LCMiJdqxi7xspMERTxWs2ZtSU9P5c8/55GcLCugCiGEKDoF3Y3pRdJo1DT2aUrrVu34/fffSM9Ip+sbb5GWlsbRY3m7dhuypKSEQl8xXqHWoCm0CYyy2bJpBIbWNqRQG8B6GEVIEhTxWOPHf1TUIQghhBAvpclTvmLIkP8xceK3gLZL12efjyQ7W9bKeKqUdLDJu05JsZKSXtQRFKlCS1A0Gg2KF7Gwh4F5likDhRBCiOLm/v//xe3/y6SkJKZP/7aowzAo+f05UETfRVPMExRF9N2iDqFIFfhCjfeTkux7g8JeNa/qdRWEVzFBFUII8WSZmemYmVk+vaB45ZmZWZKZ+fSWAeW1l6c7X0Ep7vegwBMUlUq76mZqakpBn6pIvKrXVRDu/ywIIYQoPpKS7mBn71jUYQgDYGfvSFLSnaeWU8QmYXQp8qnlXlVGlyJRxBbvsb8FnqDY3Ft0KDExgazMV2t5mqzMTBITC3fg2MvM5qEFqIQQQhQPcbFRODuXk1b0Yk6hUODsXI642Kh8lVcFX8Yo6unJzKvGKOoOquBHL6pdnBR4gmJtbYOxsQkAt2/HvDJJSlZmJrdvy8KF+WVsbIK1tU1RhyGEEKKQpaQkkJAQh5tb1aIORRQhN7eqJCTEkZKS/we7qt2ni1VLitGlSFS7Txd1GAahwBMUANcyZTE3tyAnJ5uoqAju3oknKzPzpRswp9FoyMrM5O6deKKiIsjJkZk48sPc3ALXMmWLOgwhhBBF5HrYeUxMzXF3ryYtKcWMQqHA3b0aJqbmXA87/8zHq4IvY7zzhLbbU1LaKzX9rkKtQZGUhtGlSIx3npCWk4cU2ixezi6liIwIJzs7i8TEBOkaVUwYG5vg7FKqqMMQQghRxC5dPIGbezVq1W5GTMxNEu7GkZGR+tI9rBRPp1AoMDOzxM7eEWfnciQkxHHp4onnry82CVUxH5NR3BTqOiiuZcqSnJxEUmIiOTnZ8o/SK0qhUKBSGWNjayvduoQQQuhcDzuPlZUdjk6lqeRZG1NTc2lReQVpNBoyM9NJSrrDldAzz9StSwgogoUara1t5EurEEIIUUylpCTIF1YhxBMVyhgUIYQQQgghhMgPSVCEEEIIIYQQBkMSFCGEEEIIIYTBkARFCCGEEEIIYTAkQRFCCCGEEEIYDElQhBBCCCGEEAZDEhQhhBBCCCGEwZAERQghhBBCCGEwJEERQgghhBBCGAxJUIQQQgghhBAGQxIUIYQQQgghhMGQBEUIIYQQQghhMCRBEUIIIYQQQhgMSVCEEEIIIYQQBkMSFCGEEEIIIYTBkARFCCGEEEIIYTAkQRFCCCGEEEIYDElQhBBCCCGEEAZDEhQhhBBCCCGEwZAERQghhBBCCGEwJEERQgghhBBCGAxJUIQQQgghhBAGQxIUIYQQQgghhMFQXQ+7WtQxCCGEEEIIIQqRm3vFog7hsVSGHJwQQgghhBCieJEuXkIIIYQQQgiDIQmKEEIIIYQQwmBIgiKEEEIIIYQwGJKgCCGEEEIIIQyGJChCCCGEEEIIgyEJihBCCCGEEMJgSIIihBBCCCGEMBiSoAghhBBCCCEMhiQoQgghhBBCCIMhCYoQQgghhBDCYEiCIoQQQgghhDAYkqAIIYQQQgghDIYkKEIIIYQQQgiDIQmKEEIIIYQQwmBIgiKEEEIIIYQwGJKgCCGEEEIIIQyGJChCCCGEEEIIgyEJihBCCCGEEMJgSIIihBBCCCGEMBiSoAghhBBCCCEMhiQoQgghhBBCCIMhCYoQQgghhBDCYEiCIoQQQgghhDAYkqAIIYQQQgghDIYkKEIIIYQQQgiDIQmKEEIIIYQQwmBIgiKEEEIIIYQwGJKgCCGEEEIIIQyGJChCCCGEEEIIgyEJihBCCCGEEMJgSIIihBBCCCGEMBiSoAghhBBCCCEMhiQoQgghhBBCCIMhCYoQQgiDYG5ujr9fgN5rzZrNfDtlOu7uFZ+73s/GTsDfLwB7+xL/Ocb3evamZYvWALi5uePvF8DXX03J9/EPX+PQjz7Os3/IkOG6/S+Co6MTvXv1o2qVarpto0d9jr9fAFZWVo88Ztmyv5g7Z+EznadhQx+9vze/nftYvuwvhgwejpmZGQC+vh3x9wvQ3b+Hz/O0mF6U+vUb8eeKjXTv/n6Bnue+LyZMxsenaaGcS4hXiSQoQgghDEpiYgKBgXsJDNxL+M0b1K1bnx9/+Pm5E4z4O/FERISjzs39z7G9914fWrbUfsG+e+eO9s+7d56rrkY+TfJs82mUd9t/4eToRK9e/ahStdrTC78AFy+eZ9cuP/bt201OTg7dur3NgAGDAUhNTSEiIpy0tLRCieVRGjduTnZ2Fk0aNyvwc7m5VcDDw7PAzyPEq0hV1AEIIYQQD7tx8zrfTZ2k+/xmt+4MHjyM1q3asn7Dmmeub8GCeSxYMO9FhghAUnISOTnZxN+Jf+ZjMzMzcS7pTIUKFbl27SoA7m4VcHEpRWZmJqampi863EKxZ+8utmzZCICJiQmLFq6gSeNmzJ79MwcOBHLgQGCRxWZuboF37bqsWfMn77/fn0qVKhMaeumFn6dNm/b06T0QlUq+YgnxvKQFRQghhEE7889pAEo6O/NaO1/8/QLo07s/v89bzC8/zwWgatXqzJr5G9u27uLPFWt5/70+KBTa/+L+3X3I3r4E4z7/ik0bd7J+3Vb+N3wEJiYmuvM1aNCI+fOXsG3rLn6ft1j3tH3njr0YGxvj49MUf78AqlfzIiEhQdeC4u5WgR+m/8TmTX78uWIdAwcOwdTU7JHXdPr0SdRqtV73Hx+fJuTkZHPmzCm9sg4Ojnwx4Rs2bdzJhvXbGT3qc6ytbQB092P4sE/5fd5iNm7cwRcTvsHCwhKv6jX46ac5AAwZPJydO/bq1duunS9Ll6xizZrN9OnzQZ4Yra1t2L5tN5O+marbZmVlxY7te/jyi2+e+HcGkJWVRXR0lO6+34+1ceOi6fLUoL4PmZlZ+P+9nZs3r9O4cXO9/V9/9R1du3bXfS5d2pU/V2ykRAkHAGxt7Rg5YhyLF/3FzBlzqFe3IX+u2EjVql569QQFHWTc+E8ZM3Z4wV+UEK8oSVCEEEIYtNq1vQG4HROj29azZ2/sS5QgPPwmTk4l+X7aDFzLlGXb9s1ERUfSp88HvPde7zx1KRQKJn0zlQYNG7Frtx/BwUF07NiZD/p/CGiTk8mTpmFibMJOv20YGxvz5ZffUKNGLf5asxK1Wk1kZARr1qzkduxtvpn0BYcOHUChUDB58jRKl3Zl48a1XLp8ge5v96Dj650feU2JiYmcP39Wr0uXj09TTp0OIS0t9aF4jZgy+XuaNGnG/v37CA4+TNu27fMkCK+/3pnLoZe4du0qzZq1oMe773M79jZ79vwNQMipk/y1ZqXeMQM+GExkZARKIyXvv9eHenXr6+1PTk4iJOQE3t51MTc3B7RjOJRKJYGB+574d2ZkZEStWt5UqVKNkJCTTyxbWJo0ac7x40fIzc3l8OEDNGrYBCOj/H8NGjZ0BCVLOvPzL9NZtXoZPXr2eWS5tLRUoqIiiYqKfFGhC1HsSPujEEIIg1K+nBvjx30FaFsPvLxqkpKSzN59u6lfrwEAZ8+eYcIXY8nKymLIkOGYmpoxZuwILl26gEJhxE+zfuOd7j1Zu3aVXt2NGjbG07MyM2dN50jQIQBMTEzp3PkNFiycR5/eH5CYlMjw/31ISkoKtja2fPnlJDIzM1iyZAFvv/UuN29eZ+Gi+QDcvq1Nmuzs7HFyKsmevbtYtnwxarWaBg0acexY8GOv83DQIQYNHIKjgyMAlSp58tvsn6hZo5aujI9PYypUqMjChb+z5t61xN+Jp/vbPfCqXkNXbvXqFSxbvhhTUzPWrtlMda8aLFo8n61bN9GmzWsEBx9m48Z1euefOm0yhw7tp2FDHyZ9M5Xq1Wty/MQxvTKB+/dRv35D6tVrwIEDgTT2aUJGRgbBR4888pqGDf2EYUM/0X2Oi4vll19nPvYeFBY7O3uqVavB99O1XQeDjhzg3Xd74eVVizNnQp56fLlyblSrVoMxY4YTfSsK0LYQjR3z5XPFY2RkhEKh0H3OfQHjo4R4lUiCIoQQwqDY2trR4t5MT0lJSRw/fpSFi37XG4zu57+DrKwsADwqViIxKZFLly4AoNGoOXb8KFWqVKN0aVe9uj08KgEwcsRYGKF/3tKlyuDuXoFDh/aTkpICQGJSIqPHfMLTJCTc5cLF87Rp3Q6fRo05e+4fDuwPQKPRPPaYoKCDfDjoI73B8kFBh/QSFI+K2niDjwbpth0NDqL72z2oUNGDjPR0AK5evQJAZmYGCYkJWJhbPDXm+13Jbty4DoC5hXmeMocPHyDnk1E09mlKUNBh6tVrQHDwYTIzMx5Z54UL57hxU1tfSSdnvL3rMnzYp3wz6YunxlOQfHyakZqaSmjoRYyNjUlMTODqtVCaNG6erwSlbNly3L17R5ecAFy6dP654/n666l4VHwwgL5f/3fIzs5+7vqEeNVIgiKEEMKgnPnnFGPGfPrEMunp/5oJ6l+JgIJHu//UeurUSSQmJertu3P32Qe7P+yzsSNo2aoNtWrWxtu7Lg3qN8LOzl7X8vFvUVGR3Lh5XdfN63LoJeLj4x5ZVu/yFHmvTsPjE6HH0WjU9/58/LGpqakcP36MBg18qFu3HhYWlk/s3rV3327dIHnQjn3p1u1tKlUq2tmsGvs0w8rKioUL9P8uyriWxcTERJfsPt4j7vmz33Kd33//BTPTBwlhTk7O81cmxCtIxqAIIUQBcHUty2vtXi/qMIqFK1dDsbW1o0qVqoB23Ea9eg3IyMjIMw7gytVQQNvlJyTkBCEhJ3B2dqFKlWpkZKQTFnYNb+96ukHodnb2/PrL79S416qh0WhQPmJ2JpVKRadOb7Bnzy6m//Advfv0IC09jRo1a+Up+7CgoEPUquVN7dreHD588JHXBtCooY9uW8MG2vfX7rWaPMn9xEWlfP7nkYH792FlZcWAAYNJS0/j2PHHd1vL4973+sdNFlAYXFxKUaGCB3Pm/sTXEz/Tvb777iuMjU2o460de5OekaHX8lSypIvufXj4DeztS+DiUkq3zdOzynPHFBUVybWwK7rXk5JEIYojaUERQogC4Opaltdee52/d+0o6lBeeevW/YVv+45MmjSNwMC9VHCvSJUq1ViydGGeJ+NBQYcJDb3M4MHDqFy5ChqgZYvWnDp1klWrlrN02UImT5rG7Nl/cPLkcWrX8sbFxUX3/DwpKZGaNWrz6Sej8fPfzsWL2m5lbdq8xocfDqV9+9c5fPgA5cq7YWFuwenTT+4+FBR0kB7vvq97n3f/Ya5cuUz//oMoV648xsYmtGjRipCQE5w990+eLmz/lpiobSXq1OkNSpUqzaLFf+Tjjv47hkNkZWVRvpwbe/fuemJrQ+tWbalY0QN40MUrLi6WK1cuU7pU6Wc+94vQpHELYmKiOXQo7xTHp0+fpHHj5hwJPkTo5Yu0bvMaR4IPodFo6NnjwSD4mzevc+HCWT75eCwrVy1FpVLRs2ffwrwMIYqVQmlBWb1qI/5+AezcsRdbG9s8+5s3b6VbfXbYsKf39X0WJiYm9O7Vj86duuptnzZtJv5+Abi5uT/22PyUeRI3N3cmjP+aP1esY8tmP+bNXcg73XtibGz8XPUVhLJly+HvF8DMGb8VdShCvBIcHBwZOHAovd7vR8mSLowf9w3NmrbUGxD7MDtbO/6Y/yd/rthI5cpVH1uvQqGgXdsOzJo5j2VL1zFz5lw6deyqV6+JiQlvdnuHH3+czeJFq/lh+m/4+nZ+7LlfFXFxsXzy6UdEhIfTwbcTLi6lmff7b6xatTxPWY1GzefjRrJv324a+TShQf2G/L3Lj8lTvgbg2LFgvvpqHBkZ6bRt8xpZWVlMnjJRN9Xx0mWLSE9Pp107X5ycSurq9fffwaxZP5Cbm8tbb72LZ6XKLFu+mPXr1z4x9kuXLnLnTry2u9e9sSD/jvezz0cRGLiXpk1bUL9+Q3bs3MrXEyfk695ER0exbdtmbGxtadu2vd50yvmVnp7G8eNHAW1rypNUrVod3/Yd8W3fkUqVPDl2PJjxE8aQkfHoMSuFoXHj5hw8GPDIfQcO7KNWrTpYWlqxY+cWLl28wITxkxk+bBR+/tv0ys6ePZPY2NuMHDGOPr0HsmH9agCyn9o9TAjxrBRu7hULvF1x9aqN2NvbAzBr1g/4+W/X2z9+3Fe6AZFbtm5k9uyfX9i5raysWL9uGzduXOfDwf1026dNm4l37ToMHtKf69fDHnlsfso8jlf1GkydOgMTExNSUlLIyEjH0dEJgCtXLjN6zKd5+1AXgbJly7Hgj2WcO3eWkaNkznYh/gtn51JMmfwjd+7EERUVQdWqXly9Gkrt2nU5cuQgv/42I88xn3w8hpo162BmZsakyeN1A73/rWePPnTs2JUDB/Zx4+Z1qlerQZ069dmxYzN/rlyCQqFg1MjxVK9ek737/iY6KpJKnlVo2qQFa9b8yeYt6x5Zb3EwduwE2rRux5tvdSQ1NfXpB4g8fvppDuXKluedd7uSk1P8BnPbWNvQtl0HNm1ai1qtHbtTrpwbU7+bxeAhvXWTKghRXJiZ2nLx4pOnEPf29mbOnDmoVCqMjY11L5VKpfdSKpUolUqMjIx0r0Lr4qXtX6mhSdNmegmKiYkJ9Rs0Qq1WP9N85Iaub98BmJiYsHLlcpav0E45Wb68GxPGT8TZ2YXSpUvrZl0RQrwaur/9HnfuxDHhi1HUqdOAsmXL88OPU2jXtgM9e/bBycmZ2NgHa3l4e9ejfn0fVq5cwvvv939svc7OLnTs2JV161ayabM20fDz28rwYaNo374TGzetoZJHZby96zH9h8mcPq39T2P3Hj9UKhUtWrQulgmKQmHEh4M+omGDRiQlJUly8hy8vGrS8fXOVK1SjY0b1xXL5AQgV63Gt30nnEu6sH3HZszNLXjvvb4cPBggyYkQBaBQE5TzF85Rx7suFhaWuoWo6tapr+unW6uWt94xlpaW9O0zgGbNW2JtZc21sKusWrmcoCPaueu9qtdgxoxfOXnyONHRUfj4NAGFAn//HSxduojq1aozY8avAJQv74a/X0CeFpqKFTwYNfJzypcvz7Wwa/z88wzCwq7mif/XX37H07My//t4CJcvX9TFt+avzaSmptDzvbf05jF3dS0DwJ49f+uetty4cZ0JX4zFxMSEyMgIQLsycWpqCitXLqdjxy44OZXkWthVFvwxl7Pn/tGrb+CAIdT2rgMaDcdPHGPevN90M77cr2fVqhV07NgFOzt7Tpw8xi+/zCQ5OUlXj69vR7p374FzSReuXbvC+g1P7n4ghMi/smXLcenyhTwz8uzavZPTZ07qJSdmZmb07/che/f+zeXQi0+st3z5CigUCoKO6I9ROH78CD4+TXFyciY5OZn161fpkpP74uJiqVK52n+8speTQgHdur3F7dhY5s79pajDeSl5VqpMkybNOXT4AEuXLSzqcIpMamoK3039mvd69mPi11NJTU3l2LEjrP4rbzdCIcR/V6iD5A8eDMSreg0aNvRh377dADRp0gyNRsOhwwf0EhSFQsG3U6ZTtWp14uJiuXz5EpUrV+brr6fw44xp7N7trytbp0490tJSuX49DE/PyvTs0Yvbt2M4d+4fQk6dxLt2HdLT0zl3/h9u3rihF9OYMePJyspCqVRStUo1vpn4LQMG9s4zH/nevbvw9KxM8+YtdQlKo4aNUalUBAbuy7PIUmRkBA4Ojnz88UjWrlvNuXNnSUtLJTb2dp77Ym1tw6BBHxF8NIhbt6KpW7c+U6fOYNjwQdy8eQM7O3tmzZqNrY0tly9fQqlU0rxZS9zdKjBs2CAyszJ19QwcOIRLly5gY2ND82Ytyc3NZdq0yYB2EOeIT8eg0WgID7+Js7ML4z4v2rnphXiVxMTcon49Hw4cyNtP//6Cfvd1f/s9lCoVq/9aRunSZZ5Yb2pKMpB3JiQbWzsAEhMTSEi4y7UwbauslZUVxsamVPasQpvWr+XpS19cqNVqfDu0LuowXmobNq5lw8aX/0GWg4MjvXp9QPVqNcjMyuTw4QOsW7uS7IdahN58813s7OxZtGjeI+u4fv0a3039qrBCFqJYK9Q+VYcOHQCgaZNmACiVSnx8mnD+/Dnu3NGff75Vq7ZUrVqdM/+col//9xg5ajifjxsNwKCBQ1A9NM1jWnoaQz4awIiRw/n5Z20fbx+fJty4cZ0pU7T/mNy+HcOECWPZum2TfkyHD/DW25147/23iY+Pw9nZBXf3inliDwjcg1qtplnTFrptTe+937t3V57yc+f9SlxcHLVqeTNl8vesX7eVX3/5nXe698TS0jJP+R9nTGPixAlM+GIs8+fPwcTEhJ49egHw/nt9sLWxZfbsn/nfx4MZOmwgGzeuo2zZcrRu006vninfTuTTEcP438dDUKvVuvn1FQoj+vTuf6/M1wz6sC+9er/DsXsDH4UQ/92qVUvJzMxg4tfTGDjgI6ytbWjatCUWFvqL5rm7V+S11zqybNkC0tKePhbt/IWzDPmoL1FREbptxipjXmvXgUuXLpCQcFev/KyZv/Pbrwv43/9Gc/78WdbfG8wrRHFkZmbG+HGTsDC3YPacWaxdu5KWLdrw5pvv6srY2NjSsEHjIoxSCPGwQk1Qbt+O4dLli9Sr1xBTE1Nq16qDlZU1Bw/mnfrP597Kuhs3rtO1Zvzzz2nOnj2DnZ09lTweLPp09uwZYmJuacucPQOAQwnHfMW0bt1fZGdnk5iYoJsO0sHBIU+5u3fvcjLkOC4upfD0rIyZmRl169YnOjqKCxfzriZ77dpVPhjQi1mzfmD//n0kJCbg6VmZAQMGM/u3P/RmM8vMzNBLcrZu24RaraZ27ToANG3WAo1GjaOjE/36DqBf3wHY3Zt0wKt6Dd1xGRkZHDq0H9DO3BIfH4eZmRmWlpY4Ozvj4lKKsLCrHDyoLZOdnc3q1SvydZ+EEE8XfSuKMWOGM3v2TK5du4KVlTUfDfmEn3/6g1YttQ8TjIyMGDhgKKfPnCQ4+HC+6tVoNCQnJ+m6jikUCgYOGoaDgxPLli/IU37mrO/4cca3bNq0Fi+vmnzQf8iLu0ghXjJNmrTAxMSEGTO/4/Tpk+zfv5fNm9fRoEFjXF3LsmTxGubOWUKZMuWKOlQhxD2Fvg7KwQOBVB5Qhbr16lO/XkPttkP7qVxZf8Eju3tdF/7dJer+Zzt7e5KTtGMrsjIzdfvvJzP5nVbz4Zm0srKznnjs3j27qFe3Ac2atiA09DKmpqbsvddV7VEyMzPw89+umxSgapVqjB4zjjKuZWnv25E1a1YCkJSUpLdIU3Z2NikpKdjbl8DIyAh7O3sUCgXvvvtennNYWVk/iP9fUx0+/GXmftJ1+/a/72fsY+MXQjy77JxsDgcdICc3F0dHJ376eTq9e33AgAEfcfHiObzr1MfVtSwLFs7RLQZoYaFtVbUwt8Da2obMzIzHrjVhamrG0I8+pWZNb376+XuuX7+Wp8yFC+cACAk5TmzcbQYNHIaf/zYiI8ML6KqFMFwRETdZuWqJ3u9USmoKpqamxMREM37CCAA+GvJpUYUohPiXQk9QDhwMZMCAwTRv1pLatety6fJFbt+OyZOgJCQmAODkVJLQ0Mu67ffnnU+4exelUll4gaPtDpaRkUGzZi11K8w+qnsXQIcOnQgPv8nZey06ABcunmf37r/p13cAJexL6Lbb29tjbGysS64sLS2xtrbm7t07qNVqEhMTUKlUvN39DTQa9XPFHh+v7UJXsmRJve33pz4WQvx3JiYmeRKLiIibzJ4zi7lzluBRqTL16zXE2NiYKZN/zHP86NHaMWErVy1l+/ZNefbb2zswetR4SpRw4NvvvuTKlQf/Nn7wwRBsrG356efv9Y65n8C4uJSSBEUUSw9P3a1QKChdypXXO3Qh6MhBcnJyiIqKBCDzoYedQoiiVejz+kZHR3Ht2lVatmyNvb29rrvRvx05ou360K3b27qFDb28auLlVZPExARCr4Tm63xqtbZlwtTU9D/HnpGRQVDQQUqVKk2zZtpWlIiIvP/hN23anE8/Gc03E7+jZYvWKBTa21yihAMtWrQCIDT0kq68SmXMez0frFjbp88HKBQKTp85BcDhwwexsrKm+9vv6p1j8aI/dWNMniYmJoaYmFu4u1ekadPmuvPeX8FYCPHflCzpzPTvf8Hjoe6n95mZaQe3JyTcZfnyRUydNlHvtWTJfACWLlvA1GkTOXJvpsKHublVYNKk7zEyMuKLL8foJScAaamp1KlTnwoVPPS216ypnXwkJib6hVynEC+rJk1asGL5BqZP/xW1Ws369auKOiQhxGMUegsKaGfzqlChou79o+zdu4vOnd6gZo3aLFm8kpiYGCpXroxCoWD+H3PzPRd7WloqKSnJuLiU4vd5iwkJOcG8359/1fS9e3fTqlVblErlY1tPjhw5zOHDB/k/e/cdX1V9+H/8dc7dUGFtBQAAIABJREFUyU1CAgkgBBEQRBDZiKJg0WqltY46qtVWrVVr669q1eKqe32rdihV66pW6yoOXLjFATIFCUv2DITscfc5vz9uCATCCNybe5O8n4/H5eaM+zmfexOS+76fdfTRY5g48Vauuupaqqoqyc8vwOl0snDhAj77/JOG823b5rzzLuD448fjcDgoKOhMOBzmxRfj0xc+/59nOfbYsVxyyWWccMJJBIMB+vY9jECgjnXr1+5TvW3b4rnnn+G6P07k5ptuZ+26NeRk55BT35VORA5MdXU1tg03XH8r/3nhmYb9PXr05KJf/YbNW4pZunQx4fCun9LW1a/PsXLl9w3B429/fQKv18tll1/IYYcN4Prrbsa2bT784F2GDR3R6PFLly1mytuvM3LkaG668U4++ug9Nm8pplevPow9bjzTp3/Z5IcpIu3Jt9/O4aabr6Vjx06c9tOz+H9XXc/9D9yR6mqJSBNSElC++HIaF154MatWrWhoWt2ZbdvcdPP1XHzRbzjmmOPo168fq1ev4oUXn+Prr79s8jG788ijf+Piiy6lsLAHa9et2fsD9mDuvDlEo1EcDrNRyNhRNBrl9jtu5rjjjufkk06hz6F9yc3NY+3aNXz08Qe8PeWNRtMSV1dXM2nS37jookvJy8tj6dLF/OvJx1i7djUAZWWl/OEPV3LJJZcxePAQbBtmzf6Gp5/+V7PedHz00VTcbjdnn/Vzuh3UjVWrVvHYY4/wpz/dckCviYjEx7Pdfvuf+PWlV/KbS3/XsP/eex5m/fq1PPTQPU2Gk90xHSamGe/G2rv3oQ1TDJ9zzgW7nLutS9htt0/kjDPOYdSoo8nJyWXLlmL++99/88GH7x7gsxNpvQoLDyYUCrJly2Zqa2tYvXol69at4eGHHqNHj54Nf2tFJH0YPQ/pbe/1LGkwcuRR3HnHfcydO5uJN/7xgMt7791PqKmp4ayzT01A7UQkHRQUdOaE8SdzzDFjefDBe1i1ekWjiTBEpOXcfNOdVFVV8fd//F/Dvu7dC7n/vr9z083XNozTuvmmu9i4af1u10ERke28nhyWLJm7x3OGDBnCpEmTcDqduFyuhpvT6Wx0czgcOBwOTNNsuKWkBaU18vuzmPinW+jffwAQn55YRKQpW7ZsZvmK7xk6dETD4okikhpvvPkaN1x/KxdfdDkzvvmKjIxMzjzjHJYuXcyaNatSXT0RaYICyj5yuZwMGzacrVu38uyzTzJn7qxUV0lE0tjMmV8zc+a+rXMiIsmzcOF87n/gDs484xyOOeY4gqEg8+bN5uWX/6OWTZE0pS5eIiIiIiKyz5LdxavFpxkWERERERHZHQUUERERERFJGwooIiIiIiKSNhRQREREREQkbSigiIiIiIhI2lBAERERERGRtKGAIiIiIiIiaUMBRURERERE0oYCioiIiIiIpA0FFBERERERSRsKKCIiIiIikjZSHlAO6to11VUQEREREZE04UzVhV0uF4WF3Sns3g3TYVJcvJloNMrwYUNwOLZXq2jRImpqalNVTRERERERaUEpCyjDhg5h1erVBAJBTMNgyOAjmTV7Dk6nixnfzGzyMRmZOU3ud7k8yayqiEirU1mxJdVVkDRgGEbD17YNpmlgmg4Mw6y/N+I306SutorguEGYdUGIxjBrghCOYAQjmDVBjEBoW6nYDgMjHE3NkxKRNi8lAcU0TVwuF6WlpRR278badetZv2FjvEJOJ4OOGIjb7aKsvIKVK1fttTxjp227if12As9r6nHpdN7u7Ot5IiLS+uwYRhwOF263B4fThcPhwuFwYpp77tVdV1tF8AeDwDDiaaaJe6MmgKO8BrOkCue6EhybyjBLKjEsG9swMCIKLSJy4FISUCzLYsXKlQwbNpQMn4/evXuxfv0GQqEQa9asZcuWLYQjEQYNGkiXLp0pLt4MQF1tZZPl1bVk5UVERNKAYZj19+ByeXF7vLhcnkbdpJtfphEv0ICGj7N23Pb7iGVlEOuRT2RYn4bgYpZW4Vy3Fdd3q3GsLcGIRLEdJkYocqBPU0TaoZR18dq4cRMbN25ixPBhBAIBhg0bwuzZcyktLSUUDgOwefMWcnJyGgKKiIhIe2YYBrYNDqcTnzcTt8eLw+FK4AV2+mIft62O2YQ7ZhMefAhgYJZU4vp+I65vV+AorsB2muoSJiL7LCUBxe/PpGvXrnz//XJs22LTpmJ69TqELL+fXr0PYfbsudi2TaeOHSkrK0tFFUVERNKGYZg4HA68Pj8ejw/TdCTtOjvt2a9tu6AD4YIOhI85HKMmiGvhatyzlmFurYJoLIE1FpG2KCUBpba2DofDZPRRI/F4PAwfPpQVK1ZSWlZGVnYWo0YOx7Isyisq2LipOBVVFBERSaltY0p8Pj8ebyZOZwJbSnarqZGYB3bc9nsJH3UY4aMOw9xcjuez73AuXgegMSsi0qSUBBTbtlmyZBmGYTB82FBmz4m3mACsXr2G1avXpKJaIiIiKRefWcskMzMbr8/fwtfe2/Joe5tmZc/H7S55BM8dC4Br1jLc0xZiVtaqVUVEGknZGBSIB5VZs+eksgoiIiJpwTAMHA4nGZk5eDy+VFcnnjXs5G1HRvYlMqIvzqI1eKbOwaxQUBGRuJQGFBERkfZuW4uJPys35cHE2DZj1449tZK8HTuiJ3UDe+IsWoP7nZkYNQGMiIKKSHumgCIiIpIC27pT+f05Ld6VKx1FBx5MdMDBuGYtxf3ebLBsTVMs0k4poIiIiLQwwzDw+TLJyMxptMBiyhkGxo4tHTvd2/FTkno8OuowosP64n5rOq7Z30PMOvDnJSKtyt5Gw4mIiEiCGIaB0+kip0MBmf4O6RVO2HN4aNHjTpPwGccQuGICVn4OuPR5qkh7ooAiIiLSQjIys8nN64LL5U51VZpmGPVjYoyGVeVTuW31KCB43c+IHH04mHrLItJe6H+7iIhIEsUXWXTSIbczGRnZqa7OPrHTbDs8YQTBKyZgd/CDpyXWgxGRVFJAERERSRLDMPB4M8jr2DV9W0120FQrxs63VB23e3YmcNO5xA7vAU5Hql8qEUkiBRQREZEkyfR3ICsrN9XVaBajia/im0ZaHA+fdzzhn4xSly+RNkyjzkRERBLMMAyyczrhdntTXZVmMXYKCQY7bafJ8dgxAwgVdMDzzIcQ1lTEIm2NPn4QERFJIIfDSW5el1YXThpsm0orze+tvt0JXnMGdnYGeNO/+5yI7DsFFBERkQQwDBOn001uXhccjtbZQcEw68d8tJJ7CnIITTw3PhWxBs+LtBkKKCIiIgdo2/omuXmdG7ohSQvxuAhffTpWr64KKSJtROv8iEdERCSNuFwecjrkp7oa+8AmFothxWJYVgzLtrBtG+z4xL7OD+eC0wkeF3aGB7J82Dl+7I7+HQal72W1xRTdR35zCq6n3sdctkHjUkRaOQUUERGRA+ByedMynNi2TSQSIhoJx++jESwrhmFsCxo2297c27YFgPO92fHxHLaN7XJixGLxcwIh7Fw/dpc8rEO6YPcowOrZeYcWizQJKZecjGvSFMy1WyCkkLI7xx07jsMO68/KVSv56KOpqa6OyC4UUERERPaTy+WhQ276hJNoNEw4FCQUDhCNhDEMsyF8bNN4e+clEYFgGABjpzf4RnkNRnkN5uK14HNDMIzdozPWwIOx+h+M3b1Top/OfoleeSquv7yKsbWq4blIY8OGj+Dkkybw5ZefK6BIWlJAERER2Q9Op4sOuQWprgaxWJRQsI5gsBbbtrCs7QFk53CSMIH6ELNmM47N5Tg+mIudnYE1sh/W0EOx83OSc919FLn2Z7jueCG+6GMglNK6pJuMjEyysrIByMz04/V6CQaDKa6VSGMKKCIiIs1kmo6Ud+sKhwIEAjVEIqH4OJJU2dbiUlqF4+vFOD6ah9WrC/Zxg7AG9mx06vZOZU1L2HHDIHrtmbjufWnv9W8njjjiSC74xa8YNOjIhm5+Q4YMY/L/3mHx4iImv/4aX301ba/lDBk8lHvvfbBhMgjLsvjRKT9oVl26dOnKaT89kxEjRtG5cxci0QgbN25gxvSveOPNyVRXV+3yGI/bw4QJp3LssWM5+OBD8Hq9VFVV8f33S3j3vbeZPv2rZtVB0pvR85DeKfytJiIi0vrk5nXG6UzN2hvBYC2Buur4IHcrSS0kieB1Y+dlYY0fgjW8bzw5bEsQLXRvrC3B+dBrYLXvtzqn/fRMLr/8d3udYW76jK944IF7qKurbfK435/F4489TadO28N5cwPK2LE/4NprrsfjaXqdoNLSrdxy659YsWJ5w77u3Qu58477OOigbrst98MP3+fBh+5PbVhvR7yeHJYsmbvHc4YMGcKkSZNwOp24XK6Gm9PpbHRzOBw4HA5M09x+a6HnISIi0urFV4jvmJJwEgoFKC8rpqamon7AexqHE4BgGGNjKY7XvsB5/8uYC1bGu1ztuI5JkrftHgXEfvlDcDlS/WqkzIABAxvCSTQa5a0przN//jwAVqxYzpdfft5w7uijjuH662/cbVlX/f7qhnCyP90H+/Xrzw3X39QQTsrKy5gx42sWLVrYUF7Hjp2484778Pl8APj9fu695y8N4aSurpYvvvicd9+dwuLFRQ1ln3jiyZz0w1OaXSdJT+riJSIisg8Mw8CXkYXHk9Gi141GI9TWVBCNhtM/lDQlEMIIhHA8/zFmzyKs04/G6tapvitW/JSdu25t76p14MftoX2wVm7C/HoRRKIJfWqtwWmn/ayh5eSuu//M9OlfcfXV13HkkUPYtGkDd971Z4YPH8mtt9yB2+1h3rw5TZYz/gcnMnZsvKXknXff4tgx48jOzm5WXX554cU4HPGw+Pnnn/Dgg/cTCsfHCA0ccAT33PMXPB4PHTt24sQTT+att17ntNN+RkFBZwAWLy7illsnNuoCdvrpP+Pyy34HwNixx/P+1HeaVSdJT2pBERER2QdOp5vMzJYd/F1XW0V5WTHhcLB1hpMdhSIYS9fh+L9XcUz+Kt7SsWOrR33LR6PtBB23zjoOO9ef6lcgJXr36gPEWx52N05j9uyZPPTQAzz00P28+ebkXY4XFHTmyiv/AMDGjRt44olJza6H0+lk8OChAIRCQR7+618awgnAwqLvmDLljYbtw/sPAKBDTge2bt2KbVvc/8Ddu4xP+WLaZw1fZ2S07IcHkjxqQREREdmLbV27Wko0Gqa6qhzLaoOf+McszFlLsVduxDr3eCjMbzTZ8bYhJInetq46Hccdz0O4Db6me7At2Pp8Prp06Upx8aYmz/vs80+a3G8YJtdfdyOZmZlYlsX9D9y1X7N+ZWVlNbSerFu3lkCgbpdzVq1e2fC11xvvBvbIo3/lkUf/itvtJhzeddroH4z/YcPXi5csana9JD0poIiIiOyBYRhkZeVhmi0zjiEYqKG6urxFrpUyNQGMmgCO+1/GPnU01knDG3XRMiDx27l+rPPHYz73IcRaeWtUMyz7fimFhT0wDJMH//IPXn7lhYZphvfFWWedyxFHHAnAiy8+x5Ili/erHtXV1YRCQTwe726v371b94avN27a2OjYtnBy2k/PpE+fQ/H5fPTs2Yvu3QsBKC7exMsvv7hfdZP0o4AiIiKyBy6XB4+3ZbqO1FSXEww2PYNSW2V8PA9zSwXWBSc02r/zXEwJ2R7eF3v+SozFaxrWcmnrXnrpPxxz9LF4vV46derElb/9fw3H+vXrz4UXXMTceXMoKvpulxmwevfuwy8vvAiApcuW8OJ/n9/vekSjUS6/4hLc7vgEE4ZhNLpeTnYOEyb8tGF7x65bOxo8eAijR49ptG/5iu+5+ebrKS9v48G+HdEYFBERkd1oqa5dtm1RUbGFUKiu/U2TWhvEKFqD+fc3MIKRRuNHknGzz/9Bu5p2eO3aNVx77e8pKlq4y7H8/ALOP/+XPPiXv/Pkk881jBEBcLvd3HD9zTidLkKhIA/cfzexWOyA6rJx4wZWr17F6tWrGv2ce9webrnljoZB99Omfdqs7lp9eh/KnXfcT05OhwOqn6QPBRQREZEmGIaJPyu3YVG7ZInFopSXbSESDrX+gfD7q7oOY+1mzIdeg7Lq+L5ta3Yk+j7Di33OOHC2n6mHl6/4nmuu/R2/v+pynnn2STZu3LDLOd27FXL3XQ8waNBgAC655DIOPrgnAE/865+s37AuKXXLzs7m3nsfbOhGtmLFcv76t7/s9vzbbr+Zk04ex+lnTOAPV1/J7NkzATj00L7cOPHWpNRRWp4WahQREWmCy+WmQ27npF4jFotSUb4FyzqwT6bblJxM7KvPhC55YNv1waJ+4uBt2zvf78dx464XYMPWVD7TlLn66us4+aQJzPt2Ll988RnnnHM+nbdN5btkEddc8zvefefjvS7s2JRfX3oh69at3adze/fuw59vvYvOnbsA8XAy8cY/UllZsc/XczpdPPH4M3SrH79y6W9+xdq1q5tdb2keLdQoIiLSwgzDwO/PTeo1FE52o7IW4x9vwJY9vEnd2/vmfThuX3gimM1/A96W1NZU8847b3HVVZdTWxsf+3RYv/44nc79CifNccIJJ/HwQ482hJMvvvica679XaNw0qlTPhdf9BvuvusB/nzrXU2WE41GWLp0+8D97t27N3metC5tapB85rkXNLm/9qX9H9QlIiLtj9vjw+lK3mrxlhWjsqJE4WR3SqswJr0J1/wMcratX2I0Xp3xQLd7doEjDoFlGyCwfT2OtiYvryNnn/VzHn/i0d2Ob6qoKGfdujUcdtjhGIaBw+HgrSmv77HcE044iQxfBtFohHffe7thf01N9R4f53Q6ueyyKzn1J6cDEIvFeOrpJ/jf/17e5VzDMDjnnPOA+HTJu5smuWv9KvPAfk2BLOmnbQWU+h/inSmgiIhIc/j9yR1sW1lRQizWvtbjaLaqOvjnFLjh3O3jR0jw/c9/AH96MpG1TiuH9OzFnXfeR35+AbFYjH89+c8mz8vKyqZHj4MBKCsvIxAI8Oijf9tj2aNGHU2GL4NQKNzoXKfTxa9+eUnD9jczZ7B4cREAHTt24pabb6d//SKMpaVbueee21lY9F2T1ygp2cLChQsYOHAQpmly0423cfvtN7G1dHvXvFNO+Qn9DzsciIeTbdeS1q1NBRQREZED5fVlJnXNk8rKrQc8G1K7UBeCTWXw2NsYvz01OdfIy8Y+bhDMXALBtjft8MCBg8jPLwDgZz87hx49evDfl15o+Pk2TQdHHHEkl156BRkZmQB89NHUA7qmy+Xk5z/f3qOlsqqSxYuL8Pv9PPrIv8jN3d51sqysjJ+f13Tvl2AgwJ13/ZnHn3iUhx96FKfTSd++/fjXk8/x3YL5VFVV0qtXH3r37tPwmP++9B8CgcAB1V/SgwKKiIjIDjIzc5JWdl1tFZFICNtup7N1NVcwDEWrsV/9DOOscduGuif2/tSjsactaNGn1VKmvP0G+fkFDd2kRo4czciRoxuOH330GI4+evuaIitXruDFF59LSl28Xl+jcALxmbd2Z9uYmGXLlnLHnbdww/U3k5mZSYYvg1GjRu9y/ltTXufll19IbKUlZRRQRERE6nm9yWs9CYcC1NVVtb91Tg5UJApfLIR+PTCO7J3oTl4YOZnYxwyEud+3ybEoTz/zBAuLFnDRry6lV6/euz3v088+5h//eCgtWyC++WY6v770Qk79yWmMGHkU3Q7qjsvloqKigkWLvmPK22+yYMG3qa6mJFCbmma44PX3mty/5fQftXBNRESk9THI69gFhyPxn91ZlkV5WbEGxR+IDn6MP1+IkeXb7WzC+32/uQL71mehjXe9699/AAMHHMHJJ0+ge/dCNm3ayNvvvMn06V+xYcP6VFdPWhFNMywiItICXC5PUsIJQG1NhcLJgQpF4PkPk7PCfOdcjEO77b0OrdzixUW8+tpLLCyKd2lbseJ7XnvtZYUTSTsKKCIi0u4ZhkFGZlZSyg4F6wiF6pJSdrsSCGEv34g1cwkQHz/ScG/stL0fx5kwql2tLi+SzjQGRURE2j3DMHG7vUko2aampkLjThKlqhb7xU8wh/cD09x1QcbdDTTZh+PGgJ7EMrxQVZvIGqelhx/+Px5++P9SXQ2R3VILioiItHu+jOS0ntTWalB8wkWiWC99mpSizZOGgzd5C3SKyL5RQBERkXbOwOvNSHipsViUutoqTSmcaMEw9sdzMUqrEj4WxTzqcIhqrJBIqimgiIhIu+Z0upIytXCgrhrD0J/ZpPB5sF7/MvHl5mVhdM9PfLki0iz6zSkiIu2a15eZ8DJjsSiBQI1aT5IlEMKasRhKKhPfinLcIPC4Uv0MRdo1BRQREWm3DMPA4/ElvNxAoCbhZcpOLAvrnRkkcMnG+L9DDoWYgqVIKimgiIhIu2WajoR377Jtm6ACSouwvlqIEY7EWz/MnVpDzN3c7+W4mZeNkZed6qcm0q4poIiISLuVjKmFg8G2P01t2sjwEPvyu8b7dp5aeGf7cNwY3PtAaiUiB0gBRURE2iXTNPEkYfauQF2NphZuKVV1WJ/PT/g4FMeIw7Roo0gKKaCIiEi7ZFkWLpcnoWVGo2EsK5rQMmXP7K2V2Gs2xzcMIyH35mE9wNI4FJFUUUAREZF2yeFM/ExNoVBArSctrTZI7IsFGIYBe2kZac5xs0fnVD8zkXZLAUVERNold4JbT0DjT1LFmr+i0ba9m/vmHDcH9UpU9USkmRRQRESkXXIleIB8LBbFVreglLAra7E3lzdqBTnQe6PXQeBuX+uhXHzx5fzwxFNSXQ0RnKmugIiISEszDANngrt4RcLBhJYn+87wuLAWrcbRJW/7Phq3jDR323FwZyJtrLve9dfdQnHxJp57/slG+03TZPjwo+jd61ByO+SxZu1qli5d1Ogch8PBM0+/zFVX/ZqKygp+cf5F/OhHpzY6JxQKcvElP9/lumPHjufUn5xJXl5HVq9ZyQv/eYblK5Y1HHc6nZx11vkcd+zxuN1uFiyYx7P//heVlRW7lDVhwmmMOWYsE2+8mv79B3LzTXfy+6t+TVlZabNfjzPPOJdjxozlmmuuaPZjJbkUUEREpF1yOBL7JzASCWn8SYrYlbVY81fgHD9s+04DjJ0SSLO2u+S1qYHyGRkZDBgwiLemTG603+12c+PEO8gvKCAaiZKRkclNN97BzJnTmfTPh7EsC8MwGDf2BByO7TOb+f1ZTJv2CV99Pa1hXywW2+W648adwMUXXc6bb77GqtUrGHPMOG688XZuuulaNhVvBODSS3/HoCMG88qrL1BXV8upPzmTGyfezs03/5FINLLb51RWVsr7U98mGAzs12uyfPlSYrHtk1q88J/XuePOG1m6dPF+lSeJo4AiIiLtTqIXZwQIqwUlpaxl6xNeppGXjV2y66f4rdGQISOoq6vdpWXkhz+cQEZmJtde+1vOO+9XrF+3lpUrlzNx4m0MGDCI4477AaNGHt0onAD4s7JYMH8eCxfO3+01DcPg9NPO5p133uB/k18CYN682dx5x/8xYcJpPPnUJDp37sKYY8Zy3/2389133wKwZMki/vbXxxkxcjRf7xCAdrZ58yaef/6p/X1JmL9gHvMXzNvvx0vyaAyKiIi0O4mewcuyYmo9STG7LgiVtbtfMX5/tjvnpvppJczIEaOZNXvGLj+nhYUHs2RxEcHg9oC9fMUybr7lOr777ltefPFZ/jTx//HYY39r9LgsfzY1NdV7vGafPn3p1CmfTz/7sGGfbdt89tlHjBp1DACjRh1DaWlJo6BTWVnBnLmzOKr+nN3pdUgfXvjP62RkZALwxz/exGW/+T1X/+EGnn3mZe6/72907XIQp556Jo/9899MevQZTjppQsPjJ0w4jXvveZjOnbvywn9eB+DWW+5h0qPP7PG6knwKKCIi0u44EtyCEo1GMAz9SU0lw+/DWrelfoOE3DsO7Z74iqaAx+Nl0KAhzJo5fZdjmzauZ8SIoxg48EjMHX6GN23aAEB5eRkbN25ga+nWRo/Lysqib7/+3H/f33jqyf/yxz/eRH5+46mZO3fuSiQSZsuWzY32r1u/loyMDLKysunSuSvrN6zfJTitX7eWzp27NPu5jhkzjgXffcvNt1xHXV0t99zzMAMHHMm9993G5Mkvc8EvLqFHj56NHrN16xauu/53ADw66WFuv+PGZl9XEku/TUVEpN1J9AD5WDSiFpRUMw3sjVsTuqK82SkHvO5UP7MDNmTwMMLhMIsWL9zl2PtT32H58mVM/NNtHH/8iYwZM47jx52Iay//RyzL4rB+h/Pqa//lLw/ejcft4aYb78Dl2v64rKxsampqdnnstpaX7KxssrKyqG2iJaampprs7JzmPlXmz5/Lxx9PZf36tbz33hTcbjfPPPs4a9as4qOP32fr1hL69evf6DGxWIyNG+OBrLS0hM2bNzX7upJYbWoMypqZ/21yv6+F6yEiIunLMIyEj0GJxaLYdtsZUN0a2WXVWKsS+8bSyM0Cq/UHzxEjRzNn7jdNDmIPBgM8+NA9dOnSlYt+dRl5eZ248MJL+NEpp3L77ROprd01YAD88brfYRhGQzB/aPVK/v63JxgyeDgzZ8Vbaqqrq/D7/bs81u/PAqCquorq6mo65OY1eU5VVWWzn+vmLcUNX1fXB58tO+yrqanG59U7w3TXpgJKyNf0Hwf9GIqIyHYGRhICSjrp2rULw4YOJS8vl1AozLp165g9Zy7hcLjhnIMP7sERAweQ06EDkXCYTZuKmTvvW2pr44tNer1exo49lo55HVm9ZjUzpn+DtUMr0fBhQyks7M6bb72NlSazXdll1fG1TBLE6ODHcJi7LOTYmrhcLgYfOYxHHvnLHs8rLt7E5i3FzJkzkzlzZ3L3XQ8ycuRoPv30w13Ozc7OoWPHTqxatX2BzECgjuLiTXTu3LWun+zJAAAgAElEQVRh3+bNm3C53BQUdG7Uzauwew/q6uqorq6iePMmBg4c1CjsAHQv7MHmzduDhbQv6uIlIiLtjmkm9s9fU59Mp0r37t046YcnEolGmDNnLmvWrKFPnz788MTxDW/e+/TpzfHjxlJRUcnXX01nwYLv6NSpExNOORm3O96lacjgI7Esi/fee5/u3brR85CeDdfIy8vj8MP78/XXM9ImnADYpVUJLc/w+7Bj6fP89segQUOwbYuFCxc0efyYY8aSkZHRaF9p6VbWrl1NboemJwk4+ujjuOH6W/F6ty926vf7Oeigbmwp2R4qli9fxtatJYwde0LDPsMwGDfuBGbM+BKAb775iry8TgwYMKjhnJycDgwbOqLhHGl/2lQLioiIyN7ZCR/QbtvpE1COGjWKjRs38cGHHzXsKy+vYPToUXTrdhDr12+gX9++rF27ji+/+rrhnHXr1nPWWWfSo7CQ5StWUNC5gIULiyivqGD9hg0UFBSwcuUqDMPgmKNH8/33y9m8ZUsqnuJu2dV1e2hB2Xkpxr0fNzO9e3lM+hs58mjmzZvd5HoiLpeL8877JaOPGsOjkx5u2N+tWyF9+vTdZc2Ubb788jNO+dGpXHfdLUyZMhnTNDn9tLPZtGkjs2bNYOzY8YwbO56HH76PyZNf5pJLriAWjbBq9UrGjBnHQQd145FHHwRg8+ZivvzyM357xR949dUXqQvE10HZvLmYWbNmJOdF2Y26ujoGDx5OJBJhzZpVafXBQ3ujgCIiIu2KbdsJ7Qa0rcx04PV48Pszmb+g8afla9auZfToUWRnZwMbME2DYKjxui2hcLjRa2MaBnZ964hl2Zj1+wcMOByfz8vsOXOT/4SaKxx/E24DhgG2vT12GIa90/bej+NxQytuQXE4HAwdMpzHn3ikyeORSIQH/3IPV1/9J/7+tycIh8MMOmII5533Kz797MPdrnFSU1PN3ffcynk/v5ArLv8DDofJzJnTefG//8ayLDp1zKdv3/44XS4+n/YxpsPkJz85g1NPPZNVq1dy9z23Uly8fbzQU0/9k7PP+QVnn/2L+pXk5/Lsv/+1x0Uak+GNN1/ltJ+excknTeC3V15MIFDXoteX7Yyeh/ROj9+qCVBxxy+a3N/h1v+0cE1ERCSd5RcUJrS8ki3rElre/nI4HJx/3rnMmj2HxYuXNOzPze3AT0/9CdOmfcHKVavp168vo0aOYMY3M9m4cSNut4chg4+kU6eOvPHmFEKhEGPGHI1pmHz19XR+euqPWbiwiE3FxfFyvviStWvT4zk3Yppkv3pbPGjUj2loCCL7sx2zqD7ntviOVujII4fyh/93PZddfmGj8Uc7c7lcDBs2ih9POI3S0q1MmTKZ5SuWtWBNpbXxenJYsmTPH1IMGTKESZMm4XQ6cblcDTen09no5nA4cDgcmKbZcFMLioiISBsRi8V46eVXdxkXMmDA4USiUdZviE+lunTpMnJycjh69FEN54TDYaZ+8CGhUAiAb79dwAnjj+f8885lw4aNrFi5ivHjj2fDho3pGU4ALKu++aN+26j/x7D3b9thtNpwAhCNRnnm2Sf2GE4g3pIyY8aXHH74QNavW6twIimngCIiItKG7PxmtG/fQ+nTuzfTZ3xDuL4L1Ijhw+jX91AWLixiS0kJLpeLvof24YcnnsAHH35MaWkpNTU1vPHmFBwOB7FYjD59etOpYydef+NNehQWMmToYGLRGN98M5OSrVubqkpKGIYBhrFDRjEa/t2f7dasqKjpgfG78/TTjyWpJiLNo1m8RERE2qhBg47g6NFH8e38BSxdGv9UvFOnTgwYcDhffT2d2XPmsnbtOlasWMn7Uz+ksrKSo0ePalRGLBbD6/UyYvgwZs+Zg2VZHHfcGBZ+V8S6desYO/a4hvEpKWea2EZ8FMm2dg+7/t/92rbs+sEoItKSFFBERKTdSfSg9kQPuj9Qpmly7JhjGHzkkUyf/g3ffrt9sHOnTh0B2LSp8RoTtm2zaVMxHTt23CVwjBo1goqKCpYt+55OHTsSjcVYsXIlixYtxu/PxJ+162J8qWC4HLtZFd7cy6rxTR8nGgMzvb63Iu2BAoqIiLQrOy8Il5gy0+fPqcfj4aQfnkj37t344MOPWLqs8XiCbQsxHnRQ10b7DcPgoIO6UldX12hBxu7du9GjsJCvp8/YdiJW/fSrsfqxLmnz/D2u+i+Mne7Zzf69HA9FMBxp8txE2hGNQRERkXbHti0S+Rld/A166tdM8Hq9TDjlZLKysli0eAm5HXLI7ZDTcLy6poYN6zdQUlLC0aOPIi83ly0lJbhdbg49tDf5+fmN1kZxOZ2MPmoUCxZ8R2VlfBHEsrJSPB4POTk5ZGX5CQQCVFdXt/hzbYqRlZnQ1iy7LsSuIUZEkk0BRURE2hkDy7JwOBJXYnwgecuu2dAUf2YmWVlZABze/7Bdjq9fv4H16zcw9YOP6N//MHodcgj9+vUlEolQVlbO++9/QPHmzQ3nDx06hHAkwncLixr2BQJBZsyYyYkn/ADLsvjiy6/TZjV5Mztj7yc1g11dB04T9jwJlogkmAKKiIi0Mza2ldjWDtNMYNo5AFtLS3n238/v9bxoNMp33y3ku+8W7vG8b2bOanL/98uX8/3y5ftVx2Ry9O6GYRrbpxo+wHu7qrZVL9Qo0lopoIiISDtjYCU4oDidLgzDrO86Jqlg+Dw4unas3yAh91ZZNXYo9S1jiWLnZxPr1QW7ay74fdiaAKBFGZYNNQGMTeU4VhZjlFSlukppSwFFRETaFdu2iCU4oDicru2fvEtqmCaOwoKEjkGx1hTv/aRWIjqqL1a/bqmuRrtmmwZkZ2BnZ2D164a5dAPOb7QoZlM0NYWIiLQ70UhiBxU4na5WveJ4W2CHwkSXrye6dG3CumXFtlYmpJxUi55wpMJJGrL6dSN6wpGprkZaUguKiIi0O7FYNKHlmaajvotX6mfyaq+MmEV48hcEX/0MOxDC0T0fZ5/uuI7ohePQ7jh7HbTbISfb7Lw/trr1t6BER/XFOigv1dWQ3bAOyiM6qq9aUnaigCIiIu1OosegADhdbsKhQMLLlX3jcnvJyagfg+K3CFTWEZm+lMCsJVixGHYkiqOwAOeh3XEN6o2zT3cchQVNlrUttFilrbsFxc7PVstJK2D164atMSmNKKCIiEi7FItFcTgS92fQ4/YSjYSTEn5k7zxu7/YNw8Tn8+PzbV/h3rJihLbWEd5URGDOMqzaANg2jh6dcR5aiOvIPvGWli552IBVXIZhGq16WFGsV5dUV0H2UaxXF5wKKA0UUEREpF2KRiMJDSgutxe7pnV/4t5aGYaBa8eA0gTTdODLyMJHfJ0YcnKIRqOEN9UQXjOPyPQirGAIAEfPLhhZmcmudtLZXXNTXQXZR/peNaaAIiIi7Y5hGEQjYTweX8LKdDicmKZJTOtmtDjTdOxX2HQ6nTid2WRkZsd3uCEaDRNaW0EwuCHh6+W0OH/ifr4lyfS9akSzeImISLtjWRbhcDDh5br38im+JIfHm7gV5J1ON5n+HAyz9b9F0jonrYe+V421/v99IiIi+yEaTexUwwBeX2ZC1+GQvTMMI6EtYdvEom1ngUaR1kYBRURE2iXDMIhEQgkt0+l0Y5rqPd2STNOJ0+lOaJmRSKh9Bk2zkGsueYQV11/B2Yl9SdOEi1NPe5gV11/Lr3OArj9m6vWPMO/UwXj2p6wzHmbF9TdwZUfALOTssaczcfAh+1GW7EwBRURE2iXbtpPSzcuX4cdsA92DWgPDMPBl+Pd+YjOFw0FsLbzZ9kUDrK8sZV3t/rWWhepKWVdZSlUUcHZh3LDx/PqwQgWUBNDHPCIi0m6FQwEyM3MSWqbXm0ltTUVCy5Td83oTP9tWKFiX8DIlDZV8zCWPf7yfD44w9f27mLpts022OKWOAoqIiLRbsVgUy4phmo6ElWkYBh5vBqFgANvWjF7J5PX5E94Vy7JiWstmm84/5q1fnkzhhtlMd/ZlXGcfofIiHn3/RZ5cFw9x+T3GM3HscRzfORtPqJTpi6Zy7+ezWN4x/tiC5R/zvmcwZ3cr5aF//p0n3YO5ZvyPObtnF/KtKuYue597P5rG3CBABkOHnc7EkUcyIMtFqHIVb37xCrctKgYgu8d47hs/nnH52RAoZu7303ho2jTm1gG5eyp3z8+vYOE/GfvuMk467QEe7lvKZwur6N33EArNAHOLpvBkWT+uHHkkA3wR1q39hIlvv8/0OhennvYAD/ddz71Pf83oX53POCfQ82zm/elkXnntz0xcrnFM+0tt0CIi0q6FkrD6e0ZGtsJJC9hxIcZEScbPQ2uXXXgkh0dX8v7qUug4mIk//AEDTKDzyUz62en8tGOEuUtm81lVBuNG/JJHj+/b0M0pv+94LujmY0t5KVucfZl4xi+5soeL5Uum88rqSvocfjaTfjiCfCB/4Hk8deJo+kRX8f6SZWzx9eWCk0/ngizAO5jbfno6J/lLeXPOx7xZ4mLo4NO5bVghuPdcbrOYXRjXx8eKlUUURbMZOuR8Jo0/HM/m+UwtiVLY68fce2zfxt24rGI+m1/EOguoXMYr875meqXCyYFQC4qIiLRbtm0TDNQm/I2uw+HE680gqK5CSePz+RO60OY2wUCNxp/sJLThbS7478esoyNXnn8713TtxVCfi/xhxzHUXcorrzzExJV14CzkyjPOpntpZPsb+NoibnvpSZ4viZA96Aruy48w/eMnuXpJJZDBIvMabus7mpMzZ/FZ3Spe+baYuV+8zdRaF0PH38CrIwo5PN8F1d3p44OSZdN46PNZlESn8mrfQyhZvo7sgVdwwR7Kfb62Oc+2ilfefYiJyyPkD7uKz0/sS9WiFznrrW+pyhjNU5efz7guvSg0V21/SGwVz38+m9FHDKCw/FvunToNrQl/YBRQRESkXYtGwwnv5gWQkZmjgJJEvoyshJdpWTGi0WjCy23tqkqL2WIBVNUPKHfhcXdhaOdsqJ7O1LX1P+fRdTz6yoPxrzsPiD92zXReKYk/ZkDXLnjIYPT4G5gxfscrdKR7not1q2cxNXs8v/jJDUws6EJhhguoivf3KS9ieul4fn3YL5nR63SWb1rJ9OXTeWofyqU5g+CtUlaUxM+vqq4iBGwpLY4HjnApW0KA06mB8EmmgCIiIu2aYZgEg3VkJPgNr8PhxOfzEwoFNKYhgQzDxJeRpNaTYB2g1pM9ambPxdDO68lYxbz50dtM3TG7W3WsL3Ux7sRrmTQkmy3rivhs3mzWZ41m4qD6RTijq7j3hYeYe8RwxhUWMrTbAC44eDDjOkzi1j2Wm+CuVuq52SI0BkVERNo127YIBmqSUrY/q0NSym3PDMMgMzM7KWUH6qqTUm6bZBUzd3MVZPXlpB71IcJZyDXn3sCk0X3Z9TsUYfnmYkJmRwo9xXy25FumLilifdYATuqWA3TnpF4d8ZRO45r/PsltX0xj7o7LFLm7cPawQyiZ/zoTJ/+dk/71IlProLBLV6r2VK4axFoltaCIiEi7Z1kxwuEgbrc3wSUb+P0dqK4u07iGBDCM+OsJiV9EMb72iT4e33cRPpszjbmH/ZizT7uBwpWrCOX2ZVznbJaXQlNLoJYs+phXRvTlgmOv4dVui1hOZ47uVUj2plKe+mId62si0HUw15wIRa5CfnpYFwBGHz6YH3Q6jtuOPQQGDub9lesI5QxgdAYsL1rJ4kXrdl9uSwUUq47KCHDQeCadWshnC17hydUaKL+/1IIiIiLtnm3bBGqT8+m5x5uB2+3FMPQn90B5PBl4vBlJKbuutlohsrk2v88lr73OmyUwoM9wRvvr+GzWv7ny02VNBhTCy7jttX/z6MoqCnoM5+TuOWxZ8jaXTH6fomgxT370enx2riPHc0Ghi0+/eptXNpXize1Oxdx/ccmn05lrdeHkIeP56UEuiub9mys/X0VoT+W2VOaMLuPV2UWsszoy+rDhjO7gaqELt01Gz0N6t5n/jRV3/KLJ/R1u/U8L10RERFofg7yOXZIytsGyLMrLijUW5QCYpoPcvC6YZuKDXiwWpay0mLY2/iR84fGproI0g/u5T1NdhX3m9eSwZMncPZ4zZMgQJk2ahNPpxOVyNdycTmejm8PhwOFwYJrm9lsLPQ8REZG0ZhgGtTWVSSnbNE2ysnITvqhge2EYBllZuUkJJwCBQA1tLZyItGYKKCIiIsQHy4dCdUlr5XB7fPGVz5P0JrutMkyTjIxs3B5fUsq3rFibHRxvWApdrYW+V43pt6SIiEg9wzCprU1OKwqA398Bl8uj8Sj7yDBMPG4vGUmatQva+MxdNYFU10D2lb5Xjeg3pIiISL34lMO1SR0rkpPTCYfTmbTuSm2Jw+EgK7tj0sq3rBh1bTigGJvKU10F2Uf6XjWm344iIiI7MAyTmpqKpF4jJ6eTWlH2wuFwktMhP6nXSPb3OdUcK4tTXQXZR/peNabfjiIiIjuwbYtwKEA0Ek7aNUzTQU6HfEzTkbRrtGYt8fpEI2FCwbbdrcYoqcJcuiHV1ZC9MJduwCipSnU10ooCioiIyE5s26amJrldLhwOJx1yC5IyrXFrZpqOFnld4t/ftj8w2fnNMsyNZamuhuyGubEM5zfLUl2NtKOAIiIi0oRoNEIwWJvUa2zrxuR0utTlC3A4XC0SToLBWqLRllpiPPWcH81XS0oaMpduwPnR/FRXIy3pYxsREZEm2LZNTXU5Ho8vqeFhW0tKZeVWopFwu1zN3DAMnE43OR2SPzbHti1qqiuw7ZZaYjw9OL9Zhr2ymFivLthdc8Hvwza1Lk9LMiwbagIYm8pxrCxWt649UEARERHZLYPqqnKyc5I3kxTEB+Z36FBAdVUZ4XCwXa04bxgGXm8m/qzcFrledVV5uwsn2xglVTj1plhaAbUni4iI7IZtW4TDAULBuha5XlZ2HplJXPMjHfn9HVosnISCdYTDbXtgvEhboBYUERGRPbBtm+rqMlxuT4vMuuX1+XG63FRXlROLRdpkly/DMHA4nGRl5+F0ulvkmpYVo7q6rE2+niJtjVpQRERE9sIwDKoqS1vsek6nm9y8znh9/ha7ZkvKyMgmN69Li4UTgKrKUoUTkVZCLSgiIiJ7YVkWth2mprq8xbojQbz7k9ebSW1NBdFopFWPTTEMA5fLQ6a/A06nq0WvXVtbSTSavHVtRCSxFFBERET2gW3bBIO1uNwePJ6MFruu0+kip0M+oVCAutpKYlYM22o9g7wN08RhOsjIzMHj8bX49UOhOgJ11Wo9EWlFFFBERET2kW3bVFWWkpvnbNHuSQAejw+Px0cwWEugrhrLimGlcVAxDAPTdJKRmYXXm5mSOkSj4RbtmiciiaGAIiIi0kyVFSXk5nVpkUHzO/N6M/F6MwmHAgQCNUQiIcBIi6lz42uY2LhcHnw+P+4UtJhsY1kxKiu2puz6IrL/FFBERET2Q0X5FvI6dk3Z9d0eH26Pj1gsSiBQQzgUqB8r07JBxTBMDAPAwOfz4/FmJH0l+H1RWVHSqsfsiLRnqf8NIiIi0spYloVhxENKh9yClNbF4XDi93cAfwei0TDhUJBQOEA0EsYwzIQHlm0rvdu2hdPpwuPJwO3xtniXtz2prCghGo2kuhoisp8UUERERPaDbVtEoxEqK7eSk9Mp1dUB4tMTO51uMjKzsW2bSCRENBKO39fPAmYYBmBgGGDbNBlgtrWKxMeV29i2jWk6cLni5TtdblwuT31Z6aWycivhcDDV1RCRA6CAIiIisp9s2yISDqZVSNnGMAzcbi9ut3eHvTaxWAwrFosPsret+OxWto2NjYEBhhEf4G6YmKYD0+HA4XAA6RdGdlZZuZVwSCvFi7R2CigiIiIHwLbteEipKCGnQ36qq7MX8RXc02GMSKJVlG+pnzBARFo7rSQvIiJygOLdqcKUlxVrvY0WZts25WWbFU5E2hAFFBERkQSwbYtYLEZZ6SZisWiqq9MuxGJRysuKtUq8SBujgCIiIpIg8QHnNuVlxRqonWThcJDysmKFQZE2SAFFREQkgeJrkdhUVmwlEKhJdXXapECghsqKEnWnE2mjFFBERESSwqa2poKqqtJUV6RNqaoqpbamItXVEJEkUkARERFJEtu2CYeClG7dSCSicRIHIhIJU1a6iVCwTi0nIm1c25tnUEREJI3YtoVtx6fBzfRnk5GRneoqtTo1NRUE6moABROR9kABRUREpEXY1NVWEQrW4c/Kw+Vyp7pCaS8SCVNTXUY0Gkl1VUSkBSmgiIiItBDbtolGI1RWbMHjzcDvz8Uw0n+F9pZm2zY1NeUEA7WproqIpIACioiISAuzbZtQMEAoGMDvz8Hr86e6SmkjGKihpqayfspmEWmPFFBERERSYNsb8JqaCurqqsn0d8Dj8aW4VqkTCgWoranEsqIaBC/SzimgiIiIpJBt28RiUaqrSqk1He0uqIRCAepqK4nFFExEJE4BRUREJA3sGFRqDJPMzBy8vsxUVytpgoFa6uqqsKyYgomINKKAIiIikkZs28a2Y9TUlFNTU47P58fjzcTpdKW6agcsGo0QCFQTCtYBKJiISJMUUERERNLQtjfvgUAtgUANpunAl5GFx+PDNB0prt2+s6wYoVCAYKBG3bhEZJ8ooIiIiKSxbYPpY7EotTUV1FSX43S68XozcHu8OBzp17ISi0UIh4KEQgEikTBaYFFEmkMBRUREpJXY1voQjYaprY1SW1uJYZg4nW48Xh8ulweHo+X/tMdiUSKREOFQkEgkWN9NTaFERPaPAoqIiEgrtK1lxbZjhMMBIpFg/REDh8OJ0+WqDywuHA4npmke8DUtyyIWixKLRYhEQkQjEWKxSH09FEhEJDEUUERERNqA7QHBJhoNE42G6wejG4CNbYNpmhiGgWk6ME0HhmFsX8neMMC2MQyzYWYty4rVf21hWTbxUw0toigiSaWAIiIi0kbFQ8v2lg3LigHxLln7Vx5oPImIJNuBt/eKiIiIiIgkiAKKiIiIiIikDQUUERERERFJGykPKAUFBTgcrWfBKRERERERSZ6UDZI3DINu3Q6i58E9KM7OYsOGjQQCAfLycunTpzcA4VCYhUVFRKOxVFVTRERERERaUMpaUPof1g+P201tbR11dXUMHTIYt9vFgMP78+23C5g5cza1tbUc3KNHqqooIiIiIiItLGUtKP4sP+vWbyAjI4PSraUUF2/GMAwikSjhcBiAuro6srOzGx6TkZmzxzIrdrN/b48TEWlr6morU10FERGR/ZKyFpSlS5fRr++hdOrUid69e5Hl9xOLxZjxzUwAXC4nBx/cgw0bN6aqiiIiIiIi0sJS1oJSWVnF7DlzOWLgAMrKyxk48HDmzZtPXSBAhs/HoEFHsHz5Sqqqqhses7+fCOqTRBERERGR1iElLSimaTJwwOENs3eVl5VTU1OL2+OmY8c8Bg06gqJFi9lSUpKK6omIiIiISIqkpAXFsiwqKisZNXI4LpeLQYOOoKq6mlAwxJgxR1NXF+Dw/ocBUFNTQ9GixamopoiIiIiItLCUdfFav34D69dv4MhBR7B02fcEg0EAPvzok1RVSUREREREUixlAWWb+Qu+S3UVREREREQkTaR8JXkREREREZFtFFBERERERCRtKKCIiIiIiEjaSPkYFBERkaa4XG483kzcbi8Oh/5ciUjbFItFCYeDhIK1RCLhVFcnLeg3voiIpB1/Vi4et49AoJqqyq1Eo1HATnW1REQSzMDpdOJ2e8nO7kQoHKCmujzVlUo5dfESEZG0ktMhHwOD0tJN1NVVE41GUDgRkbbJJhqNUFdXTWnpJgwMcjrkp7pSKaeAIiIiacOflYsVi1FdXYZCiYi0LzbV1WVYsRj+rNxUVyalFFBERCQtuFxuPG4f1ereICLtWHV1OR63D5fLneqqpIwCioiIpAWPN5NAoBq1nIhI+2YTCFTj8WamuiIpo4AiIiJpwe32Eg4HU10NEZGUC4eDuN3eVFcjZRRQREQkLTgczvrZukRE2rdoNNqup1dXQBERkTSi7l0iIu39d6ECioiIiIiIpA0FFBERERERSRsKKCIiIiIikjYUUEREREREJG0ooIiIiIiISNpQQBERERERkbShgCIiIiIiImlDAUVERERERNKGAoqIiIiIiKQNZ6orICIi0tLy8wsA6NSpgMWLF6a4NiIisiMFFBERaTfOPONc+vcfQP/+AxvtLynZwhdffMr/Jr+UopqJSGswcuQINmzYyIYNG1JdlTZNXbxERKTNy88v4Oab7uSMM86hU6cCJk9+mcmTX+bxJ/7BtGmfAHDGGefw14cfb2hdSYQ777yDt9+e0uSxcePGsvC7BXTMy+N3V17J1KnvJ+y66WjunNmMHz9+v4835bF/TmLixD/t9vjIkSNYvKiILp0773KNiy+6iNdfn9zk44YMGczbb09hwfxvm1WffTVw4AAWLyoiKysrKeU3ZW+v1d7sz/enLbrn7rv597PP0K1bt1RXpU1TQBERkTatf/+B/PXhx+nffyCTJ7/MH66+jP9Nfon/TX6JadM+4fEn/sHd99zC5Mkvk59fwE033rlLC8v+euftd+jdqxd9+/bd5diPTj6ZmTNnUlpWxvz583l98usJuWY6+N2VV/L000+luhoUF2/mueefp7aurlmPu+Xmm1n+/XJ+ccGFCanHc/9+lisuvzwhZUlq/fJXFwEopCSZAoqIiLRpZ55xDgB33X3LLl24trWWlJRs4X+TX+IPV19Gfn4Bl/3m9wm59sxZsygpKeFHPzq50X63283xxx/PO++8C8AXX37JY48/npBrynZr167l3nvvo7q6ulmP69nzEKZMmcKCBQuSVDNprTZs2KCQ0gIUUEREpM2KjzmJt5zsPBj+5pvu5KYb72y0r6RkS0NLyplnnHvA1yiTXJEAACAASURBVLcsi/fef59TfvSjRvuPHTMGr9fLBx9+COza3ahv37489+9nmf/tPD768ANOP/00AH5x/vl8MHVqw3kTJpzCnNmzcDgcAAwbNoxv583F4/E0ut5j/5zEPXffzT/+/je+nTeXKW+9Sc+ePfnNby7l66++5MsvpnHBL37R6DFjxhzD229PYf6383jt1VcYOHBAw7GLL7qIN15/nd/+9gpmTP+aL6Z9zrnnxl+vBx64nyuv/C2jjzqKxYuKOPeccxoe16OwkP88/xxz58zh6aefoqBg1+50P/vZz/jqyy8anhPAiSeewNw5s/F6vbucbxomN944kTmzZ/HpJ59w4QUXNBxrbleqrKwsFi8qwufz8sgj/2D6118B4PP5uPPOO5g9ayYzpn/NH/94LU6ns9E1fvzjCXzy8Uf8+c+3Nirzs08/YcSIEVx11e9ZvKiIXr0OaTh27LFjeP+9d5n5zQxuv+22Rs95dz8DO2ruz4PP6+O+e+9h7pzZ/5+9+w6PotzbOH5vS2+0QCqdAAmBJFQ9HCs20KNi4SgCCvb+ir1hw94FKyqCeqx4joC90ARCQi+hhJbeSO+b3fePyOKSIKAJO8D3c117mcw8M/N7hpm4984zs5o792slJSa6re+qK6/UwgW/atXKVL326itq17btAffViBGna97cuVq7ZrU+/+xTxffrd0j7+GgRERHx+3Cu95u8pj7xhKsNIaV1EFAAAMes4cNPcV0daU5zT/BauOhnbdq0XsOHn9IiNcybN1/R0dHq27eva9pZZ52lxYuXNPvJfkBAgN57d4Y2paXp8rFj9fobb+rhhx5SfHy8Fi9ZoqioSEVGRkqShg4ZIj8/P/X7/c3h0KFDtGJFimpra5us97zzztWSJb/p4ksuVVlZueZ8+YWGDRumqyZO0mvTGu9P6B0TI0lKSkzU9GnTNOfLOfr3ZZfrt9+W6sPZs9W5c2fX+mJieikiIkKXj71CM959Vw8+cL+6d+umZ599Th999LFWr16tkaNGad78+a5lrr/+Ov3400968KEH1bVLV919151N6vz+++8VFBSkQQMHuqadMeIM/fTTz6qpqWnSfvToC2Wz2XTtdddr9ocf6s47J+vs/QLhoaqsrNTIUaNUX1+vRx55VJeO+bck6bVXX1HvmBhdd/0NumPyZI04/XRNmfKw27J3/N//6YNZs5oM1Rs3brzWrVuvD2bN0shRo5SRkemad/NNN2natOmaNn26LrzwAl00erSkPz8G/uhwj4cLLjhfFRWVuuvue7SnqEjPPfecK8xce801mjjxKk2d+qTGT7hSNptNs2fPcgtNeyUkDNAzTz+tWbNm6d+XXa7k5BWaPn26/P39/9J+N6KsrKw/DR5ZWdlubdGyeIoXAOCY1aFDqOsm+P09/sSDzU4vKMhXQUF+i92HsnbtWmVkZOrss8/Sxo0b5e3trVNPPUUPPfxws+1Hj75QO3fu0pNPPiVJWr9+gwYM6K8xl16q++6/X1lZWRo2bKg+++xzDRkyVKtXr9aQwYO1evVqDR0yRD/+9FOz6120aLH+88knkqSZH8zUyy+9pEcffUw7duxQWlqaJk28SolJiUrbvFmTJk3S13Pnasa770qSNm7cqLh+cZowYbweeeRRSVJZWZmmTHlE9fX1Sk9P18SrrtKAhAR98cUXKi4uVnVNjbZv3+FWwwcfzNL778+UJLVr205XXnllkzrLysq0ZMkSnXHmGVq2fLm8vLx08sknafKddzXbr23b0l01paSkqGPHUF0x9nJ98803f/rv0hyHw6Ht23fI6XQqNy9Xu3fvVmxsrIYOHaozzzpbmZmN4eK+++7XzJnv65VXXnUt+9RTT+u7779vss7dGRmqqalWSXFJk/3x8JQpSk5eIUkaNHCgEpMS9cmnn/7pMfDHYWc7d+48rONh1apVevz3T/8zMzI0Z86XiggPV1Z2tiZOvEpPTJ2qb3+/IvN/d0zWzz//pDPOOKPJvpw0cZLenznTdTxt2LBBp512qkaMOF1fffXfw97vRjV+woRmp0dEROjHH75XVlaW7rv//iNb1HGCKygAgGPSP/95qiRpU9qGA84/0DCuwsICSWqxkDL/m/k6+6yzft/ucJlMZv388y/Ntu3bp49iY/tq6W9LXK+R54xUdOdoSY2fmg8bNkxhYWEKCPDXrNmzNWTIYPn4+DReZVm8pNn17s7Y7fq5pKSkcdpu92l7PwHv3TtGK1ascFt+RfIKxfzhZv/snBzV19c3u/yBbN6y2fVzXl6e/P39mm03f/43GnH66TKbzTrhhGGy2+1asqT5fq3e70lby5cnN/tQgr+qd0yMcnJyXOFEklatXi2Hw6EePXq4pqWkph72ujdv3uL6OS8v37X/DnYM/NHhHA9u28vPlyT5+fsrPCxMgYGBWrEixTW/srJSGzZsUExM033Zt28fXTF2rFt94eHhio5uWt+xZv9wsjdgomVxBQUAcEwqKGh8A9ahffOPDb7wgsZ7I5ob/lVQ2Lhs4e///bvmzZuva6+5RvHx8Tr77LP164JfVV1d3Wxbp9Op+fO/0SuvvOI2fW8YWLx4iR59ZIpOGDZMyckrtHzZcj3+2OMaPHiw9uwpVnp6eovU3ITp76/C4XAeUrsff/pJjzwyRYmJiTrzjDP17XffyW63N9vW6XRfp8nUAoUegv23U1dXd9jrcDgczU4/2DHwR4dzPDiczW/vQA60L51Op5559lktXLDQbfrhPi3taDTz/fcIJ0cAV1AAAMekveGiT5/Yg7Rsqk/vxmX2hpy/a+vWrdq6dasuvPACnXzSSa6ndzVn46Y0xfePV35BgXLz8pSbl6d+8f3Up28fSdKyZcsUFBSkceOu0LLly1W0Z4+ysjJ1zdWTtHjJ4hapNy1tswYNGuQ2bfDgwdq8ZcsBlmhZVVVVWrBgoc4552ydeuopf7q/BgwY4Pb70CFDtKUF60zbvFlhYWGu+zwkKSEhQSaTSdu2bWux7fzRwY6BP2qJ4yE7J0fl5eUaNGjffT/+/v7q27ev21WXffVtUmzfWFdtuXl5Ouvss9S+ffu/3umjxLTp0wknRwABBQBwTCooyNemTevV/gBXUP5Mnz5xLRZO9po3b74uvugi2e12LVq06IDtvvjiC4UEh+ipp55UYkKCzjv3XD3x+OMKCgqSJFVUVGj1mjXq1auXli9fLklatmy5kpKStOQAw7sO1zvvvKNzR43SpEkTFd+vn+644/+UmJDgun/kYMrKy9Wta1clJSb+5Scczf9mvi65+GJVVVUp9U+GT8X06qWHHnxASUlJmjRposaMuVQzZ37wl7bZnA0bNmjZsmV68YXnNWTIYP1z+HBNnfqEvvrvf5Wff2jHSFl5uZKSkhQXF6uQkJCDtj/YMfBHLXE8NDQ0aMaMd3XXnXfq7LPPVmJCgl544XntKSrS983cV/POOzN04YUX6Lprr1V8fLxuvPEG3XLzLbI3c4XnWDNnzleEkyOAgAIAOGZt2rThsB8ZPPrCMerQIVSLFjV/j8hfNW/+fJnNZv30089/OhyosrJSV02cqLBOnfTee+/qlltu1gsvvqT58/fdqLxk8RLl5eVpx47Gm66XJyfL4XDot6VLW6TW1JUrdcONN+r888/X7NmzNGzoUF0+dqx27dp1SMt/++232rNnj95//z1dfPFFf6mGBQsWqq6uTvPmz28yjOuPvvrvf+Xt46O333pT4664Qs88+1yzN6v/HTfdfIs2b9miaa+9pmeeeVo//PCDpkx55JCXnz1rtrp3766PP/pI/eIOfl/ToRwDf9QSx8Obb72lGTPe1b333K0ZM95RfV2dxo69Qg0NDU3arl69WpMn36lzzz1XH8x8X6ecfLKuve46ZfzhPh3g7zB16dr90AaEHgVKHh3b7PSQh2Yf4UoAAIerQ2iUCvIzWnadv3/pYvv2oXrzrVebfazw/u1fevFNbdq0/oBP+cKR0bFjR/38048afdHFSktL83Q5wBHXGn8TW4qPd7DS0lb+aZuEhARNnz5dVqtVNpvN9bJarW4vi8Uii8Uis9m873WE+gEAwBFXUJCvN9961RVU/uxKyugLx+ilFxu/zf2LLz85UiWiGTExvfR/t9+mDRs2Ek6A4xBP8QIAHNMKCvJ12+3X6v77HtOFF16q4cNP0aZN67UpbYPrCV/Dh5+iDh1CXYHmYFda0Lree/dd7Sku1h133OHpUgB4AAEFAHDMKyjI1xNTH9Q/h5+qPn1i9c9/nur6npS987/88pMDfuM8jqwTTvyHp0sA4EEEFADAcaGgIN8VQDp0CHU93YurJQBgLAQUAMBxp6Agv8UfIwwAaBncJA8AAADAMAgoAAAAAAyDgAIAAADAMAgoAAAcR2668UZ99923Lb7ewYMHadPGDerUsWOLrxvA8YWb5AEAOI6sWbNGdrv9b6/ng5nva+nSZXr9jTckSbm5efpg1ixVVlX97XUDOL4RUAAAOI4sWrxYixYvbvH17t69W08++VSLrxfA8YchXgAAtCJfX1899tijSlmRrGVLf9PkyXfIat33+eDK1BRNmDBen336ieZ+/T9JUny/fvr8s0+1etVKffThbI0ZM0abNm5wLRMcHKznn3tWycuXafGihbr9tttc86668kp9NWeObrjhei1b+psWLVygMWPGuM2fM+dLSdKTU6dq08YNTV5+fn4ym8265eabtWjhAq1IXq6XX3pJQUFBkqRff/lZgwYN0i233KxNGzeoW7euiouL1aaNGxQYGHjI/Z40aaI++eQ/WrUyVe++O0OhoaGt8C8A4GhDQAEAoBW99uor6h0To+uuv0F3TJ6sEaefrilTHnZrc+MNN2jFihQ9/8ILCggI0FtvvamMzExdc+21+uabb3X77be5tX/+uefUoUMH3XDjTXr66Wc0fvw4nXrKKa75MTG9FBERocvHXqEZ776rBx+4X927dWtS2wsvvqiRo0a5XuvWrdfChQtVVVWlSZMm6sILL9DDD0/RLbfeppiYXrrh+uslSePGjde6dev1waxZGjlqlDIyMv9Sv6+cMEGvv/66Lrt8rNq1bau777rzL+9nAMcOhngBANBKYmNjNXToUJ151tnKzGx8E3/fffdr5sz39corryo/v/HLIv/3v6/1zLPPSpKuGDtWlZWVmjz5TjU0NCg5eYVCO4Zq0sSJrvV++tmnWrduvXJycmQymTRmzBj17tNbP//yiySprKxMU6Y8ovr6eqWnp2viVVdpQEKC0rdvd6uvoKBABQUFkqSxl1+u8PBwXf97CFm+PFnJySu0evVqSdK3332nAf0HSJJ2Z2SopqZaJcUl2r59x1/u94wZ7+rXXxdIkj766GNdd911f3eXAzgGcAUFAIBW0jsmRjk5Oa436ZK0avVqORwO9ejRwzUtJTXV9XOvXj2VkpqqhoYG17TUP8yXpF9/XaB/Dh+uN994XcuW/qb+/eNls9lc87NzclRfX+/6vaSkRP7+/gesMyamlyZPvkP33X+/ivbskdR4M31ISLCefvop/fLzT5o0caLbNlqi37t2796vRr9DWj+AYxsBBQCAI8xkMrn9XldX98eZTdo7nft+tlqtmj1rlkaOPEdz5nylc0aO0tKly/5yLT4+Pnr+uef0+edfaOHCha7p9957j+6/7z6tWrlKE668Si+/8spf3sZe+/cbAJrDEC8AAFpJ2ubNCgsLU2RkpOtqQkJCgkwmk7Zt29bsMlu3bNW4cVfIYrG4rqIkJiS45sfGxiouLlaJSQNVU1MjSQoKDvrLNd57zz1yOp169rnnXNNMJpMuufhi3XrrbVq4aFHjNoIOfRt/pd8AsBdXUAAAaCUbNmzQsmXL9OILz2vIkMH65/Dhmjr1CX313/+67sPY35yvvlJgYKCefeZpJSUmasyll2rMmEtd84uKiuR0OnXGGWcoOipK1117reL79VOvnr0UHBx8WPWNGHG6Lr74Ik2f/roiIsLVrVtXdevWVWazWUVFRTrppJMUGRmps848U2MuvVSdOnVUVGSkJKmsvFxJSUmKi4tVSEjI3+43AOxFQAEAoBXddPMt2rxli6a99pqeeeZp/fDDD5oy5ZEDti8vL9fV11yr6OhovfvuDJ1//r80bdp01dbWSpIyMzP1yKOP6v9uv01fffWVunbrqjsmT1aPnj0UGxt7WLWNGjlKJpNJL7zwvObNnet6tWnTRnfddbcGDkzSvLlfa9y4K3TrbbertKxM5553riRp9qzZ6t69uz7+6CP1i4v72/0GgL1MXbp2dx601VGi5NGxzU4PeWj2Ea4EAHC4OoRGqSA/w9NleFxMTC8lJibq44//45p2xdixGj16tM6/4AIPVgbgSDLy30Qf72Clpa380zYJCQmaPn26rFarbDab62W1Wt1eFotFFotFZrPZ9eIeFAAADCQkJET333effH18tWDBAnXr3k3XXnuNXnzxJU+XBgBHBAEFAAADWb48WXfeeZeuu+5a3XzzTcrOztFr06briy+/9HRpAHBEEFAAADCYb779Vt98+62nywAAj+AmeQAAAACGQUABAAAAYBgEFAAAAACGQUABAAAAYBgEFAAAAACGQUABAAAAYBgEFAAAAACGQUABABiIydMFAIABHN9/CwkoAABDaGiwy2rl+4MBwGq1qqHB7ukyPIaAAgAwhLq6Gnl5+Xi6DADwOC8vH9XV1Xi6DI8hoAAADKG2plK+voE63oc2ADjemeTrG6jamkpPF+IxBBQAgCHU19eptq5agYFtPF0KAHhMYGAb1dZVq76+ztOleAwBBQBgGBXlxTJbLAoMbCuupAA4vpgUGNhWZotFFeXFni7GowgoAABDKS0pkFNOtWsXJj+/QFmtNhFWABybTLJabfLzC1S7dmFyyqnSkgJPF+VxPC4FAGA4FeXFqrVVytvHX0HB7WWx8L8rAMemhga76upqVFZWeFwP6/oj/uIDAAypvr6O/1kDwHGIIV4AAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwCCgAAAAADIOAAgAAAMAwPBpQgoICFRAQ4MkSAAAAABiI1VMb7tC+vaI7R8lut2vXzt0qKS2VyWRS75heatu2jSRpW/p25eXle6pEAAAAAEeYRwJKp04d1bFjqMpKy+R0OtWzZw9t37FDfn5+stlsWvLbMtlsVg0eNEjl5RWqqqo6pPWGPDS7lSsHAAAA0Jo8ElACAwJUUlwie0ODGhoatH3HTjkcDnXt2kVbt6ZLkurr7crOyVHH0A7asXOXJMnPP7jZ9XnZvI9Q5cCfc3q6AOB3pSVcfQYAHJ08cg/Krt0ZCgkJUfdu3RQRHq4O7dtLkry9vFVbW+tqV1tbK2/vww8fJrNJFqtNTsnttb/95x+pdgdqf6TbWaw2mcwWw9ZnlHY2L+9Danc8hxOz2SKr1WMjRo8aNptNJpPJ02UAAGBoHnlHUVdXpzVr1ykiIlw+3t6KiAiX2WJWbV1jIKmpqZEkeXu7B5aqytJm17f/ADCbzVv+AcF8gngQgUFtVV9Xq5qaSk+XYmgdQqNUwrH0p3x8A2Sz2lReXuzpUgytTZuOqqgolr2+ztOlAABgWB65gtKrZw/5+/tLkiqrqlRQWCgfHx/l5eYrOjpSkmS1WhUeFqa8/AJPlAgAAADAAzxyBSW/oFBxcX1ltVjkcDhUU1Ordes3qKGhQf4B/jrxhKGSpB07dx7yDfIAAAAAjn4eCSglJSVavnyFoqMiZbc3KDsnxzUvLW2zJ0oCAAAAYAAevat1d0amJzcPAAAAwGCOycfu1NfXqqSYm5oPprxsj6dLOCoU5Gd4ugTDq6muUI2nizgKFBfneboEAAAMzyM3yQMAAABAcwx/BSUiIlxdOkdLkvLzC7R1W7pr3oD+8dqWvl0VFRVuy7Rt21YDkxK0dOlyle8371jj7eWl2Ni+8vHxkcPRoPXrN6qislIWi0VxsX1ltVplNpu1c9cuFRQUNlm+R/duioyM0IKFi+V0Hvvf5NE5OkpOp1O7MzI1MClBFsu+U2DDxo2qqHB/5PLxdCxJUnR0lMLCOskkkwqLirRtW7p8fHwU27e3LBaLnJI2bUprsp8kadCgJNXX1Wv1mrVHvvAj6ED7Izw8TJGREXI6nNpTXKz09O1NlrVarTrpn//Q1m3p2r2bK3MAADTH0AElIMBfXTpHa3lyiux2u/rH91NERLgcDQ6Fh4epTZsQbWvmTUBkZLgyMjMVGRmhTcf4Tfe9e8eooKBAGZlZCgwM1ICE/lqyZKmiIiNUWlqqnbt2y2Kx6IRhQ5oEFJPJpNCOodqzp1ihoR2Ul3fsDosLDAhQjx7dFRwcpO3bd0iSrFabli1P/tPljqdjKTAwQB07hio5OUWSNDApUW3ahKhzdLR27cpQYVGR2rVrq94xMUpJXem2rL+/vxwOh/z8/eTt5aXaumP3ez56x/Rqsj/S0jYrMiJcKSkr5XA4FBfbVx1DQ5WX735OhXXqpJycXEWEhxNQAAA4AEMP8eoYGqrs7FzZ7XZJ0q5du9UxNFQ5ublKXbmq2U9xvby85Ofrq82bt6pdu7Yymxu72KtXT3Xq1FGS1LZNG8X3i5MktWkTosGDByoxYYDi4vpqwID4I9S7v89kMqldu7bKzMqWJJWXl6u6qlpBQYGyNzTIy7vxG9C9bDY5HI4my3do3157ivZox85dioyIcE0fOmSwvL28JEndu3dTVGTjvOioKA0ZPEgJCf2VMKC/OkdHtXYXW0x5RYVWrV6j7Jxc1zSr1ar4fnEamJSgbt26NlnmeDqWJMnLy1vlZeVyOp1yOp2qrqmWl5eXbDabysrLJUlVVY3T9hcZGaGMjExlZWYpPCJckmSxWHTiCcNcbQYNTJS/v79MJpNi+/bRoIFJ6h/fT8OGDlZgYOCR6WQLaG5/BAYFqri4xHWeFRYVKbRjhybLhoeHaVv6dlXXVCskJFiS1KFDe/Xt09vV5uSThkuSfHy8lZQ4QEmJCerdO8Y1HQCAY52hA8r+3yRfW9v4TfN/Jjysk7KysuVwOJSXl+96I3kgsX37at3a9Vq5arUqK46u71zxstlUX1/vNjRr7z7Kzc1Vu7ZtdMIJQzVs2BBt376zyfIREeHKyMxSeXm5zBazfH19D7gtPz8/RUSEaUVKqlatWiOTydQaXTqidu3arc2btyh15WoFBgY0OVaOp2NJkoqKipS2eYskKSgoUMFBwSosLNKKlFTV/X5FpGeP7srMdH/6ntlsVts2bVRQUKis7ByFHWQ/hYeHyeF0akVKqjZs3HTQc9pomtsfZWVlat++nWtIZaeOHZsEuaCgIFVXV6uurk4ZGZluHwo0p2ePHsrKzlHqylXKycl1BWQAAI51hh7itX8g8fZxDyzNCQ8Pk8PhVHh4uMxms0LsdmVn5zTb1mq1/v5JcePzh4pLihUcEtRyHWhldfX1stlsMplMrpCyN9T1i4tTevoO5RcUyGKxKDFhgKqqqlRaViap8dPZkJAQxcX2ldT4qXBkRLjbPT5/5O/vp9LSMtcnxMUlJUegh63Hy8tLRUVFrqFIeXn5Cg4OVm7uvqcsHU/H0h+1b99OPXt016rVa9TQ0CCp8WpIv7hYVVRWNnk8eMfQUHl52TR40EBJjfu2Xbu2KikpbXb9Af7+Ki4uliTZ7XaVlx999/Y0tz/St+/QgP791OBwqKSkVH77Bf7IyHAFBgZoyOBBkiQ/P19ZrQf+ExwQ4K8tW7ZKkkpLS5u9CgoAwLHI0AElLz9f/eP7KSMzU3a7XdHRUU3GdP9R2zZtVFZeofXrN7imJSUmKCDAXw12u2w2myTJ16/xjYPdbpfJJPn6+qi6ukZtQtq0bodamNPpVGFRkSIiwpWZmaXAwAD5+vqqrKxcPj7eqqltfLPc0NAgu90uL+99n+hGRERoy9atyvp9eJjZbNawoUO0LX277A122bxsqq2rk5+vr+pqa1VZWake3bvJbDbL4XCoTUiIioqKPNLvlhAQ4K/u3bspJWWlnE6n2rdrpz179j12+Xg7lvbq0qWz2rVtq5TUVaqvr5ck+fr6Kr5fnHbt3u0W4PaKiAzXihWpqqquliSFhASrc3S09uwpltVqcV1t8/Fp3FcVlZVq26aNcnPzZLVaFRgYcIR61zKa2x9+vr7y9fVVSuoqmUym3+fvu8fEYrEoJDhES35b5prWtWsXhYV1UkVFpet48vPbF2rKKyrVpm3jfgoODuYKCgDguGHogFJRUamdu3ZryODGT2aLivYc8BNsqfGNUm6O+xuo3NxcRUZEaNfu3eoXF6c2ISEymU1yOhqvOGzYsEnx/frJbrertq72qHuSVVraFsXF9lV0VJQcjgatXbdeTqdTGzZuUq+ePeV0OmQ2W1RSUuK6Sd5kMiksrJOW/+EGcYfDodKyUnXo0F47d+5SbN++qqqqko+Pj6TGsfaZWdkaPChJdXX1v9+n4JEut4g9e4oVFFSoIYMHyuFwqLikxO3+lOPxWAoL66RePXuorKxMiQkDJEnZOTnq3DlaFrNZnaOj1Tm68Yl6K1JSG2+K9/OTxWJxhRNJKikpVVxsgGw2mzIys5SUmKCa2hrX/sjOzlFIcLAGDxqo2rpa1dYeXfsqKSmhyf5ISV2pAH9/DRs6WA6nU9lZ2Sr5w1XGsE6dVLhfoM/NzdOA/vFatjxZnTtHaUD/eLcrxFu3blNcbF9FRkSoorKSKygAgOOGqUvX7kfPO4NW0Kd3jLalb1d9fb2iIiPl7e3V7JPBjndeXl7q1rWL6x6F+Pg4ZWZmac+eYs8WZiAcS4emTZsQBQQEKCMj03XlbtnyZNdwMjSKiopURXmFiktK5O/vr759oWcd5QAAIABJREFUemtFSqqnywIAQD7ewUpLW/mnbRISEjR9+nRZrVbZbDbXy2q1ur0sFossFovMZrPrZegrKEdCcUmJEgbEq95uV4O9QRs3bfJ0SYZUV1cnh8OhgQMT5XQ4VVpaSjjZD8fSoSkvr1Dn6GiFdmgvk8msrdvSCSfNKCkpUe+YGNkb7DKbTNqUlubpkgAAOCKO+4CSm5vX7Lh6NLVl6zZPl2BoHEuHxm63H/Nf5tgSyssruGICADgucdclAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDKunCwAAAABw/MrNzdXq1avl6+uruLg4AgoAAAAAz/Lz89Opp54qs9nMEC8AAAAAxkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAHmOxWGS32/Xrr78qNzeXb5IHAAAA4DkdOnTQ6aef3vgt8nyTPAAAAAAjIaAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCCgAAAADDIKAAAAAAMAwCClrNc8++psjIaE+XARjGVVddpzNGnOPpMgAAMDTrkdpQdHRnjR93lQYMSJRk0pYtaZr5wQylpW1ytXnt1bfUs2cv1+81NTXavXuX/vOf2Vry26Im8/f37ntv6ZNPPjrg/IQBiXrqqRckSQ89fK+WL1/69zuGJvz9AzRkyAkKDAzW8OGn6Oefv1deXo5rfkxMH/17zHhFRXVWcXGR/vf1l1q48Ocm6/Hx8dW/x4zT0KEnSjJp9eoUfTBrhiorKyRJEeGRGj/+avXq1VtlZWX68cdv9L+vvzxS3WwRnjovTCazLjh/tEaOOk/t23VQVlamZs1+T0uXLmmy/MMPPa4TTvhHk+nnjDxNDQ0NB6zzUOo4mA8++ESVFRW6/oaJh7Wc0ZjNZg0cOFTdu/VUm5C22rV7pzZv3uiab7VadfHFl+ufw0+Rl5eX1q5dpfdnvq3S0hK39fj6+umdtz9ssv6PPp6pefO+cpvWtWt3PfrIM8rOztTd99zaOh0DAKAVHJGA0rt3Hz315Avy8fHRli1pqq+vV0JCovr166/77r9Ta9eudmu/YEHjm1VfPz/1jx+ghx56TPc/cJfWrl2lwsJ8SVKXLt0UFhau9evXqry8TJKUmZl5wBosFouuv/6WVuoh9ooIj9R99z2qiopyWa0WJSUN1jlnn6cPP3xP3343V9HRXXTvPY/o55+/1+dffKwePXrpmqtvUnHxHq1bt+84MJvNuvPOB+Tn66e33n5NTqdTl14yVjfd+H96+plHFRLSRlOmPKXk5KX6cs6n6hjaSWPGjJPD6dTcuXM8uAcOnSfPiwkTJmrMpZdrW/pWLVz0q4YMHqqHHnxMt952g7ZsSXNrGxQcrJKSYm3atMFtusPhdPv975yfxzIvLy/dd++j6hAaKnu9XX5+/rr/vkeVnLxU019/UQ6HQ1dffZPi+w3Qp599qKqqSp137mjdd+8jeuCByaq317vWFRAQKKfTqaeefsRtGzk52W6/m81mTbzq+iYBBwCAo0GrBxSTyazJd9wrHx9vPfrYg/rtt8WSpAEDEjX1iWd104236drrrpTTue/NztQnH3X93KN7T02b9rbOHXW+Hp5yn2v6ddfdpAvOv0jvvfe21m9Yd9A6zh11vjp37qKlSxdr2LCmnwajZVx++ZVat2613nzrVT37zKt66eVn1Kd3rC67bLwWLf5V/zpvtFatTtEHs96RJK1fv0bdu/XUkMEnuAWUxIRBioyI1uQ7b3S9wc3KytADDzyugIBAnXfeaO3atUNvvzNNkpSWtkHlFWW68Ybb9f3381RXV3fkO38YPHle+Pn5a/SFl2jdujW6867b5XQ6FBsbpxeef00nDDuxSUAJCQ7RmjWr3LbfnLfefv2w6jhenHHGSPn5++uOO27QZZdNUGbGbm3fvk333jtFsbHxys/P1T9OPElPPf2I6xxIS9uol196U4MGD9Nvvy10rSswIFCVlRVav37NQbfp4+urH3/8RsOGDW/V/gEA0NJaPaDEx/dXVFS0Fi1a4HoTJkmrV6/Up59+pIKCAlksFtnt9maX35a+VVXVVerYseNfriE4KFhXXDFBKanJWrZ8aZOAYrXadOWESTrttDPk7++vDRvX6803p2nHjvS/vM3jVVRUZ3340ftub6x/+PEbrd+wVpWVFVq5aoXy8nLdlqmorJC3t7fbtOHDT9HSZYtc4USS8vJydeut18jhcCgyIkppfxgiIzWGFB8fX8XE9HULO0bkyfOiwW7XlCn3qWjPHjmdDrd59fX1TdoHBwertKxUp59+pqKjOitt80a3mg9Fly5d9eYb72n27Pc1a/b7kqQpDz+uYcP+oTPPOlkdO3bSBzP/ox9//E7BwSFKSEjS6ItGSWq8+nnN1dfr1FPPkNli1jffzNX7789w1d6nT6yuufp69ewZo9LSEs2f/7U++nh2k755SlRUZ6Vt2qCamhrXtG3pW/TAg3cqJydL5503WkVFBW6ho7S0RKkrV2jokBPdAkpAYKDKK8r/dHtt27bTxRf9W8+/8KS6du3e8h0CAKCVtXpA6dO7ryTpt6VN39C8P3PGQZfv1StGfr5+ysvL+8s1TJgwST4+vnrjjdcUG9uvyfxrrr5e//rXhUpJTVZWZqZGjDhTzzz9oq686nJVHOTNANxlZ2fq7LPOVUbGLrfpOTlZkqQlS/a92bJYLOrTJ06DBw3TtOkvuLWPiIjU2rWrNGHCNTrh90+AFy36RZ98Olt1dXUqKipURHik2zJ7b8jv1DHM8AHFk+dFbV2tUlJXSJIu+/cViu7cRQOTBik/P0/zv5nr1tZsNisgIFDnnH2uzjv3Atf077//Rs+/8PRhb/tgTj11hOrr67Rz53Y5fx9C1rlzFwUGBSk5ean69InVmEsvV2VFhT797GN16BCqp596XjW1tZo777/q3r2Hxo27SjKZ9OGHM1u8vr8iJztTZ545Sskrlsps2vdckr3nRKeOYcrMynQL9ZKUmbH79/uv9gkMDJTD4dD119+mAf0TVVNTo7nz5ujHH791LT9h/DVasWKZNm5cR0ABAByVWj2gBAYGSZIKCwskST4+Prrn7gfc2vznkw/dbgq+796HJO0da58gSfp6rvsNoPtr376DLrvsCtfvGbt3a85Xn6t79x46++yRmjPnC2Vk7G4SUIKDQzRq1L+0YsVyPfDg3ZKklJRkPfbYUzrv3PP10cez/kq3j1sfzJqhW26erGeefkWSdOEFl2rBwp+0Zs1Kt3ZXT7pRJ598uiTpl19/0MqVK9zmh4S00chR52vb1s168aWn1bZtW40ZM04BAYF6/Y2XtWDhz3rg/sd0xohztGjxr+rUKUyTJt6gysoKWW22I9PZv8HT58VeJ5ww3HVj+8aNG1RbW+u2vM3mpSVLFsrLy0tvvf26KioqdPdd9+uMM87WDz98q7Xr/nyo0eEqLy/TjTddo4KCfNc0u71et99+o3Jzc+Tt5a23356pCy64SJ9+9rFGj75E3t4+uvOu27V58yaZTGa99OJruuTif+uzzz42xFC/b7+bp+7de+nee6ZIktLTt6q+vl6LF/+qenu9AgMDVdnMByEVFeUKCgp2m2Y2mRXaIVQpKcv0/AtT1aVLd1327wlyOBz66afvlJQ0WDExfXXnnTceia4BANAqWj2g7H3iUkhwSOMGrdYmQ6y+/+Fbt99POulUSY1PK9q1a6f+88mHSklJ/tPtBAcHa+Q557l+X7V6peZ89bluuP4WlZWVafaH7ze7XJcuXWWxWJScvMw1LSU1WQ0NDerevcch9RH7ZGVl6J57b1P37j11+233qGPHTrpz8gNatmyJXpv2vKvdl3M+0U8/faeo6M4ac+kVyj831+0JXHa7XeXlZXr9jZddnwwXFhbqoQef0OwP31Na2gbNePd1XTH2Ko0ff7Vqamr00cfva+Q5/9KePYVHvN+Hy9PnxV433XyNvL19dMrJp+m22ybrmmtu0EsvPeuaX1tbo8cef9htne+884amTXtb/fsntHhAWbbsN7dwIkk7d+1Ubm7jU+Aar/4ka+Q55yk4OEQ9uvdUaVmpNm9uDHJOp0MrUpLVu3dfhYdHaOfOHS1a319RU1Ot51+Yqk6dwnTlhGvVtm17jRs3UWefc54eeeRelZeXK6RN2ybLBQQEqqys1G3aosW/avGSBa5zYsuWNAX4++uMEedoyZIFGj/uan38n5kq+8PQSAAAjjatHlC2bN0sSerXr78WLvpVFRUVOvOskyVJ48ZdpcsvG9dkmb3zD0d6+rYmy7Vv115xcfGSpC+/mOc279FHnpQkPfdc43/3H15hMpkOuwY0cjqd2rZti6qrq/T6Gy+rXbv2uuvOBzXnq0/k7x+gjIzdKioqVFFRobbv2CZJuvyyCW4BJS8vV9u2bXb7d9n7e8fQTiovL9Mvv/ygJUsWqn379iosLJS3t7fGj7taWVnGf1qUJ8+Lrl27a8SIM7VgwS/avHmTamtr9O1383TZ5ePUr1+8W9vExIEaMmSYPvjgXVVWVkqSSn9/07z/p/stoaq66vAX2v/cbaFaWlpubo7y8nOVmpqs1JXJeuLx5zV48DDl5uUoLi5eJpPJ7XiPjIp2u1/LZDKpa5fuyi/IVUVFhWv6jh3pOu+80erTO1bt2rXX1ZNu1NWT3K+gfDh7jralb9HDD9/d+h0FAOBvavUvaly1KlV5ebk688yzFRW170v7/P39dfrpZ7bqtmtqa/S/r+e4vVauTJHU+Ent/76eoy1b02S32zVkyDDXcgMHDpbZbFZ6+rZWre9YYzabddqpTf9N165dpYaGBrVp01733P2w/nHiSW7zvby8ZLFY3KatWZOqbt17uk3r0aOXnE6n8gvydOaZI3XZv8errq5W2dlZqqur1YjTz1JGxi5lZu5u+c61ME+eF4EBARp94SUac+nlriDevn0HtW3TVmWlpQoKClKXLl3Vrl179ezZS+f/a7TOOnOka/nTTh0hSUrbvKnZ9TenqqoxeLT5/UqByWRWVFTnQ1q2S+cu6tQpTJLk7eWtgQOHaM+eIpWWlmhb+lYFB4eod+8+rvUOHDhYNTU1ys7OOuT6WtOJJ54kPz8/t2lFRYXavXun2oS00fLlS9S2bXvFxu4Lh8HBIUpKHKRly/bdo2QymXTXXQ/q1FPOcFtXz169lZ+fp7TNm3Tf/f/n9po37yvl5Gbrvvv/T6+//lLrdhQAgBbS6ldQGhoa9MyzU/Xk1Of0ysuv66efflBdfZ2G/+MkdegQ2qrbrqio0LRpL7tNO+3UEUpMHKiv537lGh7z9ddf6YILLtKTU59TdnaWTj1thEpLS/S/r4+O79MwiujoLrryymsVHBKiOXM+dU3/x4knqb6+Xtu3b9W8ef/VZZeNl7ePj7Zu3azw8AhdNPoyffd94xWuhx+aqpUrU/T9D/N12uln6dprbtbCRT+rTZt2GnPpFfr2269VVlaqrKxMXX7ZlaqorNDmzZsUG9tP5//rYj3//BOe6v5h8eR5sW79OqWmrtAJJ/xDr732lnbt2qnEhIGyWq36+OPZOu20M3TdtTdp3vz/aeb7M3TeuRfq6quv14ABCTKZzBo0aIi2bt2iX3758ZC3WVBQoPz8PJ1++pmqrq5WdOfOioyMOqRlrVabXnpxulJTG4dudQztqHfeeUOS9Pnnn+isM0fq0Uef0oIFP6tb1+7q3buv3p85wxD3n9hsNl122XgNG/oPTZv+omt6RESUevTopf99/aXy8nK1ePGvuuH62/TZZx+pqrrxe1Dy8nK1YsUynXTSaTr5pNP04otPae68r3TR6DFyOJ1KT9+i3jGxGjXyAr3yyrOqrq7Srl3uQ9r69IlTfV1dk+kAABjZEfmixvXr1+q222/QhAmTdNrpZ8hsMmv9+rWa/81cTRjv+W+IfvOt6aqorNCokf9SXFy81q9fq5dfed5tGAUObufO7Xr77WmaMOEanXzSaQoICNRtt92t9u3a6623p6mqqlJfzvlEZeWlGnH62Rp94RiVlBRr/vz/6uu5jcO7evXqo+zsLFVUVGjq1Ic1YfzVuvuuh1RZWanvv5/vard+/Rq98eYruuD8S3ThBZcoKytTz78wVWvWrvLkLjgsnjovnE6HHnv8YY29fLyGDz9J/zhxuDIyduvVV19Q8opluiDyIlfb0rJS3XnXrZo06Tr1i+svq9Wi73/4Vm+++ZocjkN/jK/T6dDUJx/VbbdO1jkjz9Wa1Su1ceN69e0bd9Bld+3aqZSU5TrttDNkMpv18X9m6/MvGgNwYWGBbr3tet16y2SdfdYolZaW6o03X9OcOZ8fZK1HRn19vZ5/bqpuv/0evfLyW6qrq1N8vwRddtkE/fLrD65HC8+Y8bouuXSsLrlk7O/fJL9S7898W/X2erVv10G9evWR1WbT3LlzVFdbqxEjztFFo/+tvLwcvfzKM0pN/fN7kQAAOJqYunTt7jxoK+AwBAeHaODAIbrk4rH68adv9euvP6qg4K8/Jho42tlsNiUlDdGokeerqKhQX3/9pbalb/F0WQAA/CU+3sFKS1v5p20SEhI0ffp0Wa1W2Ww218tqtbq9LBaLLBaLzGbzvtcR6geOI6WlJfrpp+9UXl6qpUsXEU5w3Kuvr9eyZYu1fcc2bdiwlnACAMCfOCJDvHB8mnznTZ4uATCUd999w9MlAABgeFxBAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYBBQAAAAAhkFAAQAAAGAYHg8o4WFhni4BOOKCggIVEBDg6TIAAAAMx+qpDdtsNkVFRSoqMkJmi1m5uXmy2+0amJQgi2VfWRs2blRFRaWnygRaXIf27RXdOUp2u127du5WSWmpTCaTesf0Utu2bSRJ29K3Ky8vf98yHdqrW7euMsmk6ppqbdiwSXa73VNdAAAAaDUeCyhJiQnasXOnqqtrZDaZlDCgv1akpMpqtWnZ8uRml/HzDz7CVeJoUFVZ6ukSDlmnTh3VsWOoykrL5HQ61bNnD23fsUN+fn6y2Wxa8tsy2WxWDR40SOXlFaqqqpKPj4+6dO6slJSVamhoUHh4mPz9/VVa2thvzgs052g6LwAA+COPBBSz2SybzaaioiJFRUZod0amMrOyGwuyWhXfL05eXjbtKS7R9u07PFEi0CoCAwJUUlwie0ODGhoatH3HTjkcDnXt2kVbt6ZLkurr7crOyVHH0A7asXOXwsI6ac+ePerXL1ZWi1V5eXmucAIAAHCs8UhAcTgcSt++XUlJifLz9VX37t2UmZml2tpa7dq1W/n5+aqrr1d8fJw6deqo3Nw8SXwiiKPfrt0Z6tM7RsHBwaqsrJTT4VRefr68vbxVW1vraldbW6ugwEBJko+Pj4ICA5W6cpUcDocSE/qrqqpaRXv2SOK8AAAAxxaP3SSfnZ2j5ctXqKKiUtXV1UpKSpCXl5eKiopUW1cnp9OpvLx8BQczfAXHjrq6Oq1Zu07p27erpKREERHhCgvrpNq6Wnl7e7vaeXvvCyz1dXUqKCiQ3W6Xw+FQTm6eQkI4LwAAwLHJIwElIMBfPXv2kCQ5nQ7l5OTKbDYrMCBAsXF9ZTKZJEnt27VTeVmZJ0oEWkWvnj3k7+8vSaqsqlJBYaF8fHyUl5uv6OhISY3DHMPDwpSXXyBvb28VFBaqbds2budFWVm5x/oAAADQmjwyxKuyskoWi1nDhg6Wt7e3Bg5MVHr6dhXt2aPAoEANGTxQDodDxSUlys7J9USJQKvILyhUXFxfWS0WORwO1dTUat36DWpoaJB/gL9OPGGoJGnHzp2qqqrSaaeerOQVqcrLL9CggYmSTCooLFRBYaFH+wEAANBaTF26dnd6bOMmkwYmJSoldaWcTo+VARxx0VGRstsblJ2T4+lSAAAADouPd7DS0lb+aZuEhARNnz5dVqtVNpvN9bJarW4vi8Uii8Uis9nsennsMcOS5HQ6tSIl1ZMlAB6xOyPT0yUAAAAYkse/SR4AAAAA9iKgAAAAADAMAgoAAAAAwyCgAAAAADAMAgoAAAAAwyCgAAAAADAMjz5mGEDrcYT4u/1uLqn0UCUAAACHjoACHKNq/xGr2sG9JEm+c5PlnbzFwxUBAAAcHEO8AAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQUAAACAYRBQAAAAABgGAQVoRaGhobJYLJ4uAwAA4Khh9XQBwLHIZDIpIiJcXTpHKzcoUFlZ2aqurpa3l5diY/vKx8dHDkeD1q/fqIrKSrdlzWazBg9KksPhUPKKVEnSwKQEWSz7TtcNGzeqosJ9OQAAgGMBAQVoBX16x6i2tlaVlVWqqqpSYsIAJa9IUe/eMSooKFBGZpYCAwM1IKG/lixZKqfT6Vo2pldPFRQWqV3bNq5pVqtNy5Yne6IrAAAARxRDvIBWEBAYoPyCQtntdhUVFmnpsuWy2+1q166tMrOyJUnl5eWqrqpWUFCga7lOHTvKZDYpJyfHbX1Wq1Xx/eI0MClB3bp1PaJ9AQAAOJKOiSsoNSf3c/1sqqqVd/IWD1YDSJs3b1FMr54KDAxUQ0ODsrKyVVNTo/r6ererJbW1tfL29pYk+fn5KioqUqkrV8nHx9ttfbt27VZ+fr7q6usVHx+nTp06Kjc3T5LUITSq2Rp2t1LfAAAAWtMxEVCcAb6qHdxLkuQ7l2Ew8LzS0jKlpK5Uv7hY7SkuVlxcX61evVY2m00mk8kVUry9vVVbWytJCg8Lk8ViUVLiAJnNFvn7+2nQwESlbd6qoqIi1dbVSZLy8vIVHBzsCigF+RkHqCKs1fsJAADQ0hjiBbQws9msuNi+rqd3Fe8pVkVFpWxeNhUWFSkiIlySFBgYIF9fX5WXV8jb21vb0rdr2fJkrUhZqbXr1qmiokIrUlbKZrMqNq6vTCaTJKl9u3YqLyvzWP8AAABa0zFxBQUwEofDoZLSUg0ZPFA2m03x8f1UVl6ukpJSVVVVKy62r6KjouRwNGjtuvUKCAjQwKQE/fzLgmbXt2dPsYKCCjVk8EA5HA4Vl5QoOyf3CPcKAADgyCCgAK0gMzNLmZlZ6h/fT5u3bFVNTY0kqa6uTitXrW7Sfv9wUl1d43rEsCTt3LlLO3fuat2iAQAADICAArSiNWvXeboEAACAowr3oAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIKAAAAAMMgoAAAAAAwDAIK4AGhoaGyWCyeLgMAAMBwrJ4uADiemEwmRUSEq0vnaOUGBSorK1vV1dXy9vJSbGxf+fj4yOFo0Pr1G1Xx/+3d6VIbWZrG8SdTmRLa2M0mVtuAzWrM4qquqq7uvpeJuYe5kP44cxPdERMxE9XTUVNV3ndjYwPGRuwIJCS0ISnnA2XVUMZL0RbKNv9fhL6IzHNOpnhD+WSeTO3vl9drb2tTd3eXJCmVSmn26TM5jlOtzQAAADgRZ2dBhcX/lGpq5en/k9Qy9NYyXEEBTtHlS4Pyeb3a308rnU7r6sQV2batS5cGtbW1pR9/uq4ns890ZWJchmFIkvz+GvX19ej2nbu6cfOWLMtSc3NTlbcEAADgZAxfraypf5HRNHjkfWftjop3/50rKMBpCoVDWo6uKBAIKLYd0/r6hhzHUVNTox4+eixJSiaTyqQzqq0NK5HYUy6X1917D1QsFmUYhkyPqUKhWG4zEKw7tq/MqWwRAADAp2HYIZm9vyegAKdpbu65Bgf6FQ6HVSwWtbKyqmw2q4ODgyNTtnK5nHw+nySpVCopm83q4sULam9rVSaTVTwer9YmAAAAfJCTT0mlgn7THSUVvpt6AAATTUlEQVTNg5JpMsULOE2JxJ5u37mr7e1t7ezuamRkSJZlybbt8pQuSfL5fMrlcpIO71uxLEvz8wv6/n9/1F4yqd7envKy6f3EsS8AAICqySVVXL5xolUJKMApMU1TI8ND5ad37e7sKpXal+21tR2LKRLpkCSFwyH5/X4lkyn5fD61trZodGS4HGAKBwfymJQuAABwuXzyRKsxxQs4JaVSSfFEQtdmpmTbtsbGRrWXTCoeTyidzmhkeEjdXV0qlYp6+OixQqGQpiYn9N3f/q5QMKiZmSmViiXl83k9mZ2t9uYAAAC8m2nJ7Jg80aoEFOAURaMrikZXND42qrnnL5TNZiVJ+Xxed+/df2v57/72d0nS/MKi5hcWT3WsAAAAJ2UEz8kInjvRugQUoAoePHxU7SEAAAC4jCM5RQIKAAAAgOpzXv6PCtEb3CQPAAAA4HQYpiUV8ire+w+VYi+O/q3vj7K+/TeuoAAAAAA4JfU98kz/qzyWJdM6PopwBQUAAACAaxBQAAAAALgGAQUAAACAaxBQAAAAALhG1QNKS0uLPB5PtYcBAAAAwAWq9hQvwzAUiXSot6db67VhraysKpPJqLGxQRcvXpAk5XN5PX7yRIVCsVrDBAAAAHCKqnYF5fKlQfm8Xu3vp5VOp3V14oq8XlvDQ5d1//5D3bx5W/v7++rp7q7WEAEAAACcsqpdQQmFQ1qOrigQCCi2HdP6+oYMw9DBQUH5fF6SlE6nVVtbW14nEKw7tq3MqYwYAAAAQKVV7QrK3NxzDQ70q7m5WRcunFc4FFKxWNT1GzclSbZtqaenWyurq9UaIgAAAIBTVrWAkkjs6fadu9re3tbO7q5GRoYU8PslSQG/X5NXr2p+flF7e8nyOun9xLEvAAAAAJ+HqgQU0zQ1MjxUfnrX7s6uUql9eX1eNTU1amxsVE9mn2pza6sawwMAAABQJVW5B6VUKimeSOjazJRs29bY2Kj2kknlsjl9/fXvlE5nNHT5kiQplUrpyezTagwTAAAAwCmr2k3y0eiKotEVjY+Nau75C2WzWUnSf/33d9UaEgAAAIAqq1pAeePBw0fVHgIAAAAAl6j6L8kDAAAAwBsEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAA4BoEFOCUtLS0yOPxVHsYAAAArmZVewDA584wDEUiHert6dZ6bVgrK6vKZDLyeb0aHh5STU2NSqWiHj+eVWp/v7xeT3eXHMfR6+WopiYn5PH8Uq5PZmeVSu0f1x0AAMA/NQIKUGGXLw0ql8tpfz+tdDqtqxNXdPPWbV26NKitrS0tR1cUDod1ZWJcP/zwk0LBoC5evKC6ulotLr6UJFmWres3blZ5SwAAACqPgAJUWCgc0nJ0RYFAQLHtmNbXN+Q4jpqaGvXw0WNJUjKZVCadUW1tWInEnu7df6CBgf5yG5ZlaWx0RF6vrZ3deDm4AAAAfG4IKECFzc091+BAv8LhsIrFolZWVpXNZnVwcCDHccrL5XI5+Xy+Y9t49eq1Njc3lT840NjYiNraWrW+viFJOtfSdew6rz/9pgAAAFQcAQWosERiT7fv3NXoyLB2dnc1MjKk+/cfyrZtGYZRDik+n0+5XO6t9b1er2KxmHL5vCRpY2NTdXV15YCytbn8jp7bK7I9AAAAlcRTvIAKMk1TI8ND5ad37e7sKpXal+21tR2LKRLpkCSFwyH5/X4lk6m3rqKEQkENjwzJMAxJUnNTk5J7e6e7IQAAAKeEKyhABZVKJcUTCV2bmZJt2xobG9VeMql4PKF0OqOR4SF1d3WpVCrq4aPHCoVCmpqc0Hd/+3u5jZ2dXdXWbuvazJRKpZJ243Gtrq1XcasAAAAqh4ACVFg0uqJodEXjY6Oae/5C2WxWkpTP53X33v23ln8TTp4/f1F+b2nplZaWXp3OgAEAAKqIgAKckgcPH1V7CAAAAK7HPSgAAAAAXIOAAgAAAMA1CCgAAAAAXIOAAgAAAMA1CCgAAAAAXIOAAgAAAMA1eMwwAAAAgIopFAr6/vvvFQgE1NnZqcuXL793eQIKAAAAgIqbnp5WKBQ68t6rV6+0tramVCql5uZmffnll0zxAgAAAFAdNTU16u/vV2dnZ/k9AgoAAACAqmhtbVVLS4ts2y6/R0ABAAAA4BoEFAAAAACuQUABAAAA4BoEFAAAAACuQUABAAAAUBWO46hUKslxnPKL30EBAAAAUBXPnz/X0tKSDMOQaZr661//SkABAAAAUDmGYci2bd27d0/t7e26dOlS+W+Dg4MaGhqSaZrlFwEFAAAAQMV4PB598803sm1blvXh+ME9KAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABcg4ACAAAAwDUIKAAAAABco6oBpbY2rFAoVM0hAK5DXQAAgLPMqlbH55qb1d3TpUKhoFdLrxVPJGQYhi4NDqixsUGSNL+wqI2NzWoNETh11AUAADjrqhJQ2tpa1draor3EnhzHUX//RS2+fKlAICDbtvXDj9dl25ZmpqeVTKaUTqff257/Lzfl/8vNUxo9UBnUBQAAQJUCSjgUUnw3rkKxqGKxqMWXSyqVSurr69WLFwuSpIODglbX1tTack4vl15JkgLBumoMFy6X3k9UewifBHWBT+lzqQsAwNlTlXtQXr1eVn19vS6cP69IR4fONTdLknxen3K5XHm5XC4nn8/3UW3atldeb01FxvuGz+eXZdkV7SMQCMkwKvexGIapQKCy9zdYli2fz1/RPrzeGtm2t6J9nDbq4t2oi4/zOdYFAODsqcoVlHw+rwcPHykS6VCNz6dIpEOmx1Quf3jglc1mJUk+39EDs/edEfQHwvKYnoqeNbQsS7lsRrnc+6fW/CP8NUFl0nsqlYoVad80Parx+Su6n3y+gHw1le3DNAwVS0Vl0smK9XHaqIt3oy4+zudYFwCAs6cqV1AG+i8qGAxKkvbTaW1tb6umpkYb65vq7u6UdHjQ09Hero3NrWoMETh11AUAAECVrqBsbm1rZGRIlsejUqmkbDanR4+fqFgsKhgK6qvffSFJerm09MEbgYHPBXUBAABQpYASj8d148YtdXd1qlAoanVtrfy3Z8/mqjEkoOqoCwAAgCr+DookvV6OVrN7wJWoCwAAcJZVNaB8SqdxU+heIlbxPmKx1Yq2XyoVK95HLpeu6A3TkpRKxSva/ueCuvg41AUAAO5RlZvkAQAAAOA4rryCEol0qLenW5K0ubmlF/OHP1Jnmqa+uDajH3+6/tY6jY2Nmpqc0E8/3VAylXpn24Zh6NLggBobGyRJ8wuL2tjY1Le//1rZ7C+Pbg2HQ7p1+44Sib0j609PT+ogf6D7Dx5+cDvC4bDOn+/VgweP1N7Wpu7uLklSKpXS7NNnchznyPKWZenb33+tF/MLev16+YPtS9KV8THNLyxqf39fgwP9CofDMgxp6dVrbR7zpKeP3U+S1N3dpfb2NhkytB2LaX5+QfX19RofGynvq3giobm552+t+1v200D/RTU0NMg0DUWjq1qOHk5x6unukuM4x055+i3tfy6oC+pCoi4AAJ8/1wWUUCio3p5u3bh5W4VCQeNjo4pEOhTw+9XQ2CCf7/gfIevs7NByNKrOzoievueG4s7OiGzb1g8/XpdtW5qZnlYymVI2m9WNm7clSW1trWptbXnrICwYDKpUKikQDMjn9SqXzx/bh2EYGh8fVSgYVDKVkt9fo76+Ht24eVvFYlHjY6Nqbm7S1tb2kfXa29q0trauSEfHBw/E2tva1NHRroaGes0vLCoS6ZDjOLp1+448Ho+mpyaVSOwd+b2M37KfwuGQWltbdPPnfTI1eVUNDfXyer16vRzVy5dL71z3Y/eTJLW3t8nr9erGzVsyTVNfffWlkqmk+np7VVdXq8XFl/9Q+58L6oK6oC4AAGeF66Z4tba0aHV1XYVCQZL06tVrtba06MX8gu7cuXfsOl6vVwG/X3NzL9TU1CjTPNysgYF+tbW1SpIaGxo0Njqi1tYW5XI5XZuZ1sjIsCRHgwP95YMwy/Ko/+IFPXny9K1+OjsjWl6OaiW6oo5IhyTJ4/Hoq999WV5meuqqAoGAHjx4pEwmq4b6Bg0ODEiSAoGADMOQ6TFVKLz9g3MdHe2aX1hUJptRfX2dJOncuWYNXb5UXuYP334jScrmsvJYh4+jvXjxvPr6erUdO7wXoFgsKrG3V/4l8pPsJ6/Xp3wur+npSU1cGZevxqeBgX75vF41NtRr8uqEJq6MKxQKftR+kqQvrs3I5z08kL5w4by6OiOKxXZkGIampyY1Njoin9crx5Hu3X+g1bX1t9r+LZ9DMBiUYRgaHrqs6alJjY+N6ssvZhQOh49t182oC+qCugAAnBWuCyi//pXsXO7wV7Tfp6O9TSsrqyqVStrY2CwfVBzHX1Oj5uYm3bp9R/fuPVCp5MiyPOW/RyIRra9vlA8E3zBNU40NDdra2tbK6pra39OHdHhQ5TiOduO7evxkVl6vV11dnfr6qy/lMT2Kx4/ezFpbW6tMJqN8Pq/l5ag6I5H3tj88NKRHDx8rnc4ok87q4ODgcNqJYcjn9aq5qVFer33i/RSLxRQOh/Xo4WPNLyzI8ljK5/Pajce1HF3Rnbv3tLD4UmOjI//Qfjp3rlmFYlGFYkENDfVyHEelUumdy5/kcyj9fAb9yezTD/4vuRV1QV1QFwCAs8J1U7x+feDlq/G9NR3j1zo62lUqOero6JBpmqovFLS6unbsssViUel0pvxln8/nj3zxNzc1aeGY6ROtLS3yem3NTE9JOjzr2tTUqHg8cWw/oWBQqVRK/oBfxWJRqdS+lpejmp19qoGBfvX29hyZDtLZ2aFwOKRrM9OSpEDAL8s6/uOxLEuO4yiTzUqS9pJ78gdqlE5nNHl1QvmDvLZjMWV//vtJ9tObPoKhoPovXtCzuTm1tbXKND3lOfx7e3syDEOWZZUPXN+1n2KxnWP7CYdCisfjWltbl2ma+sO33ygcDimZPP7pUyf5HHZ3dyVJhUJByeT77y9wK+qCuqAuAABnhesCysbmpsbHRrUcjapQKKi7u0sbm5vvXL6xoUF7yZQeP35Sfm/y6oRCoaCKhYJs+/BsqT/gP2x/Y1Pd3V0yTfPwYKS+7si89nA4pNQxN8lGOjt069YdpTMZSVJ9fZ16uru1s7Mry/LIMAxJUk3NYT+p/X21trSoWCqqo6O9PDVFkgoHB/KYv1y88ng8qq+r1w8//nKTc19fr9rb25RK7Ze3IfDzNhQKBRmG5PfXSJJqw7Xyer3a29vT4uJL2batqxNX9OLF/In3U6FQkNdrq6+vV7fv3CufuR7ov6iXS0va3o4pGAzI+XnZD+2nWGxHhWJBttdWLp9XwO9XPpdTOBxWXV1t+UDMNE2Zxrsv7J3kc2hsaND6+oYsy1I4HHpn225GXRzq66MujnNW6wIA8HlyXUBJpfa19Oq1rs0cngmMxXbeeTZTOvxiXl/bOPLe+vq6OiMRvXr9WqMjI2qor5dhGnJKjhZfLqmxsUF/+uO3chxH6XRGudwvN5P+/7OebwQCAXk8nvKXvyTF4wmNDIdk27aWoyuavDqhbC5bfgLR6uqa2lpb1dhw+FSkQqGg0ZFhHRwcKJ/P68nsbLmt9ra28jz5X7ZhQ1fGx3T9xk319HTpyvjYkTPmT5481djoqAIBv7K5w6ksvT3dunD+vBzH0dzzF0fm8//W/dTe3ibbtlUbDuubr3+nkuMol83qwYNHGhq6pL7eXjlOSY8ePf6o/eT1erW09ErDQ0NKp9OqqTk8iLz/4KG+uDatP/3xD3KckgqFguKJ48/6nvRzqK+r08z0lHL5nHK53FtPifpnQF282Qbq4tfOcl0AAD5PRm/fhTP1reT1enW+r1fPfn4E6NjYiKLRFe3s7H7Sfhoa6hUKhbS8HJVpmvryi2u6fuOmisW3bwI+icuXBjW/sKiDgwN1dXbK5/NqfmHxk7R9mn1Uej9Vuv3PBXXhrj6oCwCAm9X46vTs2d33LjMxMaE///nPsixLtm2XX5ZlHXl5PB55PJ5fZgyYpvuuoFTam7n1U1NX5ZQcJRKJT34QJknJZEo93d1qOdcswzD1Yn7hk37578bjmrgypoNCQcVCUbNP33660j9DH5XeT5Vu/3NBXbirD+oCAHCWnbkrKAAAAABOrtJXUFz3mGEAAAAAZxcBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuAYBBQAAAIBrEFAAAAAAuMb/AbIAizn4cflUAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "d461d4bd-c895-42bc-bf2f-9aed398b15d4",
   "metadata": {},
   "source": [
    "![image.png](attachment:4b7ab473-3178-4b91-991e-af76b2387399.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "661eff6a-22e9-456c-ba78-24aa850e9a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "79eaa48a-7b55-4ff3-a4d5-16ce0e4fedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt35 = pd.DataFrame(results_gpt35)\n",
    "df_gpt35.to_csv('data/HH_results-gpt35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b8a88bd2-7553-4222-8efc-530860f0c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_llm,answer_orig,document,question,course\n",
      "You can sign up for the course by going to the course page at http://mlzoomcamp.com/ and scrolling down to access the course materials.,\"Machine Learning Zoomcamp FAQ\n",
      "The purpose of this document is to capture frequently asked technical questions.\n",
      "We did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\n",
      "Data Engineering Zoomcamp FAQ\n",
      "In the course GitHub repository thereâ€™s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\n",
      "work\",0227b872,Where can I sign up for the course?,machine-learning-zoomcamp\n",
      "\"I am sorry, but there is no direct link provided in the FAQ database for signing up for the course. However, you can find a link in the course GitHub repository at this address: https://airtable.com/shryxwLd0COOEaqXo.\",\"Machine Learning Zoomcamp FAQ\n",
      "The purpose of this document is to capture frequently asked technical questions.\n",
      "We did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\n"
     ]
    }
   ],
   "source": [
    "!head data/results-gpt35.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aaee90a5-d4a6-4c24-8413-4f684fc7515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_llm': 'To sign up for the course, please visit the course page: [http://mlzoomcamp.com/](http://mlzoomcamp.com/). From there, you can scroll down to start accessing and going through the course materials.',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Where can I sign up for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To sign up for the course, you can use the following link provided in the course's GitHub repository: [Sign-Up Link](https://airtable.com/shryxwLd0COOEaqXo).\",\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Can you provide a link to sign up?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an FAQ for the Machine Learning course. You can find the link to it in the course GitHub repository: [Machine Learning Zoomcamp FAQ](https://airtable.com/shryxwLd0COOEaqXo).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Is there an FAQ for this Machine Learning course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, there is no mention in the FAQ that the course has a GitHub repository specifically for the sign-up link. The provided context discusses the use of GitHub for assignments, notes, and other course-related activities, but it does not indicate the existence of a GitHub repository for signing up for the course.',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Does this course have a GitHub repository for the sign-up link?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To structure your questions and answers for the course effectively, consider the following steps:\\n\\n1. **Type of Question:**\\n   - Identify what kind of question you are asking (e.g., general course-related, homework specifics, technical difficulties).\\n\\n2. **Be Specific and Clear:**\\n   - Ensure your questions are detailed and clear to get precise answers. Avoid ambiguity.\\n\\n3. **Context and Relevance:**\\n   - Provide context or background information when necessary to frame your question better.\\n\\n4. **Formatting:**\\n   - Use a clear structure, such as bullet points or numbered lists if your question has multiple parts.\\n\\n5. **Conciseness:**\\n   - Keep it concise and to the point, while including all necessary details.\\n\\nFor answers:\\n\\n1. **Direct and Clear:**\\n   - Provide a straight answer first before diving into details.\\n\\n2. **Use Examples:**\\n   - If applicable, give examples to illustrate the answer clearly.\\n\\n3. **Provide Additional Resources:**\\n   - Refer to specific course materials, links, or further reading if necessary.\\n\\n4. **Response Structure:**\\n   - Start with the direct answer, followed by an explanation, and then additional details or examples.\\n\\nBy following these guidelines, your questions and answers will be well-organized and comprehensible to both peers and instructors.',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'How can I structure my questions and answers for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course videos are pre-recorded, you can start watching the course right now. There will also occasionally be live office hours sessions where your questions will be answered, but those are recorded as well.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Are the course videos live or pre-recorded?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can start watching the course videos right now. The course videos are pre-recorded and available in the course playlist on YouTube. Additionally, you can access them through the course page (http://mlzoomcamp.com/).',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'When can I start watching the course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the live office hours sessions are recorded. You can watch them later if you miss the live session.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Are the live office hours sessions recorded?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the office hours sessions in the course playlist on YouTube.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Where can I find the office hours sessions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can access the pre-recorded course videos in the course playlist on YouTube. Additionally, you can find the links to these videos on the course page [here](http://mlzoomcamp.com/#syllabus).',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Where can I access the pre-recorded course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, sessions are recorded if you miss one. You will be able to catch up by watching the recorded sessions later. Additionally, you can ask questions in advance for office hours and they will be covered during the live stream. You can also ask questions in Slack at any time.',\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Are sessions recorded if I miss one?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can ask your questions in advance if you can't attend a live stream. You have the option to ask your questions for office hours in advance, and they will be covered during the live stream. You can also ask questions in Slack at any time.\",\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': \"Can I ask questions in advance if I can't attend a live stream?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you miss a session, don't worry—you won't miss out on the material. All sessions are recorded, so you can catch up by watching the recordings. Additionally, you can submit your questions for office hours in advance, and they will be addressed during the live stream. You also have the option to ask questions in Slack at any time.\",\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'How will my questions be addressed if I miss a session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a way to catch up on a missed session. All sessions are recorded, so you can watch the recording at your convenience. Additionally, you can ask questions in advance for office hours, and they will be covered during the live stream. You also have the option to ask questions on Slack.',\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Is there a way to catch up on a missed session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can still interact with instructors after missing a session. Since everything is recorded, you won’t miss any content, and you will be able to ask your questions in advance for office hours, which will be covered during the live stream. Additionally, you can always ask questions in Slack.',\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Can I still interact with instructors after missing a session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course includes the bare minimum of theoretical content. The focus is primarily on practical application, with theoretical concepts covered only at an intuitive level. For example, rather than deriving the gradient update rule for logistic regression, the course will teach how to use logistic regression and interpret the results.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'How much theoretical content is there in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the course will not cover theoretical derivations like the gradient update rule for logistic regression. The focus is more on practice, and theory will only be covered on an intuitive level. The course will cover how to use logistic regression and make sense of the results, but for theoretical derivations, other courses are recommended.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Will the course cover theoretical derivations like the gradient update rule for logistic regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The focus of the course is more on practice. The theory covered will be only on the intuitive level, with the aim of applying practical skills. For example, the course will teach you how to use logistic regression and interpret the results, but it won't delve into deriving mathematical theories such as the gradient update rule.\",\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Is the focus of the course more on practice or theory?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For more information on the theoretical topics not covered in this course, you can explore the referenced URL: [https://mlbookcamp.com/article/python](https://mlbookcamp.com/article/python). The course focuses primarily on practical applications, and theory is covered only on an intuitive level. For more in-depth theoretical study, other courses might be more suitable.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Where can I find more about the theoretical topics not covered in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course will help you understand how to use logistic regression and interpret its results. While it focuses more on practice rather than in-depth theory, it will cover the intuitive level of logistic regression, including how to use it and make sense of the results.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Will the course help me understand how to use logistic regression and interpret its results?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, prior knowledge in math is not necessary for this course. Although the course covers some linear algebra, it will generally have very few formulas and will focus mostly on coding. To help with the linear algebra part, you can watch recommended videos such as \"ML Zoomcamp 1.8 - Linear Algebra Refresher\" from Alexey Grigorev or the excellent playlist from 3Blue1Brown \"Vectors | Chapter 1, Essence of linear algebra.\" Additionally, you can always ask the community for help if you have any questions.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Is prior knowledge in math necessary for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The mathematical content covered in the course will include some linear algebra. However, the course will generally have very few formulas, focusing mostly on code.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'What type of mathematical content will be covered?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there are resources available to help with linear algebra. You can watch the video \"ML Zoomcamp 1.8 - Linear Algebra Refresher\" by Alexey Grigorev or the excellent playlist \"Essence of linear algebra\" by 3Blue1Brown. Additionally, you are encouraged to ask the community for help if you have any questions.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Are there resources available to help with linear algebra?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can seek help from the community for math-related questions. If you have any questions, never hesitate to ask the community for help.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Can I seek help from the community for math-related questions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there are recommended videos for learning linear algebra. You can watch \"ML Zoomcamp 1.8 - Linear Algebra Refresher\" by Alexey Grigorev, or the excellent playlist from 3Blue1Brown titled \"Vectors | Chapter 1, Essence of linear algebra.\" If you have any questions, feel free to ask the community for help.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Are there any recommended videos for learning linear algebra?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you haven't received a confirmation email after filling out the form, there are a few steps you can take:\\n\\n1. **Check your promotions tab** in Gmail, as well as your spam folder.\\n2. **Consider whether you have unsubscribed** from our newsletter, as this would also prevent you from receiving course-related updates.\\n\\nIf after these steps you still haven't received the email, don't worry. To ensure you don't miss any important information, you should **join the #course-ml-zoomcamp channel in Slack** and **our Telegram channel with announcements.** This will help you stay updated with the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': \"What should I do if I haven't received a confirmation email after filling out the form?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible that your confirmation email went to your spam folder. Please check your promotions tab in Gmail as well as your spam folder. If you have unsubscribed from our newsletter, you won’t receive course-related updates either. To ensure you don’t miss anything, it’s recommended to join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements.',\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Is it possible my confirmation email went to my spam folder?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, unsubscribing from the newsletter will affect receiving course updates. According to the context, if you unsubscribe from the newsletter, you won't get course-related updates. However, to ensure you don't miss anything, you can join the #course-ml-zoomcamp channel in Slack and the Telegram channel with announcements. This should be sufficient to keep you updated on the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Can unsubscribing from the newsletter affect receiving course updates?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you haven't received the confirmation email after filling out the form, you can get course updates by joining the #course-ml-zoomcamp channel in Slack and the telegram channel with announcements. This will ensure you stay informed about the course. Additionally, check your promotions tab in Gmail and your spam folder for the email, as it might have ended up there.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Where can I get course updates if I don’t receive the confirmation email?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you're having email issues, you can still stay informed about the course by joining the #course-ml-zoomcamp channel on Slack and our Telegram channel with announcements. This will ensure you don't miss any important updates related to the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': \"How can I stay informed about the course if I'm having email issues?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The duration of the course is approximately 4 months, but it may take longer if you choose to engage in additional activities such as extra projects or writing an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'What is the duration of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course takes approximately 4 months to complete, though it may take longer if you choose to engage in extra activities like additional projects or articles.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'How many months does the course take?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course can take more than 4 months if you choose to do some extra activities such as an extra project or an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'Can the course take more than 4 months?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Based on the provided context, the course duration is approximately 4 months. There is no specific mention of the possibility to formally extend the course duration. However, you may take longer if you want to engage in extra activities like an additional project or writing an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'Is it possible to extend the course duration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Extra activities that might extend the course time include working on an extra project or writing an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'What extra activities might extend the course time?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should allocate around 10 hours per week for this course.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'How many hours per week should I allocate for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Timur Kamaliev analyzed the time needed for different modules.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Who analyzed the time needed for different modules?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Students of the previous cohort spent around ~10 hours per week on the course.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'How much time did students of the previous cohort spend weekly?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The estimated weekly time commitment for the course is around ~10 hours per week.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Can you give an estimated weekly time commitment for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the detailed analysis of study time requirements in an article by Timur Kamaliev. The full article contains information on how much time students of the previous cohort needed to spend on different modules and projects.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Where can I find the detailed analysis of study time requirements?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To earn a certificate in this course, you need to submit at least 2 out of the 3 course projects and review 3 peers’ projects by the deadline. Even if you join the course later in the term, as long as you manage to complete and submit two projects and perform the required peer reviews, you will still be eligible for a certificate.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'How can I earn a certificate in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To receive a certificate for the course, you need to fulfill the following requirements:\\n\\n1. Submit at least 2 out of the 3 course projects.\\n2. Review at least 3 projects from your peers for each of your submissions.\\n\\nCompleting these requirements by the specified deadlines will make you eligible for a certificate.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'What are the requirements to receive a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you do not need to complete all projects to get a certificate. You need to complete at least 2 out of the 3 projects and review 3 of your peers’ projects for each submission by the deadline to be eligible for the course certificate.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'Do I need to complete all projects to get a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The exact deadline to qualify for the certificate is not explicitly mentioned in the provided CONTEXT. However, to qualify for the certificate, you need to submit at least 2 out of 3 course projects and review 3 peers' projects by the specified deadline for each. You can find the specific deadlines for the 2023 cohort on the 2023 cohort page.\",\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'What is the deadline to qualify for the certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an option for a certificate without a robot image.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'Is there an option for a certificate without a robot image?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you might still receive a certificate if you don't complete the midterm project. As long as you finish at least 2 out of the 3 projects and review 3 peers' projects by the deadline, you will be eligible for a certificate.\",\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': \"Will I receive a certificate if I don't complete the midterm project?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can still graduate without submitting the midterm project. According to the course requirements, you need to submit 2 out of 3 course projects and review 3 peers' projects by the deadline to be eligible for a certificate. Therefore, as long as you complete two other projects and the peer reviews, you can still graduate.\",\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Can I still graduate without submitting the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you are eligible for certification if you missed the midterm project. To obtain a certificate, you need to complete at least 2 out of 3 projects and review 3 peers’ projects by the deadline. If you accomplish these requirements, you will receive the certificate.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Am I eligible for certification if I missed the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not mandatory to complete the midterm project for the certificate. You will be eligible for a certificate if you complete at least 2 out of 3 course projects and review 3 peers’ projects by the deadline.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Is it mandatory to complete the midterm project for the certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, missing the midterm project does affect your certification eligibility. However, if you manage to submit at least two out of the three projects (including the capstone project) and peer-review at least three course-mates' projects for each submission by the deadline, you will still be eligible for the course certificate.\",\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Does missing the midterm project affect my certification eligibility?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To join this course, you should be familiar with some Python basics. Specifically, you need to know how to:\\n\\n1. Run a Jupyter notebook.\\n2. Import libraries and understand what they are.\\n3. Declare a variable and understand what variables are.\\n4. Perform some important operations related to data analysis.\\n\\nIf you\\'re not confident with these topics, it is recommended to read the provided introductory article and consider taking the free English course on the OpenClassrooms e-learning platform, \"Learn Python Basics for Data Analysis.\" This will help you build the foundational knowledge required for the course.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'What Python basics are required for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can learn the necessary Python knowledge for this course by following the English course on the OpenClassrooms e-learning platform titled \"Learn Python Basics for Data Analysis.\" This course is free and covers the Python basics required for data analysis. Additionally, you should be familiar with concepts like running a Jupyter notebook, importing libraries, declaring variables, and performing important operations related to data analysis. You might also want to read the linked article to ensure you understand all the necessary Python concepts.\\n\\nFor more detailed information, refer to this article: [Introduction to Python – Machine Learning Bookcamp](https://mlbookcamp.com/article/python).\\n\\n(Mélanie Fouesnard)',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'Where can I learn the necessary Python knowledge for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'It is important to know how to run a Jupyter notebook for this course because many instructional materials and examples, such as coding installations and troubleshooting guides, are provided in notebook format. For example, installation instructions for software like Xgboost and command substitutions (e.g., using `!dir` instead of `!ls -lh`) are often directly demonstrated within Jupyter notebooks. This hands-on approach allows for immediate application and testing within the notebook environment, enhancing the learning experience.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'Why is it important to know how to run a Jupyter notebook for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For this course, it is important to know some basics of Python for data analysis, such as:\\n\\n1. How to run a Jupyter notebook.\\n2. How to import libraries (and understanding what libraries are).\\n3. How to declare a variable (and understanding what variables are).\\n4. Some important operations regarding data analysis.\\n\\nIf you need to brush up on these topics, you can follow the English course \"Learn Python Basics for Data Analysis\" on the OpenClassrooms e-learning platform, which is free.\\n\\n(Mélanie Fouesnard)',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'What basic operations regarding data analysis should I know for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To learn how to declare variables in Python for this course, you can follow the \"Introduction to Python - Machine Learning Bookcamp.\" Additionally, you can take the free course \"Learn Python Basics for Data Analysis\" available on the OpenClassrooms e-learning platform. This course covers the Python basics, including how to declare variables, which are essential for data analysis.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'How can I learn to declare variables in Python for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the Machine Learning part of the course, you only need a working laptop with an internet connection. The Deep Learning part is more resource-intensive, but you can use cloud services (such as Saturn Cloud) for that.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Are there any specific hardware requirements for the course, or is everything mainly based on cloud services?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, for the Machine Learning section, you just need a working laptop with an internet connection.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Do I just need a laptop and internet for the Machine Learning section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the Deep Learning part of the course is more resource-intensive than the Machine Learning part. This is because while the Machine Learning part only requires a working laptop with an internet connection, the Deep Learning part requires more computational resources, which can be accessed via a cloud service, such as Saturn Cloud.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Is the Deep Learning part of the course more resource-intensive than the Machine Learning part?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, Saturn Cloud can be used for the Deep Learning part of the course. The context provided explicitly states that Saturn Cloud is used for deep learning tasks, although other cloud services could also be used. Additionally, there are specific templates and resources available for deep learning on Saturn Cloud, as mentioned in the second context regarding the location of the Python TensorFlow template.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Can Saturn Cloud be used for the Deep Learning part of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to use a cloud service other than Saturn Cloud for the Deep Learning section. The Deep Learning part of the course is resource intensive, but you can use any cloud service to meet the requirements.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Is it possible to use a cloud service other than Saturn Cloud for the Deep Learning section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find instructions to install TensorFlow GPU on Ubuntu in this article: [ML Zoomcamp 2023 Project](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/).',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Where can I find instructions to install TensorFlow GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a guide available to set up TensorFlow with GPU support on Ubuntu. Please refer to the following article for detailed instructions: [ML Zoomcamp 2023 Project Setup Article](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/).',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Is there a guide to set up TensorFlow with GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To configure GPU support for TensorFlow on your Ubuntu machine, you will need to set up CUDA and cuDNN. The process can be complex, but there are resources available to guide you through it. Here is a simplified guide to help you get started:\\n\\n1. **Install Nvidia Drivers:**\\n   Visit the Nvidia website to download and install the appropriate drivers for your GPU: [Nvidia Drivers](https://www.nvidia.com/download/index.aspx).\\n\\n2. **Install the CUDA Toolkit:**\\n   Download and install the CUDA Toolkit. You can find previous versions of the toolkit from the CUDA Toolkit Archive: [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive).\\n\\n3. **Install cuDNN:**\\n   Download and install cuDNN from the Nvidia website: [cuDNN Download](https://developer.nvidia.com/rdp/cudnn-download).\\n\\n4. **Configure Environment Variables:**\\n   Make sure to set the necessary environment variables in your system. For instance, you can add the following line to your `.bashrc` or `.zshrc` file:\\n   ```sh\\n   export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CUDA_PATH/lib/\\n   ```\\n\\n5. **Install TensorFlow with GPU Support:**\\n   Install TensorFlow using pip:\\n   ```sh\\n   pip install tensorflow-gpu\\n   ```\\n\\nIf you prefer a step-by-step tutorial or if you encounter any difficulties, you might find this article helpful: [ML Zoomcamp 2023 Project](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/).',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'How do I configure GPU support for TensorFlow on my Ubuntu machine?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Sure! You can follow the guide in the provided article for setting up TensorFlow with GPU support on Ubuntu: [TensorFlow with GPU on Ubuntu Guide](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/). This article has successfully worked for others and should help you with the installation process.',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Can you direct me to a resource for installing TensorFlow with GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set up TensorFlow GPU on Ubuntu, you need to follow several steps which include setting up CUDA and cuDNN. Here’s a simplified guide to help you through the process:\\n\\n1. **Install Nvidia Drivers**: \\n    - Download and install the appropriate drivers from the NVIDIA website: [Nvidia drivers download](https://www.nvidia.com/download/index.aspx)\\n\\n2. **Install CUDA Toolkit**: \\n    - Download and install CUDA Toolkit 11.x.x from the NVIDIA developer website: [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive)\\n\\n3. **Install cuDNN**: \\n    - Download and install cuDNN from the NVIDIA developer website: [cuDNN Download](https://developer.nvidia.com/rdp/cudnn-download)\\n\\n4. **Install TensorFlow**: \\n    - You can install TensorFlow with GPU support by following the TensorFlow official installation guide for Linux: [Install TensorFlow with pip](https://www.tensorflow.org/install/pip#linux_1). Make sure to follow step 4 to install CUDA for your environment.\\n\\nAdditionally, you can refer to this article for a detailed explanation: [ML Zoomcamp 2023 Project](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/).\\n\\nMake sure to verify that your setup is correct by running a small TensorFlow script to check if it recognizes your GPU. This will help ensure everything is correctly configured.',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'What is the procedure to set up TensorFlow GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To join a channel in Slack, follow these steps:\\n\\n1. Click on “All channels” at the top of your left sidebar. If you don\\'t see this option, click “More” to find it.\\n2. Browse through the list of public channels in your workspace or use the search bar to look for a specific channel by its name or description.\\n3. Select the desired channel from the list to view it.\\n4. Click \"Join Channel.\"\\n\\nFor more detailed instructions, you can visit this link: [How to join a channel in Slack](https://slack.com/help/articles/205239967-Join-a-channel).',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'How can I join a channel in Slack?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find and join course channels on Slack, follow these steps:\\n\\n1. **Visit the Slack Help Article**: You can start by checking detailed instructions at [Slack Help – Join a Channel](https://slack.com/help/articles/205239967-Join-a-channel).\\n\\n2. **Access All Channels**:\\n    - Click “All channels” at the top of your left sidebar in Slack.\\n    - If you do not see “All channels,” click “More” to find the option.\\n\\n3. **Browse or Search Channels**:\\n    - Browse the list of public channels in your workspace.\\n    - Alternatively, use the search bar to search by channel name or description.\\n\\n4. **Select and Join a Channel**:\\n    - Select a channel from the list to view it.\\n    - Click “Join Channel” to become a member of that channel.\\n\\nBy following these steps, you should be able to find and join the necessary course channels in Slack.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'What steps should I follow to find course channels on Slack?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Based on the provided context, there is no explicit mention that you need to give the URL to your GitHub repository. However, it is implied that if you are sharing your work or need it to be reviewed, the repository should be public so that the person correcting the homework can access it. Therefore, while it is not explicitly required to provide the URL, making your repository public is necessary for it to be accessed by others, including course staff. \\n\\nHere's the relevant part from the context:\\n\\n> Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\",\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Is there a need to give the URL to our GitHub repo?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You need to provide the URL to your GitHub repository for your homework in order to receive a grade. You should submit the URL via the method specified by your instructor or course platform for homework submissions.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Where do I provide the GitHub URL for our homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To join a Slack channel, follow these steps:\\n\\n1. Click “All channels” at the top of your left sidebar in Slack. If you don't see this option, click “More” to find it.\\n2. Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\n3. Select a channel from the list to view it.\\n4. Click “Join Channel”.\\n\\nFor more detailed instructions, you can refer to the official Slack help article: [Join a channel](https://slack.com/help/articles/205239967-Join-a-channel).\",\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Can you show me how to join a Slack channel?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can still participate if you join the course late. While you won't be able to submit some of the homeworks, you can still take part in the course. To earn a certificate, you will need to submit 2 out of 3 course projects and review 3 peers’ projects by the deadline. This means that even if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'If I join the course late, can I still participate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you will be able to obtain a certificate if you join the course after it has started. To be eligible for a certificate, you need to submit at least 2 out of 3 course projects and review 3 peers' projects by the deadline, even if you join the course later.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'Will I be able to obtain a certificate if I join the course after it has started?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To be eligible for a certificate, you must submit at least 2 out of 3 course projects and also review 3 course-mates’ projects for each submission.',\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'How many course projects must I submit to be eligible for a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To get a certificate, you are required to review at least 3 peers' projects by the deadline.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'What is the requirement for peer reviews to get a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can still join the course after it has started. However, you won't be able to submit some of the homeworks as the forms are closed after the due dates. While homework is not mandatory for completing the course, you need to submit 2 out of 3 course projects and review 3 peers’ projects by the deadline to be eligible for a certificate. So, even if you join the course later, you can still participate and work towards earning a certificate.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'Can I submit homework if I join the course after it has started?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course is available in a self-paced mode. You can go through the materials at any time.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Is the course available in a self-paced mode?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can start the course anytime. The course is available in a self-paced mode, allowing you to go through the materials at your convenience.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Can I start the course anytime?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The next cohort will start in September 2024, with future iterations potentially occurring in subsequent Septembers as well.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'When is the next cohort starting?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, there are no set times for the course materials. The course is available in a self-paced mode, so you can go through the materials at any time. However, if you prefer to go through the materials as a cohort with other students, specific iterations of the course start in September each year.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Are there set times for the course materials?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there will be iterations in future Septembers. The next iterations are planned for September 2023 and September 2024, and potentially other Septembers as well.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Will there be iterations in future Septembers?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Submitting your homework late depends on whether the submission form is still open. If the form is still open, you can submit your homework and it will be evaluated. However, if the form is closed after the due date, then it’s too late to submit. Homework is not mandatory for finishing the course, so missing a submission won’t prevent you from completing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Can I send in my homework late?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not feasible to submit homework post deadline. The form is closed after the due date. However, homework is not mandatory for finishing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Is it feasible to submit homework post deadline?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you miss the homework submission deadline, you won't be able to submit it because the form will be closed. However, missing the homework is not mandatory for finishing the course, so it won't affect your ability to complete the course.\",\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'What happens if I miss the homework submission deadline?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it’s not possible to submit homework after the due date because the form is closed. However, homework is not mandatory for finishing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'After the due date, can homework still be submitted?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, late homework submissions are not accepted. The form will be closed after the due date, so it’s not possible to submit homework once the form is closed. However, don't worry as homework is not mandatory for finishing the course.\",\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Are late homework submissions accepted?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The initial step after joining the course is to go to the course page (http://mlzoomcamp.com/), scroll down, and start going through the course materials. Then, you should read everything in the cohort folder for your cohort’s year, watch the videos, and also check out the office hours from previous cohorts. You can also use the link http://mlzoomcamp.com/#syllabus to access the syllabus directly.',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'What is the initial step after joining the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To view the content of the course, go to the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/), scroll down, and start going through the course materials. Additionally, read everything in the cohort folder for your cohort’s year. You can also watch the videos and office hours from previous cohorts by visiting the DTC YouTube channel, clicking on \"Playlists,\" and searching for the specific course year. Alternatively, you can directly access the syllabus using this link: [http://mlzoomcamp.com/#syllabus](http://mlzoomcamp.com/#syllabus).',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'How can I view the content of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the materials related to your cohort by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/). Once there, scroll down to access the course materials and read everything in the cohort folder for your cohort’s specific year. Additionally, you can watch videos and office hours from previous cohorts on the DTC YouTube channel by searching for your specific course year in the Playlists section. Another direct link to the syllabus is [http://mlzoomcamp.com/#syllabus](http://mlzoomcamp.com/#syllabus).',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'Where can I find the materials related to my cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To access the videos from previous office hours, go to the DTC YouTube channel and click on Playlists. Search for the course you\\'re interested in, labeled with the year it was conducted (such as \"ML Zoomcamp 2021\"). You can also find the videos in the course playlist on YouTube. Links to these resources can be found on the course page (http://mlzoomcamp.com/#syllabus).',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'How do I access the videos from previous office hours?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'ML Zoomcamp was first launched in 2021.',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'When was ML Zoomcamp first introduced?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the 2023 cohort, you can find the deadlines [here](it’s taken from the 2023 cohort page).',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where can I find the deadlines for the 2023 cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the 2023 cohort, you can see the deadline information on the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where is the deadline information available for the 2023 cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the deadlines for the 2023 course, you can view them on the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'How do I check the deadlines for the 2023 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the 2023 cohort, you can see the deadlines on the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Can you show me where to see the 2023 cohort deadlines?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the 2023 cohort, you can see the deadlines on the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where are the 2023 cohort deadlines listed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The primary changes between the 2022 and 2023 versions of the course are the introduction of different homework assignments and the removal of one special module, BentoML, which was present in the 2022 iteration. The rest of the modules remain the same as in 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'What has changed between the 2022 and 2023 versions of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a module missing in the 2023 course compared to 2022. The 2022 course included a special module on BentoML, which is not present in the 2023 course.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Is there any module missing in the 2023 course compared to 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the homework assignments in the 2023 course are different from those in 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Are the homework assignments in the 2023 course the same as in 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, BentoML was included in the 2022 course but not in the 2023 course. While the rest of the modules remained the same, the special module on BentoML was specific to the 2022 iteration.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Was BentoML included in the 2022 course but not in the 2023 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the majority of the modules are unchanged from the 2022 course. Only one special module (BentoML) from the previous iteration is different, while the rest of the modules remain the same.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Are the majority of modules unchanged from the 2022 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We will not be re-recording the course videos. The videos from the 2021 iteration will be used, as the focus of the course and the skills we aim to teach have remained the same, and the videos are still up-to-date. If you haven't taken part in the previous iteration, you can start watching those videos now and benefit from them. Please note that we recommend using Python 3.10 now instead of Python 3.8.\",\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Will you be releasing new course videos or using the ones from 2021?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the course videos are not being re-recorded for this iteration. The course will use videos from the previous iteration as they are still current and relevant.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Are the course videos being re-recorded for this iteration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, there is no difference in the skills taught in this iteration compared to 2021. The focus of the course and the skills remain the same, even though some homework assignments and one special module differed in the 2022 iteration. The course videos from 2021 are still being used as they are considered up-to-date.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Is there any difference in the skills taught in this iteration compared to 2021?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you should watch the videos even if you didn't take the course in 2021. The course videos are still up-to-date and cover the same focus and skills that are being taught now. They will be useful for you and you will learn new things. Just note that it is recommended to use Python 3.10 instead of Python 3.8.\",\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': \"Should I watch the videos if I didn't take the course in 2021?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The recommended Python version for this course iteration is Python 3.10.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Which Python version is recommended for this course iteration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When posting about your course learning on social media, you should use the tag #mlzoomcamp.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'What tag should I use when posting about my course learning on social media?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When submitting your homework, you should include your learning in public links in the designated section of the form. You can separate multiple links by any whitespace character such as a linebreak, space, or tab. Make sure to use the tag #mlzoomcamp when you post about your learning on social media. Note that you can earn extra points for these posts, capped at 7 points. Therefore, even if you submit more than 7 links, you will only earn a maximum of 7 points for this part.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'How should I submit my learning in public links when turning in homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not possible to earn more than 7 points for posting learning links in weekly homework. According to the guidelines provided, even if you submit more than 7 links, the extra scores for posting these learning in public links are capped at 7 points per week.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'Is it possible to earn more than 7 points for posting learning links in weekly homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can post the same content on up to 7 different social sites and still earn points for your homework. You will earn 7 points if you add up to 7 URLs per week. For midterms and capstone projects, the points are doubled, allowing you to earn up to 14 points for 14 URLs.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'Can I post the same content on multiple social sites to earn points for my homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For midterms and capstones, you can earn up to 14 points for posting public learning links. This is because the awarded points for these projects are doubled due to their longer duration. So you will need to make 14 posts to earn the maximum of 14 points.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'How many points can I earn for posting public learning links during midterms and capstones?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you should not share your course projects on a public platform. The context does not provide explicit permission to share course projects publicly, and given the emphasis on solo projects and peer reviews, it is advisable to keep project submissions private. However, you are encouraged to create a personal GitHub repository for your own notes, homework, and projects for private use.',\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Can I share my course projects on a public platform?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To add your notes to the Community Notes section, follow these steps:\\n\\n1. Create your own GitHub repository and include your notes, homework, projects, etc. in it.\\n2. Fork the original course repository.\\n3. Add a link to your notes in your repository under the 'Community Notes' section of the forked course repository.\\n4. Create a pull request to sync your forked repository with the original course repository.\\n\\n(By Wesley Barreto)\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'How do I add my notes to the Community Notes section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To sync your changes with the original course repository, follow these steps:\\n\\n1. Fork the original course repository to create your own copy.\\n2. Make your changes in your forked repository (e.g., add notes, homework, projects).\\n3. Add a link to your notes under the 'Community Notes' section in your own repository.\\n4. Create a pull request in the original course repository to sync your fork with it.\\n\\nThis will ensure that your contributions are integrated with the original course materials.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'What is the process to sync my changes with the original course repo?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You should host your course-related notes, homework, projects, etc., in your own GitHub repository. To share these with the community, you can fork the original course repository and add a link under the 'Community Notes' section to the notes that are in your repository. After doing so, create a pull request to sync your fork with the original course repository.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Where should I host my course-related notes and projects?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, there is a specific location to add a link to your own repo in the course repository. You should add your link under the 'Community Notes' section of the original course repo. After adding your link, create a pull request to sync your fork with the original course repo.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Is there a specific location to add a link to my own repo in the course repository?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the leaderboard links for 2023 and 2022 at the following URLs:\\n\\n- **2023 Leaderboard**: [https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml](https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml)\\n\\n- **2022 Leaderboard**: [https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml](https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml)',\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'Where can I find the leaderboard links for 2023 and 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To compute the hash for your email using Python, follow these steps:\\n\\n1. Copy the following Python code:\\n\\n```python\\nfrom hashlib import sha1\\n\\ndef compute_hash(email):\\n    return sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n\\n# Example usage **** enter your email below (YOUR_EMAIL_HERE)****\\nemail = \"YOUR_EMAIL_HERE\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\n```\\n\\n2. Replace `\"YOUR_EMAIL_HERE\"` with your actual email address inside the quotes.\\n\\n3. Run the code in your Python environment. \\n\\nFor example, if your email is `example1@gmail.com`, the code would be:\\n\\n```python\\nfrom hashlib import sha1\\n\\ndef compute_hash(email):\\n    return sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n\\n# Example usage **** enter your email below (example1@gmail.com)****\\nemail = \"example1@gmail.com\"\\nhashed_email = compute_hash(email)\\nprint(\"Original Email:\", email)\\nprint(\"Hashed Email (SHA-1):\", hashed_email)\\n```\\n\\nRun this script, and it will print the original email and its SHA-1 hashed value.\\n\\nAlternatively, you can use the website [http://www.sha1-online.com](http://www.sha1-online.com) to hash your email directly by entering your email there and copying the hashed result.',\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'How do I compute the hash for my email using Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, you do not need to format your email in any special way before computing the hash. You just need to ensure that your email is in lowercase and enclosed in quotes to denote that it is a string. Here is the Python code provided for computing the hash:\\n\\n```python\\nfrom hashlib import sha1\\ndef compute_hash(email):\\n    return sha1(email.lower().encode('utf-8')).hexdigest()\\n\\n# Example usage\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\n```\\n\\nSimply replace `'YOUR_EMAIL_HERE'` with your actual email address. Alternatively, you can use the website http://www.sha1-online.com to compute the hash by entering your email and copying the resulting hash.\",\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'Do I need to format my email in any special way before computing the hash?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can use an online tool to compute your email's hash. Specifically, you can use the website http://www.sha1-online.com. Just enter your email on the site, and it will compute the hashed value for you.\",\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': \"Is there an online tool I can use to compute my email's hash?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To check your scores on the leaderboard using your hashed email, follow these steps:\\n\\n1. **Compute the Hash of Your Email**:\\n   Use the provided Python code to generate the SHA-1 hash of your email. Here's the code snippet:\\n   ```python\\n   from hashlib import sha1\\n\\n   def compute_hash(email):\\n       return sha1(email.lower().encode('utf-8')).hexdigest()\\n\\n   print(compute_hash('YOUR_EMAIL_HERE'))\\n   ```\\n   Replace `'YOUR_EMAIL_HERE'` with your actual email address, making sure to include the quotes around it.\\n\\n2. **Run the Code**:\\n   Run this code in your Python environment. This will output a hashed string which is your email's SHA-1 hash.\\n\\n3. **Visit the Leaderboard Link**:\\n   Open the relevant Google Sheets leaderboard link for your year:\\n   - **2023 Leaderboard**: [2023 Leaderboard Link](https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml)\\n   - **2022 Leaderboard**: [2022 Leaderboard Link](https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml)\\n\\n4. **Search for Your Hashed Email**:\\n   Once you have the hashed string from the Python code, copy it and use the search function in the Google Sheets leaderboard (usually `Ctrl + F` or `Command + F` depending on your device) to find your scores.\\n\\nAlternatively, you can use the website [sha1-online.com](http://www.sha1-online.com) to generate the hash by entering your email and then using the resulting hash to search in the leaderboard similarly.\",\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'How do I use the hashed email to check my scores on the leaderboard?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To resolve the error \\'wget is not recognized as an internal or external command\\' on a Windows system, you need to install `wget`. The easiest way to do this is by using Chocolatey, a package manager for Windows. Here’s how you can do it:\\n\\n1. Install Chocolatey (if not already installed) by following the instructions at [Chocolatey\\'s installation page](https://chocolatey.org/install).\\n\\n2. Open Command Prompt or PowerShell as an Administrator and run the following command:\\n   ```sh\\n   choco install wget\\n   ```\\n\\nAlternatively, if you prefer not to use Chocolatey, you can manually download a `wget` binary:\\n\\n1. Go to [this link](https://eternallybored.org/misc/wget/) to download the `wget` binary for Windows.\\n2. Extract the downloaded file and place the `wget.exe` in a directory of your choice, e.g., `C:/tools/`.\\n3. Add this directory to your system’s PATH environment variable:\\n   - Right-click on \\'This PC\\' or \\'Computer\\' on your Desktop or in File Explorer.\\n   - Select \\'Properties\\'.\\n   - Click on \\'Advanced system settings\\'.\\n   - In the System Properties window, click on the \\'Environment Variables\\' button.\\n   - In the Environment Variables window, find the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\n   - Add the path to the directory where `wget.exe` is located (e.g., `C:/tools/`) and click OK.\\n\\nLastly, if you prefer to use alternative tools, you can use Python’s `wget` library or built-in modules to achieve similar functionality. Here are a couple of alternatives:\\n\\n### Using Python\\'s wget library:\\n1. Install the wget library:\\n   ```sh\\n   pip install wget\\n   ```\\n2. Use it in your Python code:\\n   ```python\\n   import wget\\n   wget.download(\"URL\")\\n   ```\\n\\n### Using Python’s urllib module:\\n```python\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n```\\n\\nThese methods should help you bypass the issue of `wget` not being recognized on a Windows system.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': \"How can I resolve 'wget is not recognized as an internal or external command' on a Windows system?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To install `wget` on a Mac, follow these steps:\\n\\n1. **Install Homebrew**: If you don\\'t already have Homebrew installed, you can install it by running the following command in your terminal:\\n   ```sh\\n   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\\n   ```\\n\\n2. **Install `wget` with Homebrew**: Once Homebrew is installed, you can install `wget` by running:\\n   ```sh\\n   brew install wget\\n   ```',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'What are the steps to install wget on a Mac?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can use Python to perform the same task as `wget`. Here are a few ways to do it:\\n\\n1. **Using the `wget` Python library**:\\n   First, you need to install the `wget` library using pip:\\n   ```sh\\n   pip install wget\\n   ```\\n   Then you can use it in your Python script like this:\\n   ```python\\n   import wget\\n   wget.download(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n   ```\\n   This will download the file to the same directory where your script is located.\\n\\n2. **Using `pandas` to read a CSV file directly from a URL**:\\n   If you are working with CSV files, you can use the `pandas` library to read the file directly from the URL:\\n   ```python\\n   import pandas as pd\\n   url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n   df = pd.read_csv(url)\\n   ```\\n\\n3. **Using the built-in `urllib.request` module**:\\n   You can use the `urllib.request` module, which is a standard library in Python for making HTTP requests:\\n   ```python\\n   import urllib.request\\n   url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n   urllib.request.urlretrieve(url, \"housing.csv\")\\n   ```\\n   This will download the file and save it as \"housing.csv\" in your current directory.\\n\\nEach of these methods allows you to download a file from a URL in Python, similar to how you would use `wget`.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'Can I use Python to perform the same task as wget? If so, how?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To download files from URLs in Python using the `urllib.request` module, you can use the `urlretrieve()` function. Here is an example:\\n\\n```python\\nimport urllib.request\\n\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n```\\n\\nThis function downloads the file from the specified URL and saves it locally as \"housing.csv\".',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': \"What function in Python's urllib.request can I use to download files from URLs?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can read a CSV file directly from a URL using pandas by using the `pd.read_csv()` function and passing the URL as an argument. Here\\'s the code snippet to do this:\\n\\n```python\\nimport pandas as pd\\n\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\n```\\n\\nThis will load the CSV file from the provided URL directly into a pandas DataFrame. Valid URL schemes include `http`, `ftp`, `s3`, `gs`, and `file`.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'How can I read a CSV file directly from a URL using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To download a CSV file inside a Jupyter notebook, there are multiple methods you can use based on the given context:\\n\\n1. **Using Python `wget` Library**:\\n    ```python\\n    import wget\\n    wget.download(\"URL\")\\n    ```\\n    Install `wget` first if needed:\\n    ```python\\n    pip install wget\\n    ```\\n\\n2. **Using Pandas**:\\n    ```python\\n    import pandas as pd\\n    url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n    df = pd.read_csv(url)\\n    ```\\n\\n3. **Using Built-in Python Functionality (`urllib.request`)**:\\n    ```python\\n    import urllib.request\\n    url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n    urllib.request.urlretrieve(url, \"housing.csv\")\\n    ```\\n\\n4. **Using Wget Command**:\\n    ```python\\n    !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n    ```\\n\\n5. **Using Kaggle API (requires Kaggle account and `kaggle.json` setup)**:\\n    ```python\\n    import os\\n    # Set your kaggle.json path\\n    os.environ[\\'KAGGLE_CONFIG_DIR\\'] = \"<STRING_OF_YOUR_FILE_PATH>\"\\n    \\n    # Download dataset\\n    !kaggle datasets download -d kapturovalexander/bank-credit-scoring\\n    \\n    # Unzip the downloaded file\\n    !unzip -o bank-credit-scoring.zip\\n    ```\\n\\nEach approach has its own context of use, so you can choose the one that best suits your specific requirements.',\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How can I download a CSV file inside a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In a Jupyter notebook, the exclamation mark (!) is used to run shell commands directly from a cell. For example, instead of using the `ls` command to list directory contents in a Windows environment, you can use `!dir` to achieve a similar outcome.\\n\\n',\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'What does the exclamation mark (!) do in a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In a Jupyter notebook, you can execute shell commands such as `ls`, `cp`, and `mkdir` by prefixing them with an exclamation mark (`!`). However, if you are using a Windows system and encounter an error stating that `'ls' is not recognized as an internal or external command`, the solution is to use the equivalent Windows command. For example:\\n\\n- Instead of `!ls -lh`, you can use `!dir` to get a similar output in Windows.\\n\\nThis method allows you to execute shell commands directly within a Jupyter notebook cell.\\n\\nFor example:\\n```python\\n# List the contents of the current directory in Windows\\n!dir\\n\\n# Copying files in a generic way\\n!cp source_path destination_path\\n\\n# Making a new directory\\n!mkdir new_directory_name\\n```\\n\\nThis should enable you to run these commands smoothly within your Jupyter notebook.\",\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How can I execute shell commands like ls, cp, and mkdir in a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To create a directory named 'data', you can use the following command:\\n\\n```sh\\n!mkdir -p data\\n```\\n\\nThis command can be executed inside a Jupyter Notebook to create the directory.\",\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': \"What command do I use to create a directory named 'data'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To move a file into a different directory within a Jupyter notebook, you can use shell commands. Specifically, you can use the `mv` command to move a file. Here’s an example of how you could do it:\\n\\n```python\\n!mv filename.extension /path/to/destination/\\n```\\n\\nFor instance, if you wanted to move a file called `example.txt` to a directory named `data`, you would use:\\n\\n```python\\n!mv example.txt data/\\n```\\n\\nNote that the exclamation mark `!` allows you to execute shell commands inside your Jupyter notebooks. This method works for various shell commands such as `ls`, `cp`, `mkdir`, `mv`, etc.',\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How do I move a file into a different directory within a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set up a WSL development environment on a Windows 11 device, follow these steps:\\n\\n1. **Install WSL:**\\n   - Open the \"Windows Features\" dialog by searching for \"features\" in the Windows search bar.\\n   - Ensure that the \"Virtual Machine Platform\" feature is activated by checking the relevant checkbox.\\n   - Make sure that your system is capable of virtualization and that it is enabled in your BIOS settings.\\n\\n2. **Install a Linux Distribution:**\\n   - Open the Microsoft Store and search for a Linux distribution such as Ubuntu or Debian.\\n   - Click on the desired distribution and install it.\\n   - Once installed, open the app and set a username and password. Note: When you type your password, the input will be invisible.\\n\\n3. **Setup and Basic Commands:**\\n   - Open your Linux terminal and test some basic commands like `pwd` to confirm that you are in the Linux environment.\\n   - To navigate to your Windows file system, use the command `cd ../..` to move back two directories, then `cd mnt` to access the \"mnt\" directory. Here, you can find and navigate to your Windows folders.\\n\\n4. **Configure Python:**\\n   - Python should be pre-installed, but you can verify and install it by running: `sudo apt install python3`.\\n   - Optionally, make a specific folder your default when you start your Ubuntu terminal by adding this command to your `.bashrc` file: `echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc`.\\n   \\n5. **Additional Configurations:**\\n   - To disable bell sounds, you can modify the `inputrc` file: `sudo vim /etc/inputrc`, uncomment the line `set bell-style none`, and save the changes.\\n   - Install pip using: `sudo apt install python3-pip`.\\n\\n6. **Connect to VS Code:**\\n   - Download and install VS Code on your Windows system.\\n   - Install the Microsoft verified VS Code extension \\'WSL\\'.\\n   - This extension allows you to remotely connect to your WSL Ubuntu instance from within VS Code, essentially treating it like a virtual machine.\\n\\nFor detailed guidance, you can refer to the Microsoft Learn documentation at [Set up a WSL development environment | Microsoft Learn](https://learn.microsoft.com/en-us/windows/wsl/setup/environment).\\n\\nBy following these steps, you will have a functional WSL development environment on your Windows 11 device, integrated with VS Code for a seamless development experience.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'How can I set up a WSL development environment on a Windows 11 device?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To connect your WSL Ubuntu instance to VS Code on Windows 11, follow these steps:\\n\\n1. **Set Up a WSL Development Environment**:\\n   - Visit the Microsoft Learn link: [Set up a WSL development environment](https://learn.microsoft.com/en-us/windows/wsl/setup/environment).\\n\\n2. **Install VS Code and the WSL Extension**:\\n   - Download and install Visual Studio Code (VS Code) from the official website.\\n   - Open VS Code and go to the Extensions view by clicking the Extensions icon in the Activity Bar on the side of the window or by pressing `Ctrl+Shift+X`.\\n   - Search for \"WSL\" and install the Microsoft verified VS Code extension \"Remote - WSL\". This extension allows you to remotely connect to your WSL Ubuntu instance as if it were a virtual machine.\\n\\nBy following these steps, you can effectively connect and work with your WSL Ubuntu instance using VS Code on a Windows 11 device.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'How do I connect my WSL Ubuntu instance to VS Code on Windows 11?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To connect VS Code to WSL, you should download the Microsoft verified VS Code extension called 'WSL'. This extension allows you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.\",\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Which extension should I download to connect VS Code to WSL?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can use VS Code to remotely connect to your WSL Ubuntu instance. To do this, download the Microsoft verified VS Code extension called ‘WSL’. This extension will allow you to connect to your WSL Ubuntu instance as if it were a virtual machine. For more detailed instructions, you can refer to the Microsoft Learn guide on setting up a WSL development environment.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Can I use VS Code to remotely connect to my WSL Ubuntu instance?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, there is a guide on Microsoft Learn for setting up WSL on Windows 11. You can use the link provided in the context: [Set up a WSL development environment | Microsoft Learn](https://docs.microsoft.com/learn/modules/set-up-wsl-development-environment/).\\n\\nThis guide will help you to access Linux using the built-in WSL feature in Windows 11. Additionally, to connect WSL to VS Code, you can download the Microsoft verified VS Code extension 'WSL', which allows remote connection to your WSL Ubuntu instance.\",\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Is there a guide on Microsoft Learn for setting up WSL on Windows 11?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix the error message \\'src refspec master does not match any\\' when using `git push`, you should follow these steps:\\n\\n1. Commit your changes using:\\n   ```\\n   git commit -m \"initial commit\"\\n   ```\\n2. Push your commit to the `main` branch instead of `master` by running:\\n   ```\\n   git push origin main\\n   ```\\n\\nIf this is your first time using GitHub, you might find it helpful to go through a tutorial such as [this one](https://dennisivy.com/github-quickstart).\\n\\nAlternatively, you can use GitHub\\'s \"upload file\" functionality or directly share your code from Google Colab if you\\'re coding there.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"How can I fix the error message 'src refspec master does not match any' when using git push?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If this is your first time using GitHub, you can find a great and straightforward tutorial at [Dennis Ivy's GitHub Quickstart](https://dennisivy.com/github-quickstart). This guide should help you get started with using GitHub.\",\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"Where can I find a tutorial on using GitHub if it's my first time?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To push an initial commit to GitHub, you can follow these steps based on the provided context:\\n\\n1. **Initialize your repository (if not already initialized):**\\n    ```bash\\n    git init\\n    ```\\n\\n2. **Add your files to the staging area:**\\n    ```bash\\n    git add .\\n    ```\\n\\n3. **Create an initial commit:**\\n    ```bash\\n    git commit -m \"initial commit\"\\n    ```\\n\\n4. **Connect your local repository to the remote repository on GitHub (if not already done):**\\n    ```bash\\n    git remote add origin https://github.com/your-username/your-repo.git\\n    ```\\n\\n5. **Push your initial commit to GitHub:**\\n    ```bash\\n    git push origin main\\n    ```\\n\\nMake sure your branch name, in this case, \\'main\\', matches with the branch name on GitHub. If \\'main\\' is not the default branch on your remote repository, replace \\'main\\' with the appropriate branch name. Additionally, if you encounter authentication issues, consider using a personal access token as recommended for GitHub authentication.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': 'What are the steps to push an initial commit to GitHub?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can use the 'upload file' functionality on GitHub to submit your homework. According to the context provided, this is an acceptable method. You can also upload your code directly from Google Colab to GitHub if you prefer.\",\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"Can I use the 'upload file' functionality on GitHub to submit my homework?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to directly share code from Google Colab to GitHub. As per the information provided by Pranab Sarma:\\n\\n> If you write your code on Google Colab you can also directly share it on your Github.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': 'Is it possible to directly share code from Google Colab to GitHub?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A singular matrix error occurs during matrix inversion in the homework for several reasons:\\n\\n1. **Non-Invertible Matrix**: Not every matrix can be inverted. A matrix is singular if it does not have an inverse. This often happens in cases where the determinant of the matrix is zero.\\n\\n2. **Order of Multiplication**: Multiplication in matrices is not commutative, meaning \\\\(X.dot(Y)\\\\) is not necessarily equal to \\\\(Y.dot(X)\\\\). Ensuring the correct order of multiplication is crucial.\\n\\n3. **Redundant Inversion**: If you perform matrix inversion on the same matrix more than once in your code, it could lead to errors.\\n\\nThese issues need careful handling to avoid encountering singular matrix errors in your homework.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'Why does a singular matrix error occur during matrix inversion in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To avoid getting a singular matrix error while using the `.dot` method in the homework, it's essential to pay attention to the order of multiplication, because matrix multiplication is not commutative. This means that \\\\( X.dot(Y) \\\\) is not necessarily equal to \\\\( Y.dot(X) \\\\). Ensuring that matrices are multiplied in the correct order is crucial to avoid inadvertently creating a singular matrix, which cannot be inverted. Additionally, be mindful not to attempt the inversion of a matrix multiple times within your code, as this can also lead to errors.\",\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'How can I avoid getting a singular matrix error while using .dot method in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The primary cause of a singular matrix error in matrix multiplication tasks is due to the fact that not every matrix can be inverted. This particular situation arises in part because matrix multiplication is not commutative (i.e., \\\\(X.dot(Y) \\\\) is not necessarily equal to \\\\(Y.dot(X)\\\\)). Therefore, it is essential to respect the order of multiplication to avoid ending up with a singular matrix that cannot be inverted. Additionally, you may encounter a singular matrix error if you attempt to invert the same matrix multiple times in your code.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'What is the primary cause of a singular matrix error in matrix multiplication tasks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'X.dot(Y) is not necessarily equal to Y.dot(X) due to the fundamental properties of matrix multiplication. For matrix multiplication, the order in which you multiply the matrices affects the result of the multiplication. Specifically, the number of columns in the first matrix must match the number of rows in the second matrix to perform the multiplication. Changing the order can lead to a dimension mismatch or different resultant matrices even if the multiplication is valid for both orders.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'In the homework, why is X.dot(Y) not necessarily equal to Y.dot(X)?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To prevent singular matrix errors in the machine learning course, you should:\\n\\n1. **Pay close attention to matrix multiplication**: Ensure that you respect the order of multiplication (using the `.dot` method), as matrix multiplication is not commutative. This means that `X.dot(Y)` is not the same as `Y.dot(X)`.\\n\\n2. **Check for repeated inversion**: Be mindful not to invert a matrix more than once in your code, as this can also lead to singular matrix errors.\\n\\nBy following these guidelines, you can avoid the common pitfalls that lead to singular matrix errors.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'What should I pay close attention to in order to prevent singular matrix errors in the machine learning course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If the `conda` command is not recognized as an internal command, it could be due to several reasons. Here are the steps you can follow to resolve this issue:\\n\\n1. **Ensure Anaconda or Miniconda is Installed:**\\n   If you don’t have Anaconda or Miniconda installed, you need to install one of them first. You can download and install Anaconda from its official website.\\n\\n2. **Use the Anaconda Prompt:**\\n   If you are using Windows and have installed Anaconda, make sure to use Anaconda’s own terminal called “Anaconda Prompt”. This terminal is pre-configured to recognize the `conda` command.\\n\\nBy following these steps, you should be able to get the `conda` command working on your system.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': 'What should I do if conda is not recognized as an internal command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the command `conda create -n ml-zoomcamp python=3.9` will work with Python 3.8. The CONTEXT indicates that any version of Python 3.8, 3.9, or 3.10 should be fine for setting up the environment.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Will the command 'conda create -n ml-zoomcamp python=3.9' work with Python 3.8?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not strictly necessary to use Anaconda\\'s own terminal on Windows. However, if you\\'re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called \"Anaconda Prompt\" to ensure compatibility and smooth operation with Anaconda-specific commands.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Is it necessary to use Anaconda's own terminal on Windows?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, if you don't already have Anaconda or Miniconda, you should install it first. This is necessary to effectively create and manage the conda environments required for the project.\",\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Do I need to install Anaconda or Miniconda if I don't already have them?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the specified Python version for the course does not need to be exactly 3.9. The recommended version for the 2023 cohort is Python 3.10.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': 'Does the specified Python version for the course need to be exactly 3.9?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If your code to read a dataset with Pandas in Windows is not working, it is likely due to the issue with the backslash (`\\\\`) used in the file path. In Windows, the backslash is used to navigate through files but it also serves a purpose as an escape sequence in Python, which can cause conflicts.\\n\\nFor example, if you have:\\n```python\\ndf = pd.read_csv('C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n```\\nThis might not work as intended because the backslashes are being treated as escape sequences.\\n\\nTo solve this problem, you can add an `r` before the file path string to make it a raw string, which tells Python to treat the backslashes as literal characters. Here’s the corrected code:\\n```python\\ndf = pd.read_csv(r'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n```\",\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"How do I read a dataset with Pandas in Windows when my code doesn't work?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The issue with your code `df = pd.read_csv('C:\\\\\\\\Users\\\\\\\\username\\\\\\\\Downloads\\\\\\\\data.csv')` not working in Windows stems from how backslashes (`\\\\`) are treated in Python strings. In Python, the backslash is used for escape sequences (e.g., `\\\\n` for a newline, `\\\\t` for a tab). Therefore, the backslashes in your file path might be interpreted as escape characters, causing errors.\\n\\nTo resolve this, you should prefix the string with an `r` to indicate that it is a raw string, meaning backslashes will be treated as literal characters and not escape sequences. Here is the correct way to load the file:\\n\\n```python\\ndf = pd.read_csv(r'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n```\\n\\nBy adding the `r` before the string, Python will interpret your path as a raw string, avoiding the conflict caused by escape sequences.\",\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"Why does my code df = pd.read_csv('C:\\\\Users\\\\username\\\\Downloads\\\\data.csv') not work in Windows?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The reason behind backslash causing an issue when reading files in Windows using Pandas is that Windows uses the backslash (`\\\\`) to navigate file paths. However, in Python, the backslash has a special meaning as it is used for escape sequences (e.g., `\\\\n` for a new line or `\\\\t` for a tab). This can lead to conflicts and errors when specifying file paths directly. To resolve this, you can add an \"r\" before the string to treat it as a raw string, which tells Python to treat backslashes as literal characters and not as escape sequences. Here\\'s how you should write the path:\\n```python\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n```\\n\\nThis way, Python will correctly interpret the backslashes in the file path.',\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': 'What is the reason behind backslash causing an issue when reading files in Windows using Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Adding 'r' before the file path when reading a file in Windows with Pandas makes Python treat the file path as a literal string. This avoids conflicts with Python's escape sequences, which can occur because Windows uses backslashes (\\\\) in file paths. You should use:\\n```python\\ndf = pd.read_csv(r'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n```\\nThis ensures that the backslashes are interpreted correctly as part of the file path rather than as escape characters.\",\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"What does adding 'r' before the file path do when reading a file in Windows with Pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To read a file with Pandas in Windows, you need to account for the backslash (`\\\\`) used in Windows file paths, which can cause issues in Python because it's also used for escape sequences. You can avoid this problem by prefixing the file path with `r` to treat it as a raw string. Here's an example code snippet:\\n\\n```python\\ndf = pd.read_csv(r'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n```\\n\\nThis code will correctly load the dataset without running into issues related to escape sequences.\",\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': 'Can you show an example of reading a file with Pandas in Windows?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix a \\'403 Forbidden\\' error when pushing to GitHub, you should run the following commands:\\n\\n1. First, run this command to check the current remote URL:\\n   ```sh\\n   git config -l | grep url\\n   ```\\n\\n   The output should look like this:\\n   ```sh\\n   remote.origin.url=https://github.com/github-username/github-repository-name.git\\n   ```\\n\\n2. Change the remote URL to include your GitHub username:\\n   ```sh\\n   git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n   ```\\n\\nMake sure the change is reflected by checking the remote URL again with the command in step 1.',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': \"What command should I run to fix a '403 Forbidden' error when pushing to GitHub?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the current URL configuration for your Git repository, you can use the following command:\\n\\n```bash\\ngit config -l | grep url\\n```\\n\\nThe output of this command should display the URL configuration, which typically looks like this:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'How can I check the current URL configuration for my Git repository?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Before making changes to fix a GitHub push error, the `remote.origin.url` should look like this:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'What should the remote.origin.url look like before making changes to fix a GitHub push error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix a \\'403 Forbidden\\' error when pushing to GitHub, follow these steps:\\n\\n1. Check your current remote URL configuration:\\n   ```bash\\n   git config -l | grep url\\n   ```\\n\\n2. The output should look something like this:\\n   ```bash\\n   remote.origin.url=https://github.com/github-username/github-repository-name.git\\n   ```\\n\\n3. Change the URL to include your GitHub username:\\n   ```bash\\n   git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n   ```\\n\\n4. Verify that the change has been made by repeating step 1 to ensure the new URL format is used.',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': \"How do I change the remote URL to fix a '403 Forbidden' error when pushing to GitHub?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'After updating the Git URL, you can ensure the changes are correctly applied by running the following command:\\n\\n```\\ngit config -l | grep url\\n```\\n\\nThe output should reflect the new URL format:\\n\\n```\\nremote.origin.url=https://github-username@github.com/github-username/github-repository-name.git\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'After updating the Git URL, how can I ensure the changes are correctly applied?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You are encountering the 'fatal: Authentication failed for https://github.com/username' error when pushing code because GitHub removed support for password authentication on August 13, 2021. \\n\\n**Solution:** \\n\\nCreate a personal access token from your GitHub account and use it to authenticate when pushing your changes. You can follow the instructions given in this [GitHub documentation](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls) for currently recommended modes of authentication. \\n\\nAdditionally, more details about generating and using a personal access token can also be found [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\\n\\nAuthored by Bruno Bedón.\",\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': \"Why am I encountering 'fatal: Authentication failed for https://github.com/username' error when pushing code?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If password authentication for GitHub is no longer supported, you should create a personal access token from your GitHub account and use it for authentication when pushing changes to your repository. You can find detailed instructions on how to create and use a personal access token in the GitHub documentation at the following link: \\n\\n[GitHub: Cloning with HTTPS URLs](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls)\\n\\nAdditionally, for a more secure method, you can generate an SSH key for authentication:\\n\\n1. Follow the instructions in these GitHub docs to create an SSH private and public key and add them to your GitHub account:\\n   - [Generating a new SSH key and adding it to the SSH agent](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\\n   - [Adding a new SSH key to your GitHub account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui)\\n\\n2. Once the SSH key is added to your GitHub account, you can authenticate through a terminal by running the command:\\n   ```bash\\n   ssh -T git@github.com\\n   ```\\n   You should receive a successful authentication notice.\\n\\nUsing these methods, you can securely authenticate with your GitHub account without the need for password authentication.',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'What should I do if password authentication for GitHub is no longer supported?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For information on recommended authentications for GitHub after the support for password authentication was removed, you can refer to the following solutions:\\n\\n1. **Personal Access Token**:\\n   - You can create a personal access token from your GitHub account and use it when you push your code. Guidance on this can be found [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\\n\\n2. **SSH Key**:\\n   - You can also generate a new SSH key, add it to the SSH agent, and link it to your GitHub account. The instructions for generating a new SSH key and adding it to your GitHub account can be found [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui).\\n\\nFurther information about remote repository authentication modes can also be found [here](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls).',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'Where can I find information on recommended authentications for GitHub after password support was removed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To configure your GitHub authentication without using a password, you can create a personal access token and use it for your Git operations. Here\\'s how you can do it:\\n\\n1. Create a personal access token from your GitHub account. You can follow the instructions [here](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls).\\n\\n2. Use the personal access token when you push your changes. Instead of using your GitHub password, you will enter this token.\\n\\nAlternatively, you can authenticate using SSH keys. Here’s how:\\n\\n1. Follow the instructions in these GitHub docs to create an SSH private and public key: \\n   - [Generating a new SSH key and adding it to the SSH agent](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\\n   - [Adding a new SSH key to your GitHub account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui)\\n\\n2. Once your SSH key is added to your GitHub account, you can authenticate by using the SSH key instead of a password.\\n\\nIf you are using Saturn Cloud, you can also use the default public keys provided by Saturn Cloud:\\n\\n1. Click on your username in Saturn Cloud and go to the \"Manage\" section.\\n2. Copy the default public key provided by Saturn Cloud.\\n3. Paste this key into the SSH keys section of your GitHub account.\\n4. Open a terminal on Saturn Cloud and run the command: `ssh -T git@github.com`.\\n5. You should receive a successful authentication notice.\\n\\nFollowing these steps should help you configure your GitHub authentication without the need for a password.',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': \"How can I configure my GitHub authentication if I can't use a password?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can create a personal access token for GitHub authentication from your GitHub account. Detailed instructions on how to create a personal access token can be found in the GitHub documentation: [creating a personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token).\\n\\nHere is a simplified outline of the steps:\\n1. Log in to your GitHub account.\\n2. Go to **Settings**.\\n3. Navigate to **Developer settings**.\\n4. Select **Personal access tokens**.\\n5. Click on **Generate new token**.\\n6. Configure the token with the required scopes and permissions.\\n7. Click **Generate token**.\\n8. Copy the generated token (it will not be shown again).\\n\\nUse this token instead of your password when performing Git operations that require authentication.',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'Where do I create a personal access token for GitHub authentication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To resolve the \"wget: unable to resolve host address\" error in Kaggle, please follow these steps:\\n\\n1. **Turn on the Internet for your Kaggle session:**\\n   - In your Kaggle notebook, locate the settings panel on the right-hand side of the screen.\\n   - Enable the option to turn on the internet for your session.\\n   - You may be asked to verify your phone number to prove that you are not a bot.\\n\\nThis should enable your Kaggle session to access external URLs, allowing you to use `wget` to download datasets without encountering the host address resolution error.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'How can I resolve the wget: unable to resolve host address error in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In Kaggle, when `wget` fails to resolve a host address, such as when you encounter the error \"unable to resolve host address \\'raw.githubusercontent.com\\',\" you should turn on the Internet for your Kaggle notebook session. You can do this by going to the settings panel on the right-hand side of the Kaggle screen and enabling the Internet. You will be required to verify your phone number to confirm that you are not a bot.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'What should I do when wget fails to resolve a host address in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To import data with `wget` in Kaggle without encountering errors, follow these steps:\\n\\n1. **Turn on the Internet for your Kaggle Session:**\\n    - In your Kaggle notebook, go to the settings panel on the right-hand side of the screen.\\n    - Turn on the Internet option. \\n    - You may need to verify your phone number as part of the process so Kaggle knows you are not a bot.\\n\\n2. **Use the `wget` Command Properly:**\\n    - Ensure you are using the correct `wget` syntax. Prefix the command with an exclamation mark (`!`).\\n    - Example command:\\n      ```bash\\n      !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n      ```\\n\\n3. **Read the Dataset Using Pandas:**\\n    - After downloading the data, you can read it into a pandas DataFrame using the `pd.read_csv` function.\\n    - Example:\\n      ```python\\n      import pandas as pd\\n      df = pd.read_csv(\\'housing.csv\\')\\n      ```\\n\\nBy following these steps, you should be able to import data using `wget` in Kaggle without encountering the \"unable to resolve host address\" error.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'How to import data with wget in Kaggle without encountering errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix wget errors in Kaggle, you need to adjust the Internet settings for your session. Follow these steps:\\n\\n1. Open your Kaggle notebook.\\n2. On the right-hand side of the Kaggle screen, locate the settings panel.\\n3. Turn on the Internet for your session.\\n4. You will be prompted to verify your phone number to confirm you are not a bot.\\n\\nEnabling the Internet in Kaggle should resolve the issue where wget is unable to resolve host addresses, allowing you to download datasets from external sources like GitHub.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'What settings need to be adjusted in Kaggle to fix wget errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You need to verify your phone number to use wget in Kaggle because enabling the Internet for your session requires verification. This step is necessary to confirm you are not a bot. When you try to fetch data using wget, you need the Internet to be turned on in your Kaggle notebook settings. This setting prompts Kaggle to ask for phone number verification.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'Why do I need to verify my phone number to use wget in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should watch the video \"Creating Virtual Environment for Python from VS Code\" to set up a virtual environment for Python in VS Code.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'What video should I watch to set up a virtual environment for Python in VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to use Jupyter Notebooks in Visual Studio Code (VS Code) without a web browser. VS Code has native support for Jupyter Notebooks, which means you do not need to open a web browser to code in a Notebook.\\n\\nFor more information on how VS Code supports Jupyter Notebooks, you can refer to this link: [Native Jupyter Notebooks support in VS Code](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'Is it possible to use Jupyter Notebooks in VS Code without a web browser?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To execute remote Jupyter Notebooks files from your local machine using VS Code, follow these steps:\\n\\n1. **Enable Port Forwarding**: Ensure port forwarding is enabled so your local machine can communicate with the remote server.\\n\\n2. **Run Jupyter Notebook on Remote Server**: On the remote machine, execute the command `jupyter notebook`.\\n\\n3. **Configure Remote Connection**: Set up a remote connection in your `.ssh/config` file. The configuration should be in line with the setup as suggested by Alexey's video (although the specifics of the video aren't detailed in the context).\\n\\n4. **Open Jupyter Notebooks in VS Code**: With the above configurations, you can now open VS Code on your local machine. VS Code provides native support for Jupyter Notebooks, meaning you can open and run your notebooks directly without needing a web browser.\\n\\nFor detailed guidance, you may refer to the [VS Code Jupyter Notebooks documentation](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).\\n\\nBy following these steps, you should be able to execute notebooks on a remote server seamlessly from your local VS Code environment.\",\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'How can I execute remote Jupyter Notebooks files from my local machine using VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more information about working with GitHub from VS Code in the [Git support from VS Code] section provided in the context. You can visit this link for detailed instructions: [Git support from VS Code](https://code.visualstudio.com/docs/sourcecontrol/overview). It covers how to stage and commit changes using the VS Code UI.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'Where can I find more information about working with GitHub from VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To run Jupyter Notebooks remotely using VS Code, follow these steps:\\n\\n1. **Enable Port Forwarding**: Ensure that port forwarding is enabled on your remote server. This allows your local machine to connect to the remote server through specific ports.\\n\\n2. **Run Jupyter Notebook on Remote Machine**: On the remote server, execute the command `jupyter notebook`. This will start the Jupyter Notebook server.\\n\\n3. **Configure Remote Connection in .ssh/config**: Set up a remote connection configuration in your `.ssh/config` file as suggested by the video tutorial (Creating Virtual Environment for Python from VS Code). This setup is essential for secure and streamlined remote connections.\\n\\n4. **VS Code Setup**: With these configurations, VS Code can run and execute remote Jupyter Notebooks files directly from your local machine without the need to open a web browser. This allows you to take advantage of the native Jupyter support provided by VS Code.\\n\\nFor more detailed instructions, you may visit the documentation provided by VS Code: [VS Code Documentation for Jupyter Notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).\\n\\nBy following these steps, you can efficiently manage and execute Jupyter Notebooks on a remote server through VS Code.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'What setup is necessary to run Jupyter Notebooks remotely using VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, it is not necessary to run 'conda create -n ...' every time you start VS Code to work on the project. You only need to run this command the first time to create the environment. After the environment is created, you just need to run 'conda activate ml-zoomcamp' whenever you want to use it.\",\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': \"Is it necessary to run 'conda create -n ...' every time I start VS Code to work on the project?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To activate the environment in VS Code after creating it, you should use the command:\\n\\n```bash\\nconda activate ml-zoomcamp\\n```\\n\\nThis command only needs to be run whenever you want to use the already created environment.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'What command should I use to activate the environment in VS Code after creating it?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To save your current Conda environment to a file, you can use the following command:\\n\\n```bash\\nconda env export > environment.yml\\n```\\n\\nThis will create a YAML file named `environment.yml` that contains all the information needed to reproduce your existing environment. You can then recreate the environment later using:\\n\\n```bash\\nconda env create -f environment.yml\\n```',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'How can I save my current Conda environment to a file?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To recreate an environment from a saved YAML file, you can use the command `conda env create -f environment.yml`.\\n\\nThis command reads the specifications from the `environment.yml` file and recreates the environment accordingly.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'What command allows me to recreate an environment from a saved YAML file?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To use the Conda environment after it is created, you need to execute the command:\\n\\n```bash\\nconda activate ml-zoomcamp\\n```',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'After the Conda environment is created, which command is executed to use it?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"When you invert a matrix and multiply by the original matrix, you don't get an exact identity matrix because of the limitations of floating point precision in computers. Floating point arithmetic often introduces small rounding errors, which is why the resulting matrix is very close to the identity matrix but not precisely it. For more details on this phenomenon, you can refer to the explanation here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken.\",\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': \"If I invert a matrix and multiply by the original, why don't I get an exact identity matrix?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The reason the matrices do not return an exact identity after multiplication is due to the limitations of floating point precision in computer calculations. When dealing with floating point numbers, the precision is not perfect, which leads to small numerical errors. This is why, even though mathematically an inverse multiplied by the original matrix should yield an identity matrix, the result may have small deviations from an exact identity matrix. For example, you'll observe values very close to zero, but not exactly zero, in some positions of the matrix. You can read more about these limitations here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken.\",\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Why do the matrices not return an exact identity after multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Imprecise results after matrix inversion can be attributed to the limitations of floating point arithmetic on computers. When you invert a matrix and multiply the inverse by the original matrix, the expected outcome is an Identity matrix. However, due to floating point precision issues, the result may not be exact. For example, when performing such an operation, you might get a matrix like:\\n\\n\\\\[ \\\\text{Inverse} \\\\times \\\\text{Original} = \\\\begin{bmatrix} 1.00000000e+00 & -1.38777878e-16 \\\\\\\\ 3.16968674e-13 & 1.00000000e+00 \\\\end{bmatrix} \\\\]\\n\\nIn this result, the off-diagonal elements are very close to zero but not exactly zero, which indicates the presence of minor numerical errors typical in floating point calculations. This is a common phenomenon in computational mathematics and is elaborated upon in discussions like the one found here: [StackOverflow discussion on floating point math](https://stackoverflow.com/questions/588004/is-floating-point-math-broken).',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Can you explain the imprecise results after matrix inversion?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The slight errors when multiplying an inverted matrix by the original are caused by the limitations of floating-point math on computers. Floating-point representation is not exact, leading to tiny inaccuracies that result in slight deviations from the expected identity matrix when performing such operations. This phenomenon is well-documented and further explained in this [Stack Overflow thread](https://stackoverflow.com/questions/588004/is-floating-point-math-broken).',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'What causes the slight errors when multiplying an inverted matrix by the original?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Floating-point math affects the accuracy of matrix operations because floating-point numbers on computers have limitations in precision. This sometimes results in small numerical inaccuracies (as shown in the example matrix provided), where operations like matrix inversion and multiplication don't produce perfectly accurate results. When an inverse matrix is multiplied by the original matrix, it ideally should yield an identity matrix. However, due to floating-point precision issues, the result can have tiny deviations from the expected identity matrix, as demonstrated in the given example:\\n\\n```\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\n```\\n\\nThese small numerical discrepancies occur because floating-point numbers cannot perfectly represent all real numbers, leading to minor but unavoidable errors in calculations. More details on the intricacies of floating-point math can be found in specialized resources like the linked Stack Overflow discussion.\",\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Why does floating-point math affect the accuracy of matrix operations?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The command that helps print dataset information in pandas is `pandas.DataFrame.info()`. This command prints details such as the index datatype, number of entries, column information with not-null counts and datatypes, and memory usage by the dataset. It is used as follows:\\n\\n```python\\ndf.info()\\n```',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'What command helps print dataset information in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check column information and memory usage of a pandas DataFrame, you can use the `pandas.DataFrame.info()` method. This method prints the following information:\\n\\n- Index datatype\\n- Number of entries\\n- Column information with not-null count and datatype\\n- Memory usage by dataset\\n\\nYou can use it in the following way:\\n\\n```python\\ndf.info()\\n```\\nThis will provide you with a concise summary of the DataFrame, including information about memory usage.\\n\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'How to check column info and memory usage of a pandas DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To view the index datatype and the number of entries in a dataset, you should use the `pandas.DataFrame.info()` method. Here’s how you can use it:\\n\\n```python\\ndf.info()\\n```\\n\\nThis method prints information about the dataset including the index datatype and the number of entries.',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'What method should I use to view index datatype and the number of entries?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The pandas function that prints column data type and not-null count is `pandas.DataFrame.info()`. You can use it as follows:\\n\\n```python\\ndf.info()\\n```\\n\\nThis function provides information about the dataset, including:\\n- Index datatype\\n- Number of entries\\n- Column information with not-null count and datatype\\n- Memory usage by the dataset',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'Which pandas function prints column data type and not-null count?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To obtain general dataset information in pandas, you can use the `pandas.DataFrame.info()` method. This method prints out the following details about the dataset:\\n\\n- Index datatype\\n- Number of entries\\n- Column information with not-null count and datatype\\n- Memory usage by the dataset\\n\\nYou use it as follows:\\n```python\\ndf.info()\\n```',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'How can I obtain general dataset info in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you get a `NameError` for 'np' in your code, it is likely because the `numpy` library has not been imported. To resolve this issue, you should include the following import statement at the beginning of your code:\\n\\n```python\\nimport numpy as np\\n```\\n\\nMake sure this line appears before you attempt to use any `numpy` functions or methods.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"What should I do if I get a NameError for 'np' in my code?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix a `NameError` for 'pd' when using pandas, make sure you import the pandas library at the beginning of your script. You can do this by adding the following line before using any pandas functionality:\\n\\n```python\\nimport pandas as pd\\n```\\n\\nThis will allow you to use 'pd' as the shorthand alias for the pandas library throughout your code.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"How can I fix a NameError for 'pd' when using pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you receive a NameError for 'np', it might be due to the numpy library not being imported. You should include the import statement at the beginning of your script as follows:\\n\\n```python\\nimport numpy as np\\n```\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"What libraries might not be imported if I receive a NameError for 'np'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You are seeing a `NameError` for 'pd' in your machine learning script because the Pandas library has not been imported. To resolve this issue, ensure that you include the following import statement at the beginning of your script:\\n\\n```python\\nimport pandas as pd\\n```\\n\\nThis will import the Pandas library and allow you to use 'pd' as its common alias.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"Why am I seeing a NameError for 'pd' in my machine learning script?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution to the NameError issue in the FAQ section was added by Manuel Alejandro Aponte.',\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': 'Who added the solution to the NameError issue in the FAQ section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To select columns by data type in a pandas dataframe, you can use the `select_dtypes` method. Here are the concise ways to get columns with specific data types:\\n\\n- To get columns with numeric data types, use:\\n  ```python\\n  df.select_dtypes(include=np.number).columns.tolist()\\n  ```\\n\\n- To get columns with object data types, use:\\n  ```python\\n  df.select_dtypes(include='object').columns.tolist()\\n  ```\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'How can I select columns by data type in pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To get numeric columns in a dataframe concisely, you can use the following method:\\n\\n```python\\ndf.select_dtypes(include=np.number).columns.tolist()\\n```\\n\\nThis will give you a list of the names of columns that contain numeric data.',\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'What is a concise method to get numeric columns in a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To list columns with an object data type in a dataframe, you can use the following code:\\n\\n```python\\ndf.select_dtypes(include='object').columns.tolist()\\n```\\n\\nThis will return a list of all columns in the dataframe that have an object data type.\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'How do I list columns with object data type in a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To filter columns by dtype in pandas, you can use the `select_dtypes` method. Here are the specific methods to filter for numeric and object dtypes:\\n\\n```python\\n# To get columns with numeric dtype\\ndf.select_dtypes(include=np.number).columns.tolist()\\n\\n# To get columns with object dtype\\ndf.select_dtypes(include='object').columns.tolist()\\n```\\n\\nThese commands will return a list of column names that match the specified dtype.\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': \"What's the pandas function to filter columns by dtype?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution for selecting columns by dtype was contributed by Gregory Morris.',\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'Who contributed the solution for selecting columns by dtype?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To determine the structure of a dataset in Pandas, you can use the `.shape` attribute. Here's how you can use it:\\n\\n```python\\ndf.shape  # Returns a tuple representing the dimensionality of the DataFrame.\\n```\\n\\nAdditionally, if you want to identify the number of rows and columns separately, you can use:\\n\\n```python\\ndf.shape[0]  # For identifying the number of rows.\\ndf.shape[1]  # For identifying the number of columns.\\n```\",\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'How can I determine the structure of a dataset in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The attribute in Pandas that helps identify the number of rows in a dataset is the `.shape` attribute. Specifically, `df.shape[0]` will give you the number of rows in the dataset.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'What attribute in Pandas helps identify the number of rows in a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To identify the number of columns in a Pandas DataFrame, you would use the `.shape` attribute:\\n\\n```python\\ndf.shape[1]  # This returns the number of columns in the DataFrame.\\n```\\n\\nThis information was added by Radikal Lukafiardi.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'Which attribute would you use to identify the number of columns in a Pandas DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about identifying the shape of a dataset was contributed by Radikal Lukafiardi.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'Who contributed the information about identifying the shape of a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One method to find the shape of a dataset in Pandas is by using the `.shape` attribute. For example, you can use `df.shape` to get the overall shape (number of rows and columns), `df.shape[0]` to get the number of rows, and `df.shape[1]` to get the number of columns.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'What is one method to find the shape of a dataset in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should use the `np.dot()` function for matrix multiplication to avoid value errors in your homework. Ensure that the number of columns in the first matrix matches the number of rows in the second matrix to prevent dimension mismatches. Additionally, be aware that the order of multiplication is crucial and affects the result of the multiplication.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'What function should I use for matrix multiplication to avoid value errors in our homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The order of matrices in matrix multiplication is important because it affects both the dimensions and the values of the resulting matrix. \\n\\nFor example, if you multiply matrix A (with dimensions m x n) by matrix B (with dimensions n x p), the result will be matrix C with dimensions m x p. However, if you reverse the order and multiply matrix B (n x p) by matrix A (m x n), the result will instead be matrix D with dimensions n x n. \\n\\nThese different dimensions usually result in matrices with different values. Thus, changing the order of multiplication changes the result, making the order of matrices crucial in matrix operations.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'Why is the order of matrices important in matrix multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check if the dimensions of two matrices are compatible for multiplication, ensure that the number of columns in the first matrix matches the number of rows in the second matrix. \\n\\nFor example, if you have matrix A with dimensions \\\\( m \\\\times n \\\\) and matrix B with dimensions \\\\( n \\\\times p \\\\):\\n\\n- Matrix A has \\\\( m \\\\) rows and \\\\( n \\\\) columns.\\n- Matrix B has \\\\( n \\\\) rows and \\\\( p \\\\) columns.\\n\\nThe matrices A and B are compatible for multiplication because the number of columns in matrix A (which is \\\\( n \\\\)) matches the number of rows in matrix B (which is also \\\\( n \\\\)). The resulting matrix will have dimensions \\\\( m \\\\times p \\\\).\\n\\nIf the dimensions do not meet this condition, you will encounter a dimension mismatch error. You can rearrange the order of multiplication to satisfy this condition, but the order of multiplication affects the result.\\n\\nRemember, incorrect orders, such as trying to multiply an \\\\( n \\\\times p \\\\) matrix by an \\\\( m \\\\times n \\\\) matrix, will lead to errors or unintended results, as shown in the example provided:\\n- \\\\( A (m \\\\times n) * B (n \\\\times p) = C (m \\\\times p) \\\\)\\n- \\\\( B (n \\\\times p) * A (m \\\\times n) = D (n \\\\times n) \\\\)\\n\\nMatrices C and D will have different sizes and usually different values.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'How can I check if the dimensions of two matrices are compatible for multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The number of columns in the first matrix should match the number of rows in the second matrix when performing matrix multiplication.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'What should the number of columns in the first matrix match when doing matrix multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information on avoiding Value errors with array shapes was provided by Leah Gotladera.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'Who provided the information on avoiding Value errors with array shapes?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To handle NaN values in a column, you can replace them with the average of that column using a method called imputing. Here's how you can do it:\\n\\n1. Calculate the average of the column and save it to a variable.\\n2. Replace the NaN values in the column with that average variable.\\n\\nThis technique is useful when you have NaN or null values but do not want to discard the entire row because it contains valuable information in other columns.\",\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'How do I handle NaN values in a column?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We should keep rows with NaN values because those rows often contain valuable information that contribute to other columns. By retaining these rows, we can leverage the existing data within them to improve our models. Rather than discarding the rows entirely, which could result in loss of important information and possibly affecting the model's performance, we can use techniques such as imputing to handle NaN values. For instance, replacing NaN values with the average of the respective column ensures that we maintain the row's contribution while addressing the missing data.\",\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'Why should we keep rows with NaN values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In the context of machine learning, 'imputing' refers to the process of replacing missing values (NaN or null values) in a dataset with specific values, such as the average of the column. This technique allows you to retain rows that have valuable information in other columns without discarding the entire row due to missing data. By imputing, you ensure that the dataset remains as complete and statistically accurate as possible for analysis or model training.\",\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': \"What is meant by 'imputing' in machine learning?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Before replacing NaN values, you need to get the average of the column and save it to a variable. This step ensures you have the value to use for imputing the NaN values in the column.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'What needs to be done before replacing NaN values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Replacing NaN values with the column average is a method known as imputing. The reason for doing this is to retain rows that have missing values (NaNs) because they may still contain valuable information in other columns. By imputing the NaNs with the column average, you ensure that the dataset remains complete and useful without discarding any potentially important data.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'Why replace NaN values with the column average?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The mathematical formula for linear regression mentioned in Question 7 involves multiplying a Matrix \\\\( X \\\\) by some unknown weights \\\\( w \\\\), resulting in the target \\\\( y \\\\). This represents the core concept of linear regression in matrix form. Unfortunately, the exact formula is not explicitly detailed in the provided context. However, based on common practices in linear regression, the formula typically resembles:\\n\\n\\\\[ y = Xw \\\\]\\n\\nwhere:\\n- \\\\( y \\\\) is the target vector,\\n- \\\\( X \\\\) is the matrix of input features,\\n- \\\\( w \\\\) is the vector of weights or coefficients.\\n\\nFor a deeper understanding, additional resources such as \"Ordinary least squares,\" \"Multiple Linear Regression in Matrix Form,\" and \"Pseudoinverse Solution to OLS\" are suggested for further reading.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'What is the mathematical formula for linear regression mentioned in Question 7?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To solve the initial problem involving a Matrix X in linear regression, you need to multiply the Matrix X by some unknown weights \\\\( w \\\\), resulting in the target \\\\( y \\\\). Here’s a straightforward explanation:\\n\\n1. **Matrix Representation**: In linear regression, the relationship between the input features (represented by Matrix X) and the target variable (y) can be modeled as:\\n   \\\\[\\n   y = Xw\\n   \\\\]\\n\\n2. **Ordinary Least Squares (OLS)**: To find the unknown weights \\\\(w\\\\), you typically use the ordinary least squares method, which minimizes the difference between the observed target values and the values predicted by your linear model.\\n\\n3. **Solution via Normal Equation**: The weights \\\\( w \\\\) can be calculated using the normal equation:\\n   \\\\[\\n   w = (X^TX)^{-1}X^Ty\\n   \\\\]\\n   Here, \\\\( X^T \\\\) denotes the transpose of Matrix X, and \\\\((X^TX)^{-1}\\\\) denotes the inverse of the product of \\\\( X^T \\\\) and \\\\( X \\\\).\\n\\nFor a more in-depth understanding, you can refer to the following resources:\\n- Ordinary least squares\\n- Multiple Linear Regression in Matrix Form\\n- Pseudoinverse Solution to OLS\\n\\nThese resources should help you grasp the concept and mathematical derivation behind solving linear regression problems using matrices.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'How can I solve the initial problem involving a Matrix X in linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The additional reading recommended for understanding ordinary least squares includes:\\n\\n1. **Ordinary least squares**\\n2. **Multiple Linear Regression in Matrix Form**\\n3. **Pseudoinverse Solution to OLS**\\n\\nThese resources were added by Sylvia Schmitt with comments from Dmytro Durach.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'What additional reading is recommended for understanding ordinary least squares?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more resources about multiple linear regression in matrix form by referring to the section titled \"Question 7: Mathematical formula for linear regression\" in the \"Introduction to Machine Learning\" section of the FAQ. This section mentions additional reading and videos specifically on Multiple Linear Regression in Matrix Form, among other related topics.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'Where can I find more resources about multiple linear regression in matrix form?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about the mathematical formula for linear regression was added by Sylvia Schmitt.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'Who added the information about the mathematical formula for linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The final multiplication in section 1 does not have 5 columns because you likely interchanged the first step of the multiplication. Therefore, you used incorrect dimensions for matrix multiplication, leading to a different number of columns in the result.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Why does the final multiplication not have 5 columns in section 1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The issue with the final multiplication missing columns is most likely because you interchanged the steps of the multiplication. Specifically, you might have performed the multiplication in the wrong order, resulting in an incorrect number of columns in the final result.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'What likely caused the issue with the final multiplication missing columns?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, interchanging the first step of multiplication can cause errors. When multiplying matrices, the order of multiplication is critical and affects the result. For instance, multiplying matrix A (m x n) with matrix B (n x p) gives a different result (matrix C of size m x p) compared to multiplying matrix B (n x p) with matrix A (m x n) (resulting in matrix D of size n x n). Hence, changing the order of multiplication leads to differences in the resulting matrix's size and values.\",\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Can interchanging the first step of multiplication cause errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The note about the multiplication step error was added by Emmanuel Ikpesu.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Who added the note about the multiplication step error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your final multiplication is incorrect in the first section, you should check whether you interchangeably performed the first step of the multiplication incorrectly. It is possible that you incorrectly arranged the order of multiplication. Remember, in matrix multiplication, the number of columns in the first matrix should match the number of rows in the second matrix, and changing the order of multiplication can lead to incorrect results.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'What should I check if my final multiplication is incorrect in the first section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When performing matrix-matrix multiplication in Python, the preferred operator is the `@` operator or the function `np.matmul()` from the numpy library. While `numpy.dot()` or `ndarray.dot()` can also be used, the numpy documentation recommends using `@` or `np.matmul()` for matrix-matrix multiplication.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'When performing matrix-matrix multiplication in Python, which operator is preferred?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Matrix-vector multiplication can be written as the * operator in some sources. However, it is performed using the @ operator or np.matmul() via numpy. The * operator, in contrast, is used for element-wise multiplication (Hadamard product).',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'How can matrix-vector multiplication be written in some sources?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In numpy, the `*` operator performs element-wise multiplication, also known as the Hadamard product.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'What operator does the * symbol perform in numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For scalar multiplication in numpy, you can use the `numpy.multiply()` function or the `*` operator.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'What functions can be used for scalar multiplication in numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about multiplication operators was provided by Andrii Larkin.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'Who provided the information about multiplication operators?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you encounter an ImportError when launching a new Jupyter notebook in a new environment, specifically an error like \"ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py)\", you should switch to the main environment and run the command \"pip install nbconvert --upgrade\".',\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': 'What should I do if I encounter an ImportError when launching a new Jupyter notebook in a new environment?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To resolve the error related to 'contextfilter' from 'jinja2' when starting a Jupyter notebook, you should switch to your main environment and run the following command:\\n\\n```bash\\npip install nbconvert --upgrade\\n```\\n\\nThis should resolve the issue.\",\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"How can I resolve an error related to 'contextfilter' from 'jinja2' when starting a Jupyter notebook?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix the ImportError involving 'contextfilter' from 'jinja2', switch to the main environment and run the command:\\n\\n```sh\\npip install nbconvert --upgrade\\n```\",\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"What command should I run to fix the ImportError involving 'contextfilter' from 'jinja2'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution for ImportError issues when launching Jupyter notebooks was added by George Chizhmak.',\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': 'Who added the solution for ImportError issues when launching Jupyter notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To solve the 'contextfilter' ImportError in Jupyter notebook, you need to upgrade the `nbconvert` package. You can do this by switching to the main environment and running the command:\\n\\n```bash\\npip install nbconvert --upgrade\\n```\",\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"Which package do I need to upgrade to solve the 'contextfilter' ImportError in Jupyter notebook?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix wget hanging on MacOS Ventura M1:\\n\\nIf you observe that wget is hanging and showing IPv6 addresses in the terminal, follow these steps to resolve the issue:\\n1. Navigate to your System Settings.\\n2. Go to Network.\\n3. Select your network connection and click on Details.\\n4. Find the Configure IPv6 option.\\n5. Set it to Manually and click OK.\\n6. Try running wget again.\\n\\nAlternatively, you can use curl as a substitute for wget:\\n1. Instead of wget, use the following command to download the file:\\n   ```bash\\n   curl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n   ```\\n   - `curl` is a utility for downloading data from the internet.\\n   - `-o` specifies the file name for the downloaded content.\\n   - `./housing.csv` is the name you are using to save the file locally.\\n\\nYou can refer to the Curl Documentation for more details on how to use curl.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'How do I fix wget hanging on MacOS Ventura M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If wget shows IPv6 addresses and hangs, you should adjust your network settings on macOS Ventura M1. Specifically, go to your **System Settings > Network > your network connection > Details > Configure IPv6** and set it to **Manually**, then click **OK**. After making this change, try running wget again.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'What should I do if wget shows IPv6 addresses and hangs?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To configure IPv6 settings on MacOS Ventura M1, follow these steps:\\n\\n1. Open System Settings.\\n2. Navigate to Network.\\n3. Select your network connection.\\n4. Click on Details.\\n5. Go to Configure IPv6.\\n6. Set it to Manually.\\n7. Click OK.\\n\\nYou should now have configured the IPv6 settings manually on your MacOS Ventura M1.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'How do I configure IPv6 settings on MacOS Ventura M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set IPv6 to Manually on MacOS Ventura, follow these steps:\\n\\n1. Go to **System Settings**.\\n2. Select **Network**.\\n3. Choose your **network connection**.\\n4. Click on **Details**.\\n5. Find **Configure IPv6** and set it to **Manually**.\\n6. Click **OK**.\\n\\nThese steps should help you configure your IPv6 settings manually.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'Where can I find the option to set IPv6 to Manually on MacOS Ventura?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'There is no specific network setting adjustment mentioned for resolving wget issues on MacOS M1. However, you can use alternatives such as `curl` to download files. Here\\'s an example:\\n\\n```sh\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIf you prefer to use `wget`, you can install it on MacOS using `brew`:\\n\\n```sh\\nbrew install wget\\n```\\n\\nAlternatively, you can also use Python\\'s built-in libraries to download files:\\n\\n```python\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n```',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'Is there a network setting adjustment for wget issues on MacOS M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you are using macOS and having trouble with Wget, you can use `curl` as an alternative. Here is an example of how to use `curl` to download a file:\\n\\n```sh\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nHere's a breakdown of the command:\\n- `curl`: A utility for retrieving information from the internet.\\n- `-o`: Tells `curl` to store the result as a file.\\n- `./housing.csv`: The name you choose for the downloaded file.\\n- `https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv`: The URL of the file you want to download.\\n\\nYou can find more detailed information and documentation about `curl` by referring to the Curl Documentation.\",\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'What alternative can I use to Wget if I am using macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Sure, here's an example of using `curl` to download a file on macOS:\\n\\n```sh\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\n### Explanation:\\n- **curl**: A command-line tool for getting or sending data including files using URL syntax.\\n- **-o**: This option tells `curl` to save the downloaded data to a file.\\n- **./housing.csv**: The name you want to assign to the downloaded file.\\n- **https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv**: The URL from which `curl` will download the file.\\n\\nThis command will download the file from the specified URL and save it as `housing.csv` in the current directory.\",\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'Can you provide an example of using curl to download a file on macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `-o` option in the `curl` command specifies the name of the file to which the retrieved information will be saved. Here is an example command:\\n```sh\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\nExplanation:\\n- `-o`: This option tells `curl` to save the downloaded data to a file.\\n- `./housing.csv`: This is the name of the file where the data from the URL will be stored.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'What does the -o option do in the curl command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To specify the name of the file when using `curl` on macOS, you can use the `-o` option followed by the desired filename. Here is an example command:\\n\\n```bash\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIn this example:\\n- `curl` is the utility used for retrieving information from the internet.\\n- `-o` tells `curl` to store the result as a file.\\n- `./housing.csv` is the specified filename for the downloaded file.\\n- `https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv` is the web address (URL) from which `curl` will extract data and save it under the filename `housing.csv`.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'How do you specify the name of the file when using curl on macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For more information about using `curl`, you can refer to the official Curl Documentation. Here is an example of a basic `curl` usage to download a file:\\n\\n```sh\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nExplanation:\\n- `curl`: A utility for retrieving information from the internet.\\n- `-o`: Option to store the result as a file.\\n- `filename`: The name of the file where the content will be saved.\\n- `URL`: The web address from which to fetch the data.\\n\\nAdditionally, `curl` can also be used for testing HTTP POST requests and interacting with various web services. You can find specific examples in the provided context. For further details and advanced usage, you should visit the Curl Documentation.\\n\\nAdded by David Espejo.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'Where can I find more information about using curl?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can use the `round()` function or f-strings to limit the number of decimal places.\\n\\nExamples:\\n\\n1. Using `round()`:\\n   ```python\\n   round(number, 4)  # This will round the number up to 4 decimal places\\n   ```\\n\\n2. Using f-strings:\\n   ```python\\n   avg = 3.14159\\n   print(f'Average mark for the Homework is {avg:.3f}')  # This will format avg to 3 decimal places\\n   ```\\n\\nAdditionally, if you need to round values in an entire pandas Series, you can use `pandas.Series.round`. More details can be found in the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'What function can I use to limit a number of decimal places?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To round a number to 4 decimal places in Python, you can use the `round()` function. Here's an example:\\n\\n```python\\nrounded_number = round(number, 4)\\n```\\n\\nThis will round the variable `number` to 4 decimal places.\\n\\nAlternatively, you can use f-strings for formatted string output, although the example given is for 3 decimal places, you can easily adapt it to 4 decimal places:\\n\\n```python\\nprint(f'Rounded number is {number:.4f}')\\n```\\n\\nIf you need to round values in a whole pandas Series, you can use the `pandas.Series.round` method. Check the pandas documentation for further details: [pandas.Series.round](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'How can I round a number to 4 decimal places in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To format a number to 3 decimal places using f-strings in Python, you can use the following syntax:\\n\\n```python\\nprint(f'Your number formatted to 3 decimal places is {number:.3f}')\\n```\\n\\nFor example, if you have a variable `avg` that you want to format:\\n\\n```python\\navg = 3.141592653589793\\nprint(f'Average mark for the Homework is {avg:.3f}')\\n```\\n\\nThis will output:\\n\\n```\\nAverage mark for the Homework is 3.142\\n```\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Can you demonstrate using f-strings to format a number to 3 decimal places?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a function to round an entire Series in pandas. You can use the `pandas.Series.round` function to round values in the whole Series. You can check the official documentation for more details: [pandas.Series.round](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).',\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Is there a function to round an entire Series in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more information about rounding values in pandas Series by visiting the official pandas documentation at the following link: [pandas.Series.round](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).',\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Where can I find more information about rounding values in pandas Series?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can ask questions for the Live Sessions for Week 2 by visiting this link: [https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions](https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I ask questions for the Live Sessions for Week 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The start date for Week 2 of the Machine Learning for Regression course is September 18, 2023.',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'What is the start date for Week 2 of the Machine Learning for Regression course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the Week 2 homework for the course at the following link:\\n\\n[Week 2 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md)',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I find the Week 2 homework for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To submit the Week 2 homework, you need to use the following link: [Submit HW Week 2](https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform). This link is also available at the bottom of the Week 2 homework link provided in our course materials.',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'How can I submit the Week 2 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can check the calendar for the weekly meetings [here](https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I check the calendar for the weekly meetings?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To visualize the distribution of the `median_house_value` in the housing dataset, you can use a histogram. Here's a step-by-step guide:\\n\\n1. Import necessary libraries.\\n2. Load the housing dataset.\\n3. Create a histogram to visualize the distribution.\\n\\nHere's the example code:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the data\\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\\ndf = pd.read_csv(url)\\n\\n# Create the histogram\\nsns.histplot(df['median_house_value'], kde=False)\\nplt.show()\\n```\\n\\nThis will produce a histogram of the `median_house_value` variable, allowing you to visually inspect its distribution.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'How can we visualize the distribution of the median_house_value in the housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the skewness of the `median_house_value` variable from the dataset, you can use the following method:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n\\n# EDA: Plot a histogram\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\n\\n# Describe the \\'median_house_value\\' variable\\nprint(df[\\'median_house_value\\'].describe())\\n\\n# Calculate and print the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n```\\n\\nThis will generate a histogram to visually inspect the distribution and print descriptive statistics along with the skewness value to numerically assess the skewness of the `median_house_value` variable.',\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'What is a method to check the skewness of the median_house_value variable from the dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To generate a histogram for the `median_house_value`, you need the following Python libraries:\\n\\n1. `pandas`\\n2. `matplotlib.pyplot`\\n3. `seaborn`\\n\\nHere's an example code snippet for generating the histogram:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the data\\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\\ndf = pd.read_csv(url)\\n\\n# EDA\\nsns.histplot(df['median_house_value'], kde=False)\\nplt.show()\\n```\\n\\nThese libraries allow you to load your dataset (`pandas`), visualize the histogram (`matplotlib.pyplot` and `seaborn`), and perform basic exploratory data analysis.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'Which Python libraries are needed to generate a histogram for the median_house_value?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The command `sns.histplot(df['median_house_value'], kde=False)` generates a histogram for the `median_house_value` column from the DataFrame `df`. The `kde=False` parameter ensures that only the histogram is plotted without the Kernel Density Estimate (KDE) curve. This can be useful for visualizing the distribution and any long tails in the data.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': \"What does the command sns.histplot(df['median_house_value'], kde=False) do?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Checking the distribution or skewness of the target variable in a regression task is important because if the target variable is highly skewed, it can negatively impact the performance of the regression model. The easiest way to evaluate the skewness is by plotting the distribution of the target variable. Transforming the target variable, for instance, by applying a logarithm transformation, can help normalize the distribution, making the regression model more effective and robust. This approach is particularly useful if skewness is detected in the target variable's distribution.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'Why is checking the distribution or skewness important in machine learning for regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You might encounter a \"LinAlgError: Singular matrix\" error while following course videos due to several reasons:\\n\\n1. **Explanation in Videos**: This error will be discussed and explained in the Regularization video of the course. It\\'s considered a normal occurrence.\\n   \\n2. **Inversion Error**: You may get this error if you\\'ve tried to invert matrix \\\\(X\\\\) multiple times in your code. Inverting a matrix once might be fine, but doing it a second time can cause issues.\\n\\n3. **Matrix Properties**: Not every matrix can be inverted. A \"Singular matrix\" error indicates that the matrix in question does not have an inverse.\\n\\nPay careful attention to these points as they could be underlying reasons for encountering the error.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Why might I encounter a LinAlgError: Singular matrix error while following course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the Regularization video will explain the LinAlgError: Singular matrix issue. It addresses why this error occurs and is a normal part of the learning process.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Will the Regularization video explain the LinAlgError: Singular matrix issue?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is normal to get a Singular Matrix error when following the course. This can occur during certain exercises due to the nature of the matrix operations involved. The course will explain why this happens in the Regularization video. Additionally, this error might also occur if you attempt to invert the same matrix more than once in your code. Don’t worry, encountering this error is a common part of the learning process.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Is it normal to get a Singular Matrix error when following the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, performing the inverse of \\\\( X \\\\) twice in your code can cause a LinAlgError. According to the context provided, an error can occur because you did the inverse of \\\\( X \\\\) once and then attempted to do it a second time. This kind of operation can lead to issues such as encountering a singular matrix which cannot be inverted, thus causing a LinAlgError.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Can performing the inverse of X twice in my code cause a LinAlgError?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The additional information regarding the Singular Matrix error was provided by Cécile Guillot.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Who provided the additional information regarding the Singular Matrix error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The CONTEXT provided does not explicitly mention a specific dataset used for regression in the course. However, based on the CONTEXT, it is implied that the dataset might be chosen by the students based on their interest and the problem they wish to solve. Students are encouraged to find datasets from sources such as Kaggle, UCI Machine Learning Repository, OpenML, etc.\\n\\nFor further guidance, you can refer to these links to find an appropriate dataset:\\n- https://www.kaggle.com/datasets\\n- https://www.kaggle.com/competitions\\n- https://archive.ics.uci.edu/ml/index.php\\n- https://www.openml.org/search?type=data\\n- https://newzealand.ai/public-data-sets\\n- https://datasetsearch.research.google.com\\n\\nSo, in summary:\\nThe course encourages students to select a dataset of their choice for regression tasks, using sources like Kaggle, UCI Machine Learning Repository, OpenML, and others.',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'What dataset is used for regression in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a detailed description of the California housing dataset at this link: [California housing dataset description](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Where can I find information about the California housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, details on the dataset used in Section 2 are provided. You can find a detailed description of the California housing dataset at this link: [https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Do you provide details on the datasets used in Section 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a link to learn more about the California housing dataset. You can find a detailed description of the dataset [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Is there a link to learn more about the California housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To access the description for the dataset in \"Machine Learning for Regression,\" there is no specific direct reference for a dataset description in the provided context. However, you can access the dataset itself and possibly infer its description by reading it into a pandas dataframe using the following code:\\n\\n```python\\nimport pandas as pd\\n\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis dataset is related to housing and can be found on GitHub. Once you have read the dataset into a pandas dataframe, you can explore the columns and the data itself to get more information about its description.\\n\\nKrishna Anand',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'How can I access the description for the dataset in Machine Learning for Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You are getting NaNs after applying `.mean()` when calculating RMSE using for loops because there were NaN values present in your data array. Specifically, you applied `fillna(0)` only to your training data but did not apply it to your validation and test datasets. As a result, the RMSE computation encountered NaN values when reaching the mean step after squaring the errors. To resolve this issue, you should ensure that `fillna(0)` is applied to all datasets: train, validation, and test datasets. After doing this, your for loops to calculate RMSE should work correctly.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'Why did I get NaNs after applying .mean() when using for loops to calculate RMSE?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'I identified the cause of NaNs in my RMSE calculation by tracing the NaN values back to the data splitting stage. Initially, I used `fillna(0)` only on the train data but not on the validation and test data. When my data reached the mean step after squaring the error in the RMSE function, the presence of NaN values caused the resulting RMSE to be NaN. The issue was resolved after applying `fillna(0)` to all datasets (train, validation, and test).',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'How did you identify the cause of NaNs in your RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The issue with the data that led to NaNs in the RMSE function was the presence of NaNs in the dataset after the data splitting process. Specifically, the NaNs were present because `fillna(0)` was applied only to the training data but not to the validation and test data. This caused NaNs to persist in the arrays being processed, leading to NaNs in the RMSE calculation. The problem was resolved by applying `fillna(0)` to the entire dataset, including the train, validation, and test data.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'What was the issue with the data that led to NaNs in the RMSE function?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The problem of getting NaNs in the RMSE calculation was resolved by applying `fillna(0)` to all parts of the dataset: the training data, validation data, and test data. Initially, only the training data had `fillna(0)` applied, which caused NaNs to appear in the validation and test data. By ensuring that `fillna(0)` was applied uniformly across the entire dataset, the issue was fixed, and the RMSE calculation produced valid results.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'What step resolved the problem of getting NaNs in the RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The need to apply `fillna(0)` on all datasets (train, val, and test) arises because NaN values can propagate through your data processing and evaluation pipeline, causing problems at various steps. In the provided CONTEXT, it was observed that only applying `fillna(0)` to the training data resulted in issues later when calculating metrics like RMSE due to the presence of NaN values in the validation and test sets. By applying `fillna(0)` to all datasets, you ensure consistency across the datasets and avoid errors caused by missing values, thereby making your calculations reliable.\\n\\nIn summary, `fillna(0)` was needed on all datasets to ensure that missing values were uniformly handled across the train, validation, and test sets, preventing NaNs from causing problems in subsequent analyses, such as calculating error metrics.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'Why did you need to apply fillna(0) on all datasets, including train, val, and test?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should transform the target variable to a logarithm distribution when the target variable has a long tail distribution, such as in prices, which can exhibit a wide range. Additionally, this transformation is recommended if the target variable is highly skewed. This technique is useful for handling skewness and making the data distribution more normal, thereby potentially improving the performance of machine learning models. However, be cautious if the target variable has negative values, as the logarithmic transformation method np.log1p() would not work in such cases.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Why should we transform the target variable to logarithm distribution?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, we do not transform the target variable to logarithm distribution for all machine learning projects. Whether to apply this transformation depends on the distribution of the target variable. Specifically, we consider transforming the target variable if it is highly skewed or has a long tail distribution. It is essential to plot the distribution of the target variable to evaluate its skewness before deciding on applying the transformation.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Do we transform the target variable to logarithm distribution for all machine learning projects?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You should transform the target variable when it has a long tail distribution, such as with prices that have a wide range. This transformation can be done using the `np.log1p()` method. However, this method won't work if your target variable has negative values. Additionally, transforming the target to a logarithm distribution is particularly useful when you see that your target variable is highly skewed, which you can evaluate by plotting its distribution.\",\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'When should we transform the target variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To evaluate if the target variable is highly skewed, you can plot the distribution of the target variable. This visual representation can help you understand the skewness. Here is an example code snippet that you can use to plot the distribution and check the skewness:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\n\\n# Describe the dataset\\nprint(df[\\'median_house_value\\'].describe())\\n\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n```\\n\\nBy plotting the histogram and examining the skewness value, you can determine whether the target variable is highly skewed. Skewness values can give you a quantitative measure of the asymmetry of the distribution.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'How can we evaluate if the target variable is highly skewed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Understanding the skewness of the target variable is important because it allows you to determine whether a transformation is necessary to improve the performance of your machine learning model. If the target variable is highly skewed, applying transformations such as a logarithmic distribution can help normalize the data, making it more suitable for regression methods. Evaluating the distribution of the target variable by plotting it helps to visualize the skewness, thus aiding in deciding the appropriate transformation to apply.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Why is it important to understand the skewness of the target variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To read a dataset directly from GitHub into a pandas dataframe, you can use the pandas `read_csv` function with the GitHub raw URL. Here is the example technique:\\n\\n```python\\nimport pandas as pd\\n\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis code will load the dataset from the provided GitHub link directly into the dataframe `dfh`.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'How can I read a dataset directly from GitHub into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To load data from a GitHub link into pandas, you can use the following method:\\n\\n```python\\nimport pandas as pd\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis code reads the dataset directly from the provided GitHub link and loads it into a pandas DataFrame.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'What method allows me to load data from a GitHub link into pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To read data from a GitHub URL using pandas, you can use the `pd.read_csv()` function in pandas directly with the URL of the CSV file on GitHub. Here\\'s an example of how you can do this:\\n\\n```python\\nimport pandas as pd\\n\\n# URL of the CSV file on GitHub\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n\\n# Read the CSV file directly into a pandas DataFrame\\ndf = pd.read_csv(url)\\n```\\n\\nThis will load the dataset from the specified GitHub URL into a pandas DataFrame called `df`.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'Could you explain how to read data from a GitHub URL using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To import a dataset from a GitHub link into a pandas dataframe, use the following code:\\n\\n```python\\nimport pandas as pd\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis technique will read the dataset directly into a pandas dataframe.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'What is the code to import a dataset from a GitHub link into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fetch data from a GitHub repository into a pandas dataframe, you can directly use the `pd.read_csv` method from pandas, as shown in the example below:\\n\\n```python\\nimport pandas as pd\\n\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n``` \\n\\nThis will load the dataset from the provided GitHub link into a pandas dataframe named `dfh`.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'How do I fetch data from a GitHub repository into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To load the dataset directly into Kaggle Notebooks, you can use the following steps:\\n\\n1. Use the `wget` command to download the dataset:\\n    ```python\\n    !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n    ```\\n    Ensure you include the `!` before `wget`.\\n\\n2. Once the dataset is downloaded, you can load it into a pandas DataFrame:\\n    ```python\\n    import pandas as pd\\n    df = pd.read_csv('housing.csv')\\n    ```\\n\\nThese steps will allow you to directly load and use the dataset within your Kaggle notebooks.\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'How can I load the dataset directly into Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To download the dataset to Kaggle Notebooks, you need to use the following command:\\n\\n```bash\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nRemember to include the `!` before the `wget` command. After the dataset is downloaded, you can read it into a pandas DataFrame using:\\n\\n```python\\nimport pandas as pd\\ndf = pd.read_csv('housing.csv')\\n```\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'What command do I need to use to download the dataset to Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, there is no specific symbol you need to use before the `wget` command by default. However, in certain environments like Google Colab, you can use an exclamation mark (`!`) before the `wget` command to execute it as a shell command. Here is an example of using `wget` in Google Colab:\\n\\n```bash\\n!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n```\\n\\nIn other environments, you can simply use `wget` as is, without any special symbols:\\n\\n```bash\\nwget URL\\n```\\nIf you face any issues, such as `wget` not being recognized or you are using a Mac and don't have `wget`, you can use alternatives like `curl` or follow specific instructions to install `wget`.\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'Is there a specific symbol I need to use before the wget command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"After downloading, you can read the dataset in a Kaggle Notebook by using the following steps:\\n\\n1. **Download the Dataset:**\\n   First, load the dataset directly in your Kaggle Notebook server using the command:\\n   ```\\n   !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n   ```\\n   Make sure you have turned on the internet for your session in the Kaggle notebook settings if you face any issues with resolving the host address.\\n\\n2. **Read the Dataset:**\\n   Once the dataset is successfully downloaded, you can read it into a pandas DataFrame with the following command:\\n   ```python\\n   import pandas as pd\\n   df = pd.read_csv('housing.csv')\\n   ```\\n\\nThis will load the dataset into the DataFrame `df`, and you can proceed with your data analysis or model building tasks from there.\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'After downloading, how do I read the dataset in a Kaggle Notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The instructions for loading the dataset in Kaggle Notebooks were provided by Harish Balasundaram.',\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'Who provided the instructions for loading the dataset in Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To filter a dataset by specific values in a column, you can use the following methods:\\n\\n1. Using logical operators:\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\n\\n2. Using the `isin` method:\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nBoth methods effectively filter the dataset to include only the rows where the `ocean_proximity` column has values \"<1H OCEAN\" or \"INLAND\". You can use `|` for ‘OR’ and `&` for ‘AND’ conditions.\\n\\nBy: Radikal Lukafiardi',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'How can I filter a dataset by specific values in a column?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The symbols used for 'OR' and 'AND' operations when filtering a dataset are `|` (pipe) for 'OR', and `&` (ampersand) for 'AND'.\",\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': \"What are the symbols used for 'OR' and 'AND' operations when filtering a dataset?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an alternative method to filter a dataset by specific column values. You can use the `isin` method to filter the rows where the column values match any of the values in a specified list. Here is an example:\\n\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nThis method is an alternative to using the logical OR operator (`|`) for filtering:\\n\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\n\\nBoth methods achieve the same result, but the `isin` method can be more concise, especially when filtering by multiple values.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'Is there an alternative method to filter a dataset by specific column values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Sure, here is an example of filtering a dataset using multiple conditions:\\n\\n```python\\n# Assume df is your DataFrame and we\\'re filtering based on the \"ocean_proximity\" column\\n# Combining conditions using logical OR (|)\\nfiltered_df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n\\n# Alternative approach using isin for multiple conditions\\nfiltered_df = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n\\n# Example of combining conditions using logical AND (&)\\nfiltered_df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") & (df[\"median_income\"] > 5.0)]\\n```\\n\\nYou can use the `|` operator for \\'OR\\' conditions and the `&` operator for \\'AND\\' conditions.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'Can you provide an example of filtering a dataset using multiple conditions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `isin()` method in the context of filtering a dataset is used to filter rows where a column\\'s value is in a specified list of values. For example, you can use it to filter rows in the `df` DataFrame where the `ocean_proximity` column has values either \"`<1H OCEAN`\" or \"`INLAND`\". Here\\'s how it\\'s used:\\n\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nThis method helps to simplify the code when you need to check for multiple values within a column.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'What does the isin() method do in the context of filtering a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To load the data using the requests library, you can follow this method:\\n\\n```python\\n# Get data for homework\\nimport requests\\n\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```\\n\\nThis script sends a GET request to the URL where the dataset is hosted. If the request is successful (status code 200), it writes the content to a file named `housing.csv`. If the request fails, it prints \"Download failed.\"',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What is an alternative way to load the data using the requests library?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To directly download the dataset from GitHub without using pandas, you can use the following methods: \\n\\n### Using the `requests` library:\\n1. **Import the library**:\\n    ```python\\n    import requests\\n    ```\\n2. **Specify the URL**:\\n    ```python\\n    url = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\n    ```\\n3. **Make the request and save the file**:\\n    ```python\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        with open(\\'housing.csv\\', \\'wb\\') as file:\\n            file.write(response.content)\\n    else:\\n        print(\"Download failed.\")\\n    ```\\n\\n### Using `urllib` library:\\n1. **Import the library**:\\n    ```python\\n    import urllib.request\\n    ```\\n2. **Specify the URL**:\\n    ```python\\n    url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n    ```\\n3. **Download the file**:\\n    ```python\\n    urllib.request.urlretrieve(url, \"housing.csv\")\\n    ```\\n\\nThese methods allow you to download the dataset file directly from GitHub and save it locally as \"housing.csv\".',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'How can I directly download the dataset from GitHub without using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Certainly! Here is a method to fetch data for homework using the `requests` library:\\n\\n```python\\nimport requests\\n\\n# Define the URL of the dataset\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\n\\n# Send a GET request to the URL\\nresponse = requests.get(url)\\n\\n# Check if the request was successful\\nif response.status_code == 200:\\n    # Open a file in write-binary mode and save the content\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    # Print a message if the download failed\\n    print(\"Download failed.\")\\n```\\n\\nThis code will download the dataset from the specified URL and save it as `housing.csv` locally. Make sure to handle any exceptions or errors as needed.',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'Could you share a method to fetch data for homework using the requests library?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If the status code is 200 when downloading data with requests, you should proceed to save the data to a file. Here’s how you can do it:\\n\\n```python\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```\\n\\nThis code checks if the response\\'s status code is 200 (indicating a successful request), and then writes the content to a file named `housing.csv`. If the status code is not 200, it prints \"Download failed.\"',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What should I do if the status code is 200 when downloading data with requests?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If the file download using the `requests` library fails, the code executes the `else` block where it prints the message \"Download failed.\" Here is the relevant part of the code:\\n\\n```python\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What does the code using the requests library do if the file download fails?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You may still see a null column after applying `.fillna()` if you have created a shallow copy of your dataframe. In a shallow copy, any changes made to the copied dataframe will also reflect in the original dataframe and vice versa. To avoid this, you should create a deep copy of your dataframe, ensuring that no references are attaching both variables. You can achieve this by using the `.copy()` method as follows:\\n\\n```python\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\n```\\n\\nThis will create a true independent copy of your dataframes, and any filling of null values will be correctly applied without unintended effects from the original dataframe.',\n",
       "  'answer_orig': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'document': '0bc4c3da',\n",
       "  'question': 'Why do I still see a null column after applying .fillna()?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To ensure that your dataframe copy does not reference the original variable, you should create a deep copy of the dataframe using the `.copy()` method. Here’s how you can do it:\\n\\n```python\\nX_copy = X.copy()\\n```\\n\\nThis creates a deep copy of `X` into `X_copy`, ensuring that any changes made to `X_copy` will not affect the original dataframe `X`. Without using `.copy()`, any assignment like `X_copy = X` would only create a shallow copy, where changes to `X_copy` would reflect back on `X`.',\n",
       "  'answer_orig': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'document': '0bc4c3da',\n",
       "  'question': 'How can I ensure that my dataframe copy does not reference the original variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The difference between a shallow copy and a deep copy in pandas is as follows:\\n\\n- **Shallow Copy**: When you create a shallow copy, you are essentially creating a new variable that references the original dataframe. Any changes made to the new variable will also reflect in the original dataframe. For example, `X_train = df_train` and `X_val = df_val` are shallow copies; they still reference the original dataframes.\\n\\n- **Deep Copy**: When you create a deep copy, you create a fully independent copy of the original dataframe. Any changes made to the new variable will not affect the original dataframe. To create a deep copy, you use the `copy()` method, like this: `X_copy = X.copy()`. This ensures that no references are linking the new variable and the original dataframe, maintaining independence between them.',\n",
       "  'answer_orig': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'document': '0bc4c3da',\n",
       "  'question': 'What is the difference between a shallow copy and a deep copy in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To create a deep copy of a dataframe in pandas, you can use the `copy()` method. Here is how you can do it:\\n\\n```python\\nX_copy = X.copy()\\n```\\n\\nThis will create a deep copy, ensuring that any changes made to `X_copy` will not affect the original dataframe `X`. Using `X_copy = X` without `.copy()` creates a shallow copy, meaning changes to `X_copy` will also reflect in `X`.',\n",
       "  'answer_orig': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'document': '0bc4c3da',\n",
       "  'question': 'How do I create a deep copy of a dataframe in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Making a duplicate dataframe using assignment (e.g., `X_train = df_train`) still links to the original dataframe because this process creates a shallow copy. In a shallow copy, the new variable references the same underlying data as the original variable. As a result, any changes made to the new variable will also affect the original variable.\\n\\nTo create an independent duplicate of the dataframe, you need to make a deep copy, which can be done using the `copy()` method, like so: `X_train = df_train.copy()`. In a deep copy, the new variable and the original variable reference separate data, ensuring that changes to one do not affect the other.',\n",
       "  'answer_orig': 'When creating a duplicate of your dataframe by doing the following:\\nX_train = df_train\\nX_val = df_val\\nYou’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\\nX_train = df_train.copy()\\nX_val = df_val.copy()\\nAdded by Ixchel García',\n",
       "  'document': '0bc4c3da',\n",
       "  'question': 'Why does making a duplicate dataframe using assignment still link to the original?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, it is allowed to use `train_test_split` from Scikit-Learn this week. Although it's implemented manually to enhance understanding, you're free to use the function if you want.\",\n",
       "  'answer_orig': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'document': 'c0ee2665',\n",
       "  'question': 'Is it allowed to use train_test_split from Scikit-Learn this week?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can implement `train_test_split` yourselves this week. The course encourages implementing it to better understand how it works. However, if you wish to start using Scikit-Learn’s `train_test_split` earlier, you are free to do so.',\n",
       "  'answer_orig': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'document': 'c0ee2665',\n",
       "  'question': 'Can we implement train_test_split ourselves this week?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you are supposed to use Scikit-Learn's `train_test_split` later in the course. Initially, you are implementing it yourself to better understand how it works, but later on, you will rely solely on Scikit-Learn’s functions. If you prefer, you can start using it earlier as well.\",\n",
       "  'answer_orig': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'document': 'c0ee2665',\n",
       "  'question': 'Are we supposed to use Scikit-Learn’s train_test_split later in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can start using Scikit-Learn’s `train_test_split` now if you prefer. While we are implementing it ourselves to better understand how it works, you are free to use it earlier if you would like to.',\n",
       "  'answer_orig': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'document': 'c0ee2665',\n",
       "  'question': 'Can I start using Scikit-Learn’s train_test_split now if I prefer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, we will rely on Scikit-Learn’s functions after this week. Initially, we implement certain functions ourselves to better understand how they work, but later we will primarily use Scikit-Learn’s functions.',\n",
       "  'answer_orig': 'Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it',\n",
       "  'document': 'c0ee2665',\n",
       "  'question': 'Will we rely on Scikit-Learn’s functions after this week?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can utilize `LinearRegression` from Scikit-Learn for this week's assignment.\",\n",
       "  'answer_orig': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'document': '3f60871d',\n",
       "  'question': \"Can I utilize LinearRegression from Scikit-Learn for this week's assignment?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, LinearRegression from Scikit-Learn will be covered next week.',\n",
       "  'answer_orig': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'document': '3f60871d',\n",
       "  'question': 'Will LinearRegression from Scikit-Learn be covered next week?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you should not be concerned about using LinearRegression from Scikit-Learn. According to the course materials, you are allowed to use it this week, and it will be covered in the upcoming sessions. Additionally, the LinearRegression function from Scikit-Learn is a standard tool and is recommended for use in the course.\\n\\nFor further reference, LinearRegression without regularization can be used with `sklearn.linear_model.LinearRegression`, and LinearRegression with regularization can be used with `sklearn.linear_model.Ridge`. More details can be found in the official Scikit-Learn documentation.',\n",
       "  'answer_orig': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'document': '3f60871d',\n",
       "  'question': 'Should I be concerned about using LinearRegression from Scikit-Learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is appropriate to apply LinearRegression from Scikit-Learn for your tasks. In fact, you will be learning how to use it next week.',\n",
       "  'answer_orig': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'document': '3f60871d',\n",
       "  'question': 'Is it appropriate to apply LinearRegression from Scikit-Learn for our tasks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there will be a detailed tutorial on LinearRegression from Scikit-Learn next week.',\n",
       "  'answer_orig': 'Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.',\n",
       "  'document': '3f60871d',\n",
       "  'question': 'Will there be a detailed tutorial on LinearRegression from Scikit-Learn next week?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The Scikit-Learn functions for linear regression models covered in week 2 are:\\n\\n- For linear regression without regularization: `sklearn.linear_model.LinearRegression`\\n- For linear regression with regularization: `sklearn.linear_model.Ridge`\\n\\nYou can find more information about these linear models on the Scikit-Learn documentation page [here](https://scikit-learn.org/stable/modules/linear_model.html).',\n",
       "  'answer_orig': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'document': 'f30217a7',\n",
       "  'question': 'What are the Scikit-Learn functions for linear regression models covered in week 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The function in Scikit-Learn for linear regression without regularization is `sklearn.linear_model.LinearRegression`.',\n",
       "  'answer_orig': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'document': 'f30217a7',\n",
       "  'question': 'What is the function in Scikit-Learn for linear regression without regularization?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The corresponding Scikit-Learn function for linear regression with regularization is `sklearn.linear_model.Ridge`. This function adds a regularization term to the linear regression cost function to help address issues like multicollinearity and overfitting.',\n",
       "  'answer_orig': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'document': 'f30217a7',\n",
       "  'question': 'Can you tell me the corresponding Scikit-Learn function for linear regression with regularization?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can find explanations of Scikit-Learn's linear models used in week 2 at the following link: [Scikit-Learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html).\",\n",
       "  'answer_orig': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'document': 'f30217a7',\n",
       "  'question': \"Where can I find explanations of Scikit-Learn's linear models used in week 2?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about Scikit-Learn functions for the linear regression models was added by Sylvia Schmitt.',\n",
       "  'answer_orig': 'What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\\nCorresponding function for model without regularization:\\nsklearn.linear_model.LinearRegression\\nCorresponding function for model with regularization:\\nsklearn.linear_model.Ridge\\nThe linear model from Scikit-Learn are explained  here:\\nhttps://scikit-learn.org/stable/modules/linear_model.html\\nAdded by Sylvia Schmitt',\n",
       "  'document': 'f30217a7',\n",
       "  'question': 'Who added the information about Scikit-Learn functions for the linear regression models?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In the context of regularization, the parameter `r` is similar to `alpha` in `sklearn.Ridge()`, but they are not the same mathematically. Both parameters control the strength of regularization, meaning increasing either `r` or `alpha` will lead to stronger regularization. However, they are applied differently:\\n\\n- In `sklearn.Ridge()`, `alpha` is used in the regularization term as follows:\\n  ```\\n  ||y - Xw||^2_2 + alpha * ||w||^2_2\\n  ```\\n- In the lesson's notebook (`train_linear_regression_reg` function), `r` modifies the main diagonal of the matrix to prevent multicollinearity:\\n  ```\\n  XTX = XTX + r * np.eye(XTX.shape[0])\\n  ```\\n\\nThus, while both `r` and `alpha` serve to enhance regularization, their mathematical applications differ.\",\n",
       "  'answer_orig': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'document': '91fc573d',\n",
       "  'question': 'In the context of regularization, is the parameter `r` the same as `alpha` in sklearn.Ridge()?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the `r` parameter in the `train_linear_regression_reg` function is similar to the `alpha` parameter in `sklearn.Ridge` in that both control the strength of regularization. Increasing either `r` or `alpha` will lead to stronger regularization. However, they are not mathematically identical. In `sklearn.Ridge`, the regularization is applied as:\\n\\n\\\\[ ||y - Xw||^2_2 + \\\\alpha * ||w||^2_2 \\\\]\\n\\nIn the `train_linear_regression_reg` function, the regularization is applied by adding a \"noise\" term to the main diagonal of the matrix \\\\(X^TX\\\\):\\n\\n\\\\[ XTX = XTX + r * \\\\text{np.eye}(XTX.shape[0]) \\\\]\\n\\nThis method helps prevent multicollinearity by \"breaking\" the process of finding the inverse of the matrix.',\n",
       "  'answer_orig': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'document': '91fc573d',\n",
       "  'question': 'Is `r` in `train_linear_regression_reg` equivalent to `alpha` in sklearn.Ridge() for regularization?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The primary function of `r` in the lesson\\'s notebook regularization is to serve as a regularization parameter that adds \"noise\" to the main diagonal of the matrix `XTX` to prevent multicollinearity, which obstructs the inversion of the matrix. In contrast, `alpha` in `sklearn.Ridge()` also controls the strength of regularization, but it does so by directly adding a penalty term to the regression objective function. Specifically, `alpha` penalizes the sum of squares of the coefficients, thus controlling overfitting. \\n\\nHere’s a detailed comparison of how they are used:\\n\\n- **sklearn.Ridge()**:\\n  - Regularization is applied through the term `alpha * ||w||^2_2`.\\n  - Formulation: ||y - Xw||^2_2 + alpha * ||w||^2_2.\\n\\n- **Lesson’s notebook (`train_linear_regression_reg` function)**:\\n  - Regularization is applied by adding `r * np.eye(XTX.shape[0])` to `XTX`.\\n  - This approach adds noise to the diagonal to prevent issues with multicollinearity.\\n\\nThough both `r` and `alpha` are used to control the strength of regularization, their mathematical implementations differ.',\n",
       "  'answer_orig': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'document': '91fc573d',\n",
       "  'question': 'What is the primary function of `r` in the lesson’s notebook regularization, and how does it differ from `alpha` in sklearn.Ridge()?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': '`r` in the `train_linear_regression_reg` function is a regularization parameter. It works similarly to `alpha` in `sklearn.Ridge()`, in that both parameters control the strength of regularization—the higher the value, the stronger the regularization. \\n\\nHowever, the way these parameters are used differs slightly:\\n- In the `train_linear_regression_reg` function, `r` adds \"noise\" to the main diagonal of the matrix (XTX + r * np.eye(XTX.shape[0])) to prevent multicollinearity and aid in finding the inverse matrix.\\n- In `sklearn.Ridge()`, `alpha` directly multiplies the L2 norm of the coefficients, thus ||y - Xw||^2_2 + alpha * ||w||^2_2.\\n\\nSo while both `r` and `alpha` serve to regulate the impact of the regularization on the model, the exact mathematical implementation of the regularization is different.',\n",
       "  'answer_orig': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'document': '91fc573d',\n",
       "  'question': 'Can you explain how `r` in `train_linear_regression_reg` helps in regression and does it behave like `alpha` in sklearn.Ridge()?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `train_linear_regression_reg` function in our lesson and the `alpha` parameter in `sklearn.Ridge()` both control the \"strength\" of regularization, but they are implemented differently.\\n\\nIn `sklearn.Ridge()`:\\nThe regularization is applied directly to the cost function as:\\n\\\\[ ||y - Xw||^2_2 + \\\\alpha * ||w||^2_2 \\\\]\\nHere, `alpha` is the parameter that scales the regularization term \\\\( ||w||^2_2 \\\\), which penalizes large coefficient values directly in the cost function.\\n\\nIn the `train_linear_regression_reg` function:\\nRegularization is applied by adding a small value `r` to the diagonal elements of the matrix \\\\( X^TX \\\\):\\n\\\\[ X^TX = X^TX + r * \\\\text{np.eye}(X^TX.\\\\text{shape}[0]) \\\\]\\nHere, `r` is the regularization parameter that adds \"noise\" to the main diagonal of \\\\( X^TX \\\\). This technique helps to prevent issues with multicollinearity and to \"break\" the problems with finding the inverse of \\\\( X^TX \\\\).\\n\\nWhile both `r` in our lesson and `alpha` in `sklearn.Ridge()` serve the purpose of regularization, they do so through different mathematical implementations.',\n",
       "  'answer_orig': '`r` is a regularization parameter.\\nIt’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here\\'s how both are used:\\nsklearn.Ridge()\\n||y - Xw||^2_2 + alpha * ||w||^2_2\\nlesson’s notebook (`train_linear_regression_reg` function)\\nXTX = XTX + r * np.eye(XTX.shape[0])\\n`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.',\n",
       "  'document': '91fc573d',\n",
       "  'question': 'How does the `train_linear_regression_reg` function in our lesson utilize `r` compared to the use of `alpha` in sklearn.Ridge()?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Linear regression doesn't give a perfect fit in lesson 2.8 despite training on X_train because it is a relatively simple model. A perfect fit would indicate overfitting, meaning the model has adapted too closely to the training data, which negatively impacts its performance on unseen data. The linear nature of the model also limits its ability to fit all data points perfectly, especially if the relationship between the features and target is not perfectly linear.\",\n",
       "  'answer_orig': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'document': 'fe3139f6',\n",
       "  'question': \"Why doesn't linear regression give a perfect fit in lesson 2.8 despite training on X_train?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you try to fit all data points using a method like `scipy.optimize.curve_fit`, you will likely end up with a model that fits the training data perfectly. However, this is usually a sign of overfitting, which means that the model may capture noise rather than the underlying pattern. Consequently, such a model is likely to perform poorly on previously unseen data, as it would not generalize well beyond the training set.',\n",
       "  'answer_orig': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'document': 'fe3139f6',\n",
       "  'question': 'What happens if you try to fit all data points using a method like scipy.optimize.curve_fit?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A linear model handles a single feature by fitting a straight line through the data points. Given that it is a simple model, it uses the least squares method to minimize the distance between the predicted values (y_pred) and the actual values (y). However, the linear model aims to generalize from the data rather than fit it perfectly to avoid overfitting. Hence, it balances the trade-off between bias and variance to provide a model that performs well on both the training set and unseen data.\\n\\nIn the case of a single feature, imagine plotting the data points on a graph and drawing the best-fit linear line through them. Since the model is linear, it cannot perfectly pass through all data points unless the points lie perfectly on a line, which is rarely the case in real-world data. Instead, it optimally positions the line to minimize overall prediction error while ensuring the model remains generalizable.\\n\\nIllustratively, to seek perfection with a single feature, you could use non-linear least squares methods (e.g., `scipy.optimize.curve_fit`), but this could lead to overfitting, making the model perform poorly on new, unseen data.',\n",
       "  'answer_orig': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'document': 'fe3139f6',\n",
       "  'question': 'How does a linear model handle a single feature when trying to fit all data points?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The linear regression model does not provide a \"perfect\" fit because it is inherently a simple model designed to find a balance between underfitting and overfitting. In lesson 2.8, the predicted values (\\\\( y_{\\\\text{pred}} \\\\)) can differ from the actual values (\\\\( y \\\\)) even after training because a linear regression model is constrained to fitting a straight line through the data, which cannot capture all the complexities or variations in the data points. If the model were to fit 100% perfectly, it would likely be a sign of overfitting, meaning that it has adapted too closely to the specific training data and would perform poorly on new, unseen data. This concept is illustrated by the challenge of drawing a line to fit all data points perfectly, which is not generally feasible for a linear model.',\n",
       "  'answer_orig': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'document': 'fe3139f6',\n",
       "  'question': 'Why is y_pred different from y after training the model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A sign that indicates a model has overfitted the training data is when \"The accuracy and the loss are both still the same or nearly the same while training.\" This suggests that the model is not generalizing well beyond the training data, fitting too closely to the specific examples in the training set rather than learning the underlying patterns.',\n",
       "  'answer_orig': 'Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\\nAdded by Andrii Larkin',\n",
       "  'document': 'fe3139f6',\n",
       "  'question': 'What sign indicates that a model has overfitted the training data?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The reason all your missing values end up in the training dataframe when using a random seed of 42 is that the purpose of the seed value is to ensure the data split is done in a consistent and reproducible manner. When a seed value of 42 is used, it results in all learners having the same behavior, which in this particular case means that all missing values are allocated to the training dataframe. If you use a different seed value, such as 9, the missing values would be distributed across the training, validation, and test dataframes instead.',\n",
       "  'answer_orig': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'document': '48aac030',\n",
       "  'question': 'Why do all my missing values end up in the training dataframe when using a random seed of 42?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The purpose of using a seed value when splitting data for training and validation is to ensure consistency and reproducibility in the data split. For example, using a seed value of 42 ensures that all learners will get the same behavior and the same split in the data, making it possible to have consistent and comparable results across different runs and users. This method allows for the generation of the same proportion split every time, which is crucial for debugging and sharing work.',\n",
       "  'answer_orig': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'document': '48aac030',\n",
       "  'question': 'What is the purpose of using a seed value when splitting data for training and validation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you use a random seed value other than 42, the distribution of missing values will change. Specifically, the missing values will no longer all end up in the training dataframe; instead, they will appear across the training, validation, and test dataframes. This change in random seed values affects the proportion split of the data and consequently the allocation of missing values among the dataframes.',\n",
       "  'answer_orig': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'document': '48aac030',\n",
       "  'question': 'If I use a random seed value other than 42, what will happen to the distribution of missing values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A random seed of 42 is used in the homework to ensure consistent behavior among learners because it ensures that everyone gets the same behavior when executing the code. Specifically, in this scenario, using a random seed of 42 causes all missing values to end up in the training dataframe, making the results consistent and comparable across all learners. This uniform behavior is crucial for effective learning and evaluation.\\n\\n',\n",
       "  'answer_orig': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'document': '48aac030',\n",
       "  'question': 'Why is a random seed of 42 used in the homework to ensure consistent behavior among learners?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Changing the random seed value affects where the missing values appear in the dataframes because it impacts the random generation of the proportion split among training, validation, and test dataframes. As per the example given in the FAQ, using a seed value of 42 consistently results in all missing values ending up in the training dataframe, ensuring uniform behavior across all learners. If a different random seed value is used, such as 9, the missing values would be distributed differently among the dataframes.',\n",
       "  'answer_orig': 'One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.',\n",
       "  'document': '48aac030',\n",
       "  'question': 'How does changing the random seed value affect where missing values appear in the dataframes?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to shuffle the initial dataset using a pandas built-in function. You can achieve this with the `pandas.DataFrame.sample` function. Here are the commands to shuffle the dataset and reset the index:\\n\\n```python\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\n```\\n\\n- Setting `frac=1` returns a shuffled version of the complete dataset.\\n- Setting `random_state=seed` ensures the same randomization as used in the course resources.',\n",
       "  'answer_orig': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'document': '28321bc2',\n",
       "  'question': 'Is it possible to shuffle the initial dataset using a pandas built-in function?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To shuffle the complete dataset using pandas, you can use the pandas built-in function `pandas.DataFrame.sample`. Here are the necessary commands to achieve this:\\n\\n1. **Set frac=1**: This parameter ensures that the sample returned is a shuffled version of the entire dataset.\\n2. **Set random_state=seed**: This parameter allows you to achieve the same randomization as used in the course resources.\\n3. **Reset the index**: This step ensures that the index is reset after shuffling.\\n\\nHere's the code to shuffle the dataset:\\n\\n```python\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\n```\\n\\nUsing these commands, you will have a completely shuffled version of the dataset with a reset index.\",\n",
       "  'answer_orig': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'document': '28321bc2',\n",
       "  'question': 'How can the complete dataset be shuffled using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When `frac` is set to 1 in `pandas.DataFrame.sample`, it results in returning a shuffled version of the complete dataset. Here is an example of how you can use this feature to shuffle your dataset and reset the index:\\n\\n```python\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\n```\\n\\nThis ensures that the entire dataset is included and shuffled, and the `random_state` parameter ensures reproducibility by yielding the same randomization each time.',\n",
       "  'answer_orig': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'document': '28321bc2',\n",
       "  'question': 'What happens when frac is set to 1 in pandas.DataFrame.sample?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To ensure the randomization of a dataset is consistent with the course resources, you should use the pandas built-in function `pandas.DataFrame.sample` with the `random_state` parameter set to a specific seed value. This will ensure that the randomization process is reproducible and consistent with the resources provided in the course. Here is the specific code to achieve this:\\n\\n```python\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\n```\\n\\nBy setting `frac=1`, you shuffle the entire dataset, and by specifying `random_state=seed`, you ensure that the shuffling process is consistent with the course.',\n",
       "  'answer_orig': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'document': '28321bc2',\n",
       "  'question': 'How do you ensure the randomization of a dataset is consistent with the course resources?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To shuffle and reset the index of a pandas DataFrame, you can use the following commands:\\n\\n1. Shuffle the DataFrame:\\n```python\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\n```\\n\\n2. Reset the index of the shuffled DataFrame:\\n```python\\ndf_shuffled.reset_index(drop=True, inplace=True)\\n```\\n\\nHere, `frac=1` ensures that the entire DataFrame is returned in a shuffled order, and `random_state=seed` guarantees reproducibility by using the same seed value each time. The `reset_index(drop=True, inplace=True)` command will reset the DataFrame's index and drop the old index, applying the changes directly to the DataFrame.\",\n",
       "  'answer_orig': 'It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\\nSetting random_state=seed will result in the same randomization as used in the course resources.\\ndf_shuffled = df.sample(frac=1, random_state=seed)\\ndf_shuffled.reset_index(drop=True, inplace=True)\\nAdded by Sylvia Schmitt',\n",
       "  'document': '28321bc2',\n",
       "  'question': 'What are the commands for shuffling and resetting the index of a pandas DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your homework answer doesn’t match any of the given options, it’s normal due to differences in environments, such as different versions of operating systems, libraries, and Python. In such cases, just select the option that’s closest to your answer. \\n\\n',\n",
       "  'answer_orig': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'document': 'edb92d22',\n",
       "  'question': \"What if my homework answer doesn't match any of the given options?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Your result might differ from the provided options due to the way you split your data into training, validation, and test sets. Different methods of splitting the data can lead to variations in the dataset composition, even if the overall ratios remain the same. For example, the data splitting strategies outlined below both achieve a 60%/20%/20% ratio but produce different training, validation, and test datasets:\\n\\n1. \\n```python\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n```\\n\\n2.\\n```python\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\n```\\n\\nIt is recommended to use the second method, which is more consistent with the lessons and homeworks provided. Additionally, differences in computational environments, such as variations in operating system versions, library versions, and even Python versions, can contribute to discrepancies in results. Therefore, if your answer is very close but not exactly matching any of the provided options, you should select the closest option available.',\n",
       "  'answer_orig': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'document': 'edb92d22',\n",
       "  'question': 'Why might my result differ from the provided options?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your homework answers differ due to different environments, such as different OS versions, library versions, or Python versions, you should select the option that is closest to your answer. This variation is normal and expected due to the diversity in computing environments.\\n\\n',\n",
       "  'answer_orig': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'document': 'edb92d22',\n",
       "  'question': 'How should I handle differing homework answers due to different environments?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Discrepancies in your homework answers compared to the options can be caused by different factors including:\\n\\n1. **Different Environments**: Your computer may have different versions of the operating system, libraries, or even Python. This can lead to slightly different results even if the code logic is the same.\\n\\n2. **Data Splitting Techniques**: The way data is split into training, validation, and test sets can affect your results. Slight differences in the data used for training and testing due to different splitting methods can lead to variations in accuracy or other performance metrics.\\n\\nIn such cases, it's recommended to select the option closest to your answer.\",\n",
       "  'answer_orig': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'document': 'edb92d22',\n",
       "  'question': 'What can cause discrepancies in my homework answers compared to the options?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If your computed homework answer isn't listed in the options, select the option that is closest to your answer.\",\n",
       "  'answer_orig': 'That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\\nIf it’s the case, just select the option that’s closest to your answer',\n",
       "  'document': 'edb92d22',\n",
       "  'question': \"What step should I take if my computed homework answer isn't listed in the options?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"‘Use the training only’ in homework 2, question 3 in section 2 means that you should compute the mean using only the training data set. You should not use the validation or test data sets for this calculation.\\n\\nYou can calculate the mean using the following methods:\\n1. `df_train['column_name'].mean()`\\n2. `df_train['column_name'].describe()`\",\n",
       "  'answer_orig': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'document': 'f488ce85',\n",
       "  'question': 'What does ‘use the training only’ mean in homework 2, question 3 in section 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In question 3 of HW02, you should use the training data set to calculate the mean, not the validation or test data sets. This ensures that the mean is computed using the same dataset that the model is trained on, maintaining consistency in the training process. You can calculate the mean using the following code snippet:\\n\\n```python\\ndf_train['column_name'].mean()\\n```\\n\\nAlternatively, you can use the `.describe()` method:\\n\\n```python\\ndf_train['column_name'].describe()\\n```\",\n",
       "  'answer_orig': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'document': 'f488ce85',\n",
       "  'question': 'When calculating the mean in HW02, question 3, should validation or test data sets be used?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To compute the mean for the training data set as described in HW02, question 3, you should use only the training data set without including the validation or test data sets. You can calculate the mean using the following Python code with Pandas:\\n\\n```python\\ndf_train['column_name'].mean()\\n```\\n\\nAlternatively, you can use the `describe()` function to get the mean along with other statistical details:\\n\\n```python\\ndf_train['column_name'].describe()\\n```\",\n",
       "  'answer_orig': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'document': 'f488ce85',\n",
       "  'question': 'How can I compute the mean for the training data set as per HW02, question 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"An alternative method to get the mean for the training data set in homework 2, question 3 in section 2 is to use the `.describe()` function. Specifically, you would use:\\n\\n```python\\ndf_train['column_name'].describe()\\n```\\n\\nThis function provides a summary of statistics for the column, including the mean.\",\n",
       "  'answer_orig': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'document': 'f488ce85',\n",
       "  'question': 'What is an alternative method to get the mean for the training data set in homework 2, question 3 in section 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In HW02, question 3, the calculation of the mean is restricted to the training data set only. This restriction is to ensure that the mean represents the distribution of the training data, which is used to build and evaluate the model. Including validation or test data in this calculation could lead to data leakage and result in biased or overly optimistic evaluations of the model's performance. Therefore, you should exclude validation or test data because they are intended for model evaluation and should not influence the training process.\\n\\nHere's how you can calculate the mean using the training data set:\\n\\n```python\\ndf_train['column_name'].mean()\\n```\\nor\\n\\n```python\\ndf_train['column_name'].describe()\\n```\",\n",
       "  'answer_orig': \"In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\\ndf_train['column_name'].mean( )\\nAnother option:\\ndf_train[‘column_name’].describe()\\n(Bhaskar Sarma)\",\n",
       "  'document': 'f488ce85',\n",
       "  'question': 'Can you explain why validation or test data sets are excluded when calculating the mean in HW02, question 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In regression, it is necessary to transform the target variable to a logarithmic distribution when the target variable has a long tail distribution, such as prices with a wide range. This transformation can help address skewness in the target variable, making the data more suitable for modeling. However, it is important to note that the `np.log1p()` method used for this transformation will not work if the target variable has negative values. \\n\\nAdditionally, this transformation should only be applied if you observe that your target variable is highly skewed, which can be evaluated by plotting its distribution. This helps in understanding the skewness and how it impacts the distribution of your dataset.',\n",
       "  'answer_orig': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'document': 'bf395099',\n",
       "  'question': 'When is it necessary to transform the target variable to a logarithmic distribution during regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"For which type of target variable distribution is np.log1p() most useful?\\n\\nnp.log1p() is most useful for target variables that have a long tail distribution, such as prices with a wide range. This transformation can help to normalize the data and make it more suitable for regression models. However, it's important to note that this method will not work if the target variable has negative values.\",\n",
       "  'answer_orig': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'document': 'bf395099',\n",
       "  'question': 'For which type of target variable distribution is np.log1p() most useful?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Transforming the target variable is commonly beneficial when the target variable has a long tail distribution, such as with prices that have a wide range. In this case, using a logarithmic transformation like the `np.log1p()` method can help manage the skewness in the distribution. However, this method is not suitable if the target variable has negative values.',\n",
       "  'answer_orig': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'document': 'bf395099',\n",
       "  'question': 'What is a common example where transforming the target variable is beneficial?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When applying `np.log1p()` to the target variable, you should be cautious if your target variable has negative values because this method will not work with negative values.',\n",
       "  'answer_orig': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'document': 'bf395099',\n",
       "  'question': 'What should you be cautious of when applying np.log1p() to the target variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `np.log1p()` method might fail when applied to certain target variables because it does not work with negative values. If the target variable includes negative values, the transformation using `np.log1p()` will be problematic, as the logarithm of a negative number is undefined in the real number space.\\n\\nFor further details, please refer to the section \"2. Machine Learning for Regression\" in the FAQ database, which mentions the specific limitation of the `np.log1p()` method when dealing with target variables with negative values.',\n",
       "  'answer_orig': 'When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work',\n",
       "  'document': 'bf395099',\n",
       "  'question': 'Why might the np.log1p() method fail when applied to certain target variables?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A `ValueError: shapes not aligned` during arithmetic operations in machine learning is caused by attempting to perform an arithmetic operation between two arrays of different shapes or dimensions. This occurs when the number of columns in the first array does not match the number of rows in the second array during operations such as matrix multiplication. \\n\\nTo avoid this error, ensure that the dimensions of the arrays align correctly: the number of columns in the first matrix should match the number of rows in the second matrix. Additionally, you can sometimes resolve this issue by using the `*` operator instead of the `dot()` method to achieve the desired dot product without encountering dimension mismatch errors.',\n",
       "  'answer_orig': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'document': '01cd3b35',\n",
       "  'question': 'What causes a ValueError: shapes not aligned during arithmetic operations in machine learning?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To resolve a ValueError related to different shapes or dimensions of arrays in a regression model, you can try the following steps based on the provided context:\\n\\n1. **Use the correct operation**: If you encounter a shape alignment issue during an arithmetic operation, consider using the `*` operator instead of the `dot()` method. This can sometimes resolve the issue by performing element-wise multiplication instead of matrix multiplication.\\n\\n2. **Check matrix dimensions**: Ensure that the dimensions of the arrays you are trying to multiply are compatible. For matrix multiplication (using `np.dot`), the number of columns in the first matrix should match the number of rows in the second matrix. If this condition is not met, rearrange or reshape the arrays so that the dimensions align properly.\\n\\nFollowing these steps should help you resolve a ValueError related to array shapes or dimensions in your regression model.',\n",
       "  'answer_orig': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'document': '01cd3b35',\n",
       "  'question': 'How can a ValueError related to different shapes or dimensions of arrays be resolved in a regression model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In some scenarios, operands could not be broadcast together with shapes might occur due to attempting an arithmetic operation between two arrays that have different shapes or dimensions. For instance, if we have one array with shape (3, 2) and another array with shape (2, 3), and we try to perform addition or multiplication directly, it would result in a broadcasting error since their shapes are incompatible for element-wise operations.\\n\\nHere is an example scenario:\\n\\n1. **Mismatch in Shapes for Addition:**\\n    ```python\\n    import numpy as np\\n\\n    array1 = np.array([[1, 2],\\n                      [3, 4],\\n                      [5, 6]])\\n\\n    array2 = np.array([[1, 2, 3],\\n                      [4, 5, 6]])\\n\\n    result = array1 + array2  # This will raise a ValueError\\n    ```\\n    In this case, `array1` has shape (3, 2) and `array2` has shape (2, 3), so they cannot be broadcast together for element-wise addition.\\n\\n2. **Mismatch in Shapes for Multiplication:**\\n    ```python\\n    array3 = np.array([[1, 2],\\n                      [3, 4],\\n                      [5, 6]])\\n\\n    array4 = np.array([1, 2])\\n\\n    result = array3 * array4  # This will raise a ValueError\\n    ```\\n    Similarly, `array3` has shape (3, 2) and `array4` has shape (2,), which are not compatible for element-wise multiplication.\\n\\nTo resolve such issues, you might consider using the `*` operator instead of the `dot()` method if you are trying to compute the dot product, ensuring that the conditions for broadcasting are met, meaning the shapes of the arrays are aligned properly for intended operations.',\n",
       "  'answer_orig': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'document': '01cd3b35',\n",
       "  'question': 'Can you explain a scenario where operands could not be broadcast together with shapes might occur?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an alternative to using the `dot()` method to avoid a `ValueError` when dealing with differently shaped arrays. According to the FAQ, you can use the `*` operator instead of the `dot()` method to solve the issue and still achieve the dot product. This can help to avoid shape-related errors during arithmetic operations between arrays.',\n",
       "  'answer_orig': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'document': '01cd3b35',\n",
       "  'question': 'Is there an alternative to using dot() method to avoid ValueError when dealing with differently shaped arrays?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix alignment errors and still obtain the dot product, you can replace the `dot()` method with the `*` operator. This change can help resolve shape or dimension mismatches that cause alignment errors.\\n\\n',\n",
       "  'answer_orig': 'If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\\n(Santhosh Kumar)',\n",
       "  'document': '01cd3b35',\n",
       "  'question': 'What operation can replace dot() to fix alignment errors and still get the dot product?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To copy a dataframe in such a way that the original dataframe remains unchanged, you can create a deep copy using the `.copy()` method. Here is how you can do it:\\n\\n```python\\nX_copy = X.copy()\\n```\\n\\nThis ensures that any changes made to `X_copy` will not affect the original dataframe `X`. If you simply use `X_copy = X`, it will create a shallow copy, and changes made to `X_copy` will reflect back to `X`.',\n",
       "  'answer_orig': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'document': '5551c92e',\n",
       "  'question': 'How can I copy a dataframe in such a way that the original dataframe remains unchanged?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To create a deep copy of a dataframe, you should use `X_copy = X.copy()`. This will ensure that any changes made to `X_copy` do not affect the original dataframe `X`.',\n",
       "  'answer_orig': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'document': '5551c92e',\n",
       "  'question': 'What should I use if I want to create a deep copy of a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you use `X_copy = X` instead of `X.copy()` to copy a dataframe, any changes made to `X_copy` will reflect back to the original dataframe `X`. This is because `X_copy = X` creates a shallow copy, meaning both `X` and `X_copy` reference the same data. In contrast, `X.copy()` creates a deep copy, ensuring that `X_copy` is an entirely separate dataframe, and changes to `X_copy` will not affect `X`.',\n",
       "  'answer_orig': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'document': '5551c92e',\n",
       "  'question': 'What happens if I use X_copy = X instead of X.copy() to copy a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Changes in `X_copy` affect `X` when using `X_copy = X` because this creates a shallow copy or a \"view\" of the original dataframe, not a deep copy. Both `X_copy` and `X` reference the same underlying data. Therefore, any changes made to `X_copy` will also be reflected in `X`. To avoid this, you can create a deep copy of the dataframe using `X_copy = X.copy()`, which ensures that `X_copy` and `X` are independent of each other.',\n",
       "  'answer_orig': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'document': '5551c92e',\n",
       "  'question': 'Why do changes in X_copy affect X when using X_copy = X?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': '`X.copy()` creates a deep copy of the dataframe `X`. This means that any modifications made to `X_copy` will not affect `X`.\\n\\nOn the other hand, `X_copy = X` does not create a true copy but rather a \"view\" on the same data. Any changes made to `X_copy` will reflect back to `X` because both variables reference the same underlying data.\\n\\nIn summary, `X.copy()` ensures `X` remains unchanged when `X_copy` is modified, while `X_copy = X` creates a linked reference where changes to one will affect the other.',\n",
       "  'answer_orig': 'Copy of a dataframe is made with X_copy = X.copy().\\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\\n(Memoona Tahira)',\n",
       "  'document': '5551c92e',\n",
       "  'question': 'What does X.copy() do that X_copy = X does not?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The impact of the long tail on the mean, median, and mode is as follows:\\n\\n1. **Mean**: In the presence of a long tail, the mean is pulled in the direction of the tail. This happens because the mean is sensitive to extreme values. Therefore, the mean becomes larger (if the tail is to the right) or smaller (if the tail is to the left) and is no longer a representative measure of central tendency for the distribution.\\n\\n2. **Median**: The median, being the middle value, is less affected by extreme values than the mean, but it can still shift slightly towards the direction of the tail. The median will remain at the point where 50% of the data values lie below it and 50% above it.\\n\\n3. **Mode**: The mode, which is the most frequent value in the dataset, remains at the peak of the distribution. In a skewed distribution with a long tail, the mode will not coincide with the mean and median; it will remain closer to the higher concentration of data points.\\n\\nOverall, the long tail results in the mean, median, and mode being different from each other, unlike the case in a normal distribution where they are typically equal. Additionally, with a long tail, the mean is no longer representative of the central value of the dataset.',\n",
       "  'answer_orig': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'document': '94f928d2',\n",
       "  'question': 'What is the impact of the long tail on the mean, median, and mode?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The normal distribution and a distribution with a long tail differ in several key ways:\\n\\n1. **Symmetry**:\\n   - **Normal Distribution**: It is symmetric, meaning the area under the curve is the same on both sides of the mean.\\n   - **Long Tail Distribution**: It is not symmetric. The area under the curve is different on each side due to the presence of a few high-value observations extending the tail.\\n\\n2. **Central Tendency**:\\n   - **Normal Distribution**: The mean, median, and mode are all equal.\\n   - **Long Tail Distribution**: The mean, median, and mode are different because the long tail skews the distribution.\\n\\n3. **Representation**:\\n   - **Normal Distribution**: The mean is a good representation of the central value.\\n   - **Long Tail Distribution**: The mean is not representative due to the skewed data, making it less reliable as a central measure.\\n\\n4. **Probability Distribution**:\\n   - **Normal Distribution**: The probability of being on the left or right side of the mean is the same.\\n   - **Long Tail Distribution**: The probability of being on one side of the mean is different from the other side, influenced by the long tail.\\n\\nThus, a long tail distribution reflects significant deviations from the characteristics of a normal distribution, especially in terms of symmetry and central tendency.',\n",
       "  'answer_orig': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'document': '94f928d2',\n",
       "  'question': 'How does the normal distribution differ from a distribution with a long tail?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In a long tail distribution, the mean is no longer representative because the presence of a few observations with extremely high values skews the distribution. Unlike a normal distribution where the mean, median, and mode are the same (the area under the curve is symmetrically distributed), a long tail distribution has an asymmetric area. As a result, in a long tail distribution, the mean, median, and mode differ, making the mean less reflective of the central tendency of the data. This discrepancy arises because the large values in the tail pull the mean towards them, which does not necessarily represent the majority of the data points.',\n",
       "  'answer_orig': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'document': '94f928d2',\n",
       "  'question': 'Why is the mean no longer representative in a long tail distribution?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One side of the long tail distribution has a different area because the distribution includes a few observations with high values. This changes the behavior of the distribution, causing the area to differ on each side. Consequently, the mean, median, and mode are no longer the same, which means the mean is no longer representative, the range is larger than before, and the probabilities of being on the left or right side are not the same.',\n",
       "  'answer_orig': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'document': '94f928d2',\n",
       "  'question': 'What causes one side of the long tail distribution to have a different area?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'High-value observations affect the distribution\\'s range by increasing it. When a distribution has a few observations with high values, it results in a \"long tail.\" This long tail means that the area under the distribution curve is different on each side, and the mean, median, and mode are no longer the same. Consequently, the range of the distribution becomes larger than it was before, making the mean less representative of the data set as a whole.',\n",
       "  'answer_orig': 'One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\\n(Tatiana Dávila)',\n",
       "  'document': '94f928d2',\n",
       "  'question': \"How do high-value observations affect the distribution's range?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.',\n",
       "  'answer_orig': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'document': '266faa6d',\n",
       "  'question': 'What does standard deviation measure in statistics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A low standard deviation indicates that the values tend to be close to the mean (expected value) of the set.',\n",
       "  'answer_orig': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'document': '266faa6d',\n",
       "  'question': 'What does a low standard deviation indicate about the values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A high standard deviation indicates that the values are spread out over a wider range. This means there is a greater amount of variation or dispersion among the values in the data set.',\n",
       "  'answer_orig': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'document': '266faa6d',\n",
       "  'question': 'What does a high standard deviation indicate about the values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If values have a low standard deviation, they tend to be close to the mean (also called the expected value) of the set.',\n",
       "  'answer_orig': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'document': '266faa6d',\n",
       "  'question': 'Where do values tend to be if they have a low standard deviation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Another name for the mean in statistics is the \"expected value.\"',\n",
       "  'answer_orig': 'In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\\n(Aadarsha Shrestha)',\n",
       "  'document': '266faa6d',\n",
       "  'question': 'What is another name for the mean in statistics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, regularization techniques are not always needed in machine learning. Their application depends on the specific situation and problem at hand. It is particularly recommended to use regularization when working with small datasets or complex models to prevent overfitting. However, the necessity of regularization varies based on the quality and size of the data. It's important to evaluate each case individually to determine if regularization is needed.\",\n",
       "  'answer_orig': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'document': 'c21f99f5',\n",
       "  'question': 'Do we need to always use regularization techniques in machine learning?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You should consider using regularization in your models primarily when you're dealing with small datasets or complex models. Regularization helps in preventing overfitting under these conditions. However, its necessity can vary depending on the quality and size of your dataset. It is advisable to evaluate each case individually to determine if regularization is needed.\",\n",
       "  'answer_orig': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'document': 'c21f99f5',\n",
       "  'question': 'When should we consider using regularization in our models?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there are scenarios where regularization is not necessary. According to the context, the necessity of applying regularization depends on factors such as the specific situation, problem, data quality, and dataset size. Regularization is particularly recommended for small datasets or complex models to prevent overfitting. Therefore, its application should be evaluated case by case to determine if it is needed.',\n",
       "  'answer_orig': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'document': 'c21f99f5',\n",
       "  'question': 'Is there a scenario where regularization is not necessary?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The decision to apply regularization in your machine learning models is influenced by several factors. According to the provided context, regularization is recommended in certain situations to prevent overfitting. Specifically, it is particularly useful when dealing with:\\n\\n1. **Small datasets**: Regularization can help prevent the model from overfitting to the limited amount of training data.\\n2. **Complex models**: When using more complex models that are prone to overfitting, regularization can be beneficial to ensure the model generalizes well to new data.\\n\\nHowever, the necessity of regularization can vary depending on the quality and size of the dataset. It is important to evaluate each case individually to determine if regularization is needed. In essence, regularization should be considered when there are risks of overfitting, but its application is not mandatory in all scenarios.',\n",
       "  'answer_orig': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'document': 'c21f99f5',\n",
       "  'question': 'What factors determine if we should apply regularization?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Deciding if regularization is required for your data depends on the specific situation and problem. Regularization is particularly recommended when training machine learning models with small datasets or complex models to prevent overfitting. However, its necessity can vary depending on the data quality and size. You should evaluate each case individually to determine if regularization is needed.',\n",
       "  'answer_orig': 'The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\\n(Daniel Muñoz Viveros)',\n",
       "  'document': 'c21f99f5',\n",
       "  'question': 'How do we decide if regularization is required for our data?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Defining functions for regression is useful because it speeds up the development process. Functions like `prepare_df`, which prepares dataframes and response vectors, and `rmse` (root mean squared error) along with `train_linear_regression` from the class notebook help automate repetitive tasks, streamline the workflow, and ensure consistency in data processing and model training. This way, you can focus more on analyzing and improving your model rather than handling the same data preparation steps repeatedly.',\n",
       "  'answer_orig': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'document': '13702957',\n",
       "  'question': 'Why is defining functions for regression useful?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `prepare_df()` function in the context of regression prepares three dataframes and corresponding three y_vectors for the regression task. It also allows for filling missing values using the `fill_na_type` parameter before splitting the initial dataframe (`initial_df`). The function aims to streamline the development process, making it faster and more efficient.\\n\\n',\n",
       "  'answer_orig': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'document': '13702957',\n",
       "  'question': 'What does prepare_df() function do in regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To handle `fillna()` effectively in regression, you should perform the filling of missing values **before splitting** your initial dataframe into training, validation, and test sets. This approach ensures that the same method is consistently applied across all data splits, and it speeds up development. \\n\\nYou can use a function like `prepare_df(initial_df, seed, fill_na_type)` that prepares all three dataframes and y_vectors, and performs `fillna()` as part of the preprocessing. This is crucial to avoid any data leakage, which occurs when information from outside the training dataset is used to create the model, adversely affecting the model’s ability to generalize to new data.',\n",
       "  'answer_orig': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'document': '13702957',\n",
       "  'question': 'How can fillna() be handled effectively in regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In addition to the Scikit-Learn functions for Linear Regression and Ridge Regression (`sklearn.linear_model.LinearRegression` and `sklearn.linear_model.Ridge` respectively), you can also reuse the following helper functions for regression tasks, as per the class notebook:\\n\\n1. `rmse()`: This function can be used to calculate the root mean square error, which is a common metric for evaluating the performance of regression models.\\n2. `train_linear_regression(X, y, r)`: This function can be used to train a linear regression model on the provided dataset.\\n\\nThese functions help in speeding up the development process by providing reusable components for common tasks in regression analysis.',\n",
       "  'answer_orig': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'document': '13702957',\n",
       "  'question': 'What other functions can be reused in regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The advice on function usage in regression was provided by Ivan Brigida.',\n",
       "  'answer_orig': 'As it speeds up the development:\\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\\n(Ivan Brigida)',\n",
       "  'document': '13702957',\n",
       "  'question': 'Who provided the advice on function usage in regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To find the standard deviation using pandas in the context of regression, you can use the built-in `std()` function provided by pandas. Here are some examples:\\n\\n1. **For a single column in a DataFrame:**\\n   ```python\\n   df['column_name'].std()\\n   ```\\n   This will give you the standard deviation of the specified column.\\n\\n2. **For multiple columns in a DataFrame:**\\n   ```python\\n   df[['column_1', 'column_2']].std()\\n   ```\\n   This will provide the standard deviation for each of the selected columns.\\n\\n3. **For a list or series of data:**\\n   ```python\\n   import pandas as pd\\n   x = [1, 2, 3, 4, 5]\\n   pd.Series(x).std()\\n   ```\\n   This example demonstrates how to find the standard deviation of a list by converting it into a pandas Series and then calling the `std()` function on it.\\n\\nPandas computes the standard deviation using the sample standard deviation by default, which is an unbiased estimator. If you need the population standard deviation instead, consider using NumPy with an adjustment to the degrees of freedom parameter.\",\n",
       "  'answer_orig': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'document': '7cd652c5',\n",
       "  'question': 'How do I find the standard deviation using pandas in the context of regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The method to calculate the standard deviation of a series in pandas is `std()`. For example, you can use `pandas.Series(x).std()` to get the standard deviation of a list or series, where `x` is your data.\\n\\nYou can also use this method on DataFrame columns like so:\\n```python\\ndf['column_name'].std()\\n```\\nto get the standard deviation of a specific column. Additionally, you can calculate the standard deviation for multiple columns using:\\n```python\\ndf[['column_1', 'column_2']].std()\\n```.\\n\",\n",
       "  'answer_orig': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'document': '7cd652c5',\n",
       "  'question': 'What is the pandas method to calculate the standard deviation of a series?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Sure, I can help illustrate using pandas to compute the standard deviation with a sample list! Here’s an example using a list of values:\\n\\n1. First, you need to create a list of values.\\n2. Then, convert this list into a pandas Series.\\n3. Finally, use the `std()` function to compute the standard deviation of the series.\\n\\nHere’s how you can do it:\\n\\n```python\\nimport pandas as pd\\n\\n# Step 1: Create a list of values\\nx = [1, 2, 3, 4, 5]\\n\\n# Step 2: Convert the list into a pandas Series\\nseries_x = pd.Series(x)\\n\\n# Step 3: Compute the standard deviation using the std() function\\nstd_dev = series_x.std()\\n\\nprint(\"Standard Deviation:\", std_dev)\\n```\\n\\nRunning this code will give you the standard deviation of the list `[1, 2, 3, 4, 5]`.',\n",
       "  'answer_orig': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'document': '7cd652c5',\n",
       "  'question': 'Can you illustrate using pandas to compute the standard deviation with a sample list?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find the standard deviation of a list using pandas, follow these steps:\\n\\n1. **Import the pandas library:**\\n   Ensure you have the pandas library imported in your environment.\\n   ```python\\n   import pandas as pd\\n   ```\\n\\n2. **Convert the list to a pandas Series:**\\n   Given a list, convert it into a pandas Series. For example, for the list `x = [1, 2, 3, 4, 5]`, you would do:\\n   ```python\\n   x = [1, 2, 3, 4, 5]\\n   series_x = pd.Series(x)\\n   ```\\n\\n3. **Call the `.std()` method:**\\n   Use the `.std()` method on the pandas Series object to calculate the standard deviation.\\n   ```python\\n   standard_deviation = series_x.std()\\n   ```\\n\\nHere\\'s the entire process in one script:\\n```python\\nimport pandas as pd\\n\\n# Step 1: Create a list\\nx = [1, 2, 3, 4, 5]\\n\\n# Step 2: Convert the list to a pandas Series\\nseries_x = pd.Series(x)\\n\\n# Step 3: Calculate the standard deviation\\nstandard_deviation = series_x.std()\\n\\nprint(\"Standard Deviation:\", standard_deviation)\\n```\\nThis will give you the standard deviation of the values in the list.',\n",
       "  'answer_orig': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'document': '7cd652c5',\n",
       "  'question': 'What steps are involved in finding the standard deviation of a list using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The pandas function that directly computes the standard deviation from a series is `std()`. For instance, if you have a series `x`, you can compute its standard deviation using `pandas.Series(x).std()`.',\n",
       "  'answer_orig': 'If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\\n(Quinn Avila)',\n",
       "  'document': '7cd652c5',\n",
       "  'question': 'Which pandas function directly computes the standard deviation from a series?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The reason for the differences in standard deviation between Numpy and Pandas is that they use different default equations to compute it. Numpy calculates the population standard deviation by default, while Pandas calculates the sample standard deviation by default. You can change Numpy's standard deviation calculation to match Pandas' by setting the `ddof` (Delta Degrees of Freedom) parameter to 1, which will make Numpy use the unbiased estimator similar to Pandas. For example:\\n\\n```python\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\n```\\n\\nThis changes the degree of freedom in Numpy to 1, making the standard deviation calculation similar to Pandas'.\",\n",
       "  'answer_orig': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'document': 'e1f93d10',\n",
       "  'question': 'What is the reason for differences in standard deviation between Numpy and Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Numpy uses the equation for **population standard deviation** to compute standard deviation.',\n",
       "  'answer_orig': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'document': 'e1f93d10',\n",
       "  'question': 'Which equation does Numpy use to compute standard deviation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The default standard deviation in Pandas is computed using the sample standard deviation method, which employs one degree of freedom. This means that by default, it uses \\\\( n-1 \\\\) in its calculation, where \\\\( n \\\\) is the number of observations in your dataset. You can adjust the degree of freedom in Pandas by using the `ddof` parameter if needed.',\n",
       "  'answer_orig': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'document': 'e1f93d10',\n",
       "  'question': 'How is the default standard deviation in Pandas computed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To change Numpy to use an unbiased estimator for standard deviation, you can set the `ddof` parameter to 1 in the `np.std` function. Here is an example:\\n\\n```python\\nimport numpy as np\\nstd_unbiased = np.std(df.weight, ddof=1)\\n```\\n\\nThis sets the degrees of freedom (`ddof`) to 1, which changes the computation to an unbiased estimator, similar to how Pandas calculates the sample standard deviation.',\n",
       "  'answer_orig': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'document': 'e1f93d10',\n",
       "  'question': 'How can you change Numpy to use an unbiased estimator for standard deviation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To match Pandas' default standard deviation computation using Numpy, you should use the `ddof` parameter in the `np.std` function and set it to 1. Here's the command you would use:\\n\\n```python\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\n```\",\n",
       "  'answer_orig': 'Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\\nNumpy\\nPandas\\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\\nimport numpy as np\\nnp.std(df.weight, ddof=1)\\nThe result will be similar if we change the dof = 1 in numpy\\n(Harish Balasundaram)',\n",
       "  'document': 'e1f93d10',\n",
       "  'question': 'What command would you use in Numpy to match Pandas’ default standard deviation computation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To calculate the standard deviation of a single column in pandas, you can use the built-in `std()` function. Here is an example:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming you have a dataframe `df` and you want to calculate the standard deviation of a column named 'column_name'\\nstandard_deviation = df['column_name'].std()\\n```\\n\\nThis will give you the standard deviation of the specified column.\",\n",
       "  'answer_orig': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'document': '36b9d1b7',\n",
       "  'question': 'How do I calculate the standard deviation of a single column in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The pandas function used to find the standard deviation is `std()`. You can call this function directly on a pandas Series or DataFrame column. For example:\\n\\n- For a single column: `df['column_name'].std()`\\n- For multiple columns: `df[['column_1', 'column_2']].std()` \\n\\nYou can also apply it to a pandas Series created from a list, e.g., `pandas.Series([1,2,3,4,5]).std()`.\",\n",
       "  'answer_orig': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'document': '36b9d1b7',\n",
       "  'question': 'What pandas function is used to find the standard deviation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can calculate the standard deviation for multiple columns simultaneously using pandas. You can achieve this by using the built-in pandas function `std()`. For example, you can use the following code to get the standard deviation of multiple columns:\\n\\n```python\\ndf[['column_1', 'column_2']].std()\\n```\",\n",
       "  'answer_orig': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'document': '36b9d1b7',\n",
       "  'question': 'Can I calculate the standard deviation for multiple columns simultaneously using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To find the standard deviation of 'column_1' and 'column_2' in pandas, you can use the built-in `std()` function as follows:\\n\\n```python\\ndf[['column_1', 'column_2']].std()\\n```\",\n",
       "  'answer_orig': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'document': '36b9d1b7',\n",
       "  'question': \"What is the syntax to find the standard deviation of 'column_1' and 'column_2' in pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In pandas, `df['column_name'].std()` returns the standard deviation of the specified column in the DataFrame. This is computed as the measure of the amount of variation or dispersion of a set of values.\\n\\nFor example:\\n```python\\ndf['column_name'].std()\\n```\\nThis code will give you the standard deviation of the data in the column named `column_name`.\",\n",
       "  'answer_orig': \"In pandas you can use built in Pandas function names std() to get standard deviation. For example\\ndf['column_name'].std() to get standard deviation of that column.\\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\\n(Khurram Majeed)\",\n",
       "  'document': '36b9d1b7',\n",
       "  'question': \"What does df['column_name'].std() return in pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To combine train and validation dataframes, you should use the `pandas.concat` function.\\n\\nFrom the provided context:\\n\\n\"Use ‘[pandas.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)’ function to combine two dataframes.\"\\n\\nExample code:\\n```python\\ndf_train_combined = pd.concat([df_train, df_val])\\n```',\n",
       "  'answer_orig': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'document': '3c8b32a1',\n",
       "  'question': 'What library function is used to combine train and validation dataframes?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To combine two numpy arrays, you should use the `numpy.concatenate` function. Here's a brief example of how you might use it:\\n\\n```python\\nimport numpy as np\\n\\n# Example numpy arrays\\narray1 = np.array([1, 2, 3])\\narray2 = np.array([4, 5, 6])\\n\\n# Combining the arrays\\ncombined_array = np.concatenate((array1, array2), axis=0)\\n```\\n\\nThis will give you a combined numpy array. For more information, you can refer to the [numpy.concatenate documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html).\",\n",
       "  'answer_orig': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'document': '3c8b32a1',\n",
       "  'question': 'What is the function to combine two numpy arrays?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can merge two pandas dataframes using the `pandas.concat` function. Here is an example of how to do it:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming df_train and df_val are your two dataframes\\ndf_train_combined = pd.concat([df_train, df_val])\\n```\\n\\nFor more details, you can refer to the official pandas documentation: [pandas.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html).',\n",
       "  'answer_orig': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'document': '3c8b32a1',\n",
       "  'question': 'How can I merge two pandas dataframes?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To combine `y_train` and `y_val` in numpy, you should use the `numpy.concatenate` function. The code would be:\\n\\n```python\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n```\\n\\nThis will concatenate the two arrays along the specified axis (in this case, the first axis).',\n",
       "  'answer_orig': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'document': '3c8b32a1',\n",
       "  'question': 'What method should I use to combine y_train and y_val in numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the documentation for `pandas.concat` at the following link: [pandas.concat documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html).',\n",
       "  'answer_orig': 'Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\\ndf_train_combined = pd.concat([df_train, df_val])\\ny_train = np.concatenate((y_train, y_val), axis=0)\\n(George Chizhmak)',\n",
       "  'document': '3c8b32a1',\n",
       "  'question': 'Where can I find the documentation for pandas.concat?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The Root Mean Squared Error (RMSE) is a key metric for evaluating the performance of a regression model. It measures the average deviation between the model\\'s predicted values and the actual observed values. RMSE provides an indication of how accurately the model can predict the target variable, with lower values indicating better performance. \\n\\nTo calculate RMSE:\\n```python\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\n\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```\\n\\nAlternatively, you can directly obtain RMSE using the `mean_squared_error` function from `sklearn` by setting the `squared` parameter to `False`:\\n```python\\nfrom sklearn.metrics import mean_squared_error\\n\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\n```\\n\\nThese methods help in determining the effectiveness of the regression model by comparing the predicted outputs to the actual data points.',\n",
       "  'answer_orig': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'document': '05fb3a16',\n",
       "  'question': 'What is RMSE, and why is it used in regression model evaluation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To calculate the RMSE (Root Mean Squared Error) score, you can follow these steps using Python libraries such as `numpy` and `sklearn`:\\n\\n1. Import the necessary libraries:\\n   ```python\\n   import numpy as np\\n   from sklearn.metrics import mean_squared_error\\n   ```\\n\\n2. Calculate the Mean Squared Error (MSE):\\n   ```python\\n   mse = mean_squared_error(actual_values, predicted_values)\\n   ```\\n\\n3. Compute the RMSE by taking the square root of the MSE:\\n   ```python\\n   rmse = np.sqrt(mse)\\n   ```\\n\\n4. Print the RMSE score:\\n   ```python\\n   print(\"Root Mean Squared Error (RMSE):\", rmse)\\n   ```\\n\\nAlternatively, you can calculate the RMSE directly using `sklearn`\\'s `mean_squared_error` function with the `squared` parameter set to `False`:\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nrmse = mean_squared_error(actual_values, predicted_values, squared=False)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```',\n",
       "  'answer_orig': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'document': '05fb3a16',\n",
       "  'question': 'How does one calculate the RMSE score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To compute RMSE (Root Mean Squared Error), you need the following libraries:\\n\\n- `numpy`: for mathematical operations, specifically for computing the square root of the Mean Squared Error (MSE).\\n- `sklearn.metrics`: for the `mean_squared_error` function, which can compute the MSE directly and the RMSE if specified.\\n\\nHere’s the code to calculate RMSE using these libraries:\\n\\n```python\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\n\\n# Example usage\\nactual_values = [1, 2, 3]\\npredicted_values = [1.1, 1.9, 3.2]\\n\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\n\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```\\n\\nAlternatively, you can also obtain RMSE directly using:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\n\\n# Example usage\\nactual_values = [1, 2, 3]\\npredicted_values = [1.1, 1.9, 3.2]\\n\\nrmse = mean_squared_error(actual_values, predicted_values, squared=False)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```',\n",
       "  'answer_orig': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'document': '05fb3a16',\n",
       "  'question': 'Which libraries are necessary for computing RMSE?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To calculate the Root Mean Squared Error (RMSE) using Python, you can follow these steps:\\n\\n1. **Import the necessary libraries**:\\n   ```python\\n   import numpy as np\\n   from sklearn.metrics import mean_squared_error\\n   ```\\n\\n2. **Compute the Mean Squared Error (MSE)**:\\n   ```python\\n   mse = mean_squared_error(actual_values, predicted_values)\\n   ```\\n\\n3. **Calculate the RMSE**:\\n   ```python\\n   rmse = np.sqrt(mse)\\n   ```\\n\\n4. **Print the RMSE**:\\n   ```python\\n   print(\"Root Mean Squared Error (RMSE):\", rmse)\\n   ```\\n\\nAlternatively, you can directly compute the RMSE using the `mean_squared_error` function from `sklearn.metrics` with the `squared` parameter set to `False`:\\n\\n1. **Import the necessary library**:\\n   ```python\\n   from sklearn.metrics import mean_squared_error\\n   ```\\n\\n2. **Calculate the RMSE in one step**:\\n   ```python\\n   rmse = mean_squared_error(actual_values, predicted_values, squared=False)\\n   ```\\n\\n3. **Print the RMSE**:\\n   ```python\\n   print(\"Root Mean Squared Error (RMSE):\", rmse)\\n   ```\\n\\nThis concise method simplifies the process by directly providing the RMSE without the need for an additional step to compute the square root.',\n",
       "  'answer_orig': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'document': '05fb3a16',\n",
       "  'question': 'Can you detail the steps to calculate RMSE using Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"RMSE (Root Mean Squared Error) is an important metric for assessing the performance of a regression model because it calculates the average deviation between the model's predicted values and the actual observed values. This provides insight into how accurately the model can forecast the target variable. By measuring the square root of the average of squared differences between predictions and actual values, RMSE penalizes larger errors more significantly than smaller ones, making it a very effective measure of model performance.\",\n",
       "  'answer_orig': 'The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model\\'s predicted values and the actual observed values, offering insight into the model\\'s ability to accurately forecast the target variable. To calculate RMSE score:\\nLibraries needed\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n(Aminat Abolade)',\n",
       "  'document': '05fb3a16',\n",
       "  'question': 'Why is RMSE an important metric for assessing the performance of a regression model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The correct syntax for OR in Pandas is `|`. For example, if you want to filter a dataset based on multiple conditions using the OR logical operator, you would use:\\n\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\n\\nAdditionally, you can use the `isin` method as an alternative:\\n\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nFor the AND logical operator, you would use `&`.',\n",
       "  'answer_orig': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'document': '225506b9',\n",
       "  'question': 'What is the correct syntax for OR in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In Pandas, the logical AND operator is written as `&`. \\n\\nSo if you want to filter a dataset using multiple conditions combined with logical AND, you would use syntax like:\\n\\n```python\\nfiltered_df = df[(df[\"condition1\"] == value1) & (df[\"condition2\"] == value2)]\\n```\\n\\nMake sure to enclose each condition in parentheses for proper evaluation.',\n",
       "  'answer_orig': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'document': '225506b9',\n",
       "  'question': 'How do you write logical AND in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You might get an error when using multiple conditions in Pandas if you do not use the correct syntax for logical operators. For instance, the correct syntax for the logical OR operation is `|`, and for the logical AND operation, it is `&`. If you use any other symbols or incorrect syntax when combining multiple conditions, Pandas will throw an error.\\n\\nHere is the correct way to use multiple conditions:\\n\\n```python\\ndf = df[(df[\"column1\"] == \"value1\") | (df[\"column2\"] == \"value2\")]\\n```\\n\\nIn this example, `|` is used for the OR condition. Similarly, for an AND condition:\\n\\n```python\\ndf = df[(df[\"column1\"] == \"value1\") & (df[\"column2\"] == \"value2\")]\\n```\\n\\nUsing the incorrect symbols or syntax will result in errors when executing your code.',\n",
       "  'answer_orig': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'document': '225506b9',\n",
       "  'question': 'Why might you get an error when using multiple conditions in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about using multiple conditions in Pandas was provided by Olga Rudakova.',\n",
       "  'answer_orig': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'document': '225506b9',\n",
       "  'question': 'Who provided the information about using multiple conditions in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To apply multiple conditions in Pandas, you can use the logical operators `|` for OR and `&` for AND. Here is an example demonstrating the correct syntax:\\n\\nFor OR condition:\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\n\\nFor AND condition:\\n Suppose you want to filter rows where \\'ocean_proximity\\' is `\"<1H OCEAN\"` and another column \\'median_income\\' is greater than 50000:\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") & (df[\"median_income\"] > 50000)]\\n```\\n\\nRemember to always use parentheses `()` around each condition to ensure the correct order of operations.',\n",
       "  'answer_orig': 'If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\\n(Olga Rudakova)\\n–',\n",
       "  'document': '225506b9',\n",
       "  'question': 'Can you explain the syntax for applying multiple conditions in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a useful video for understanding the normal equation in linear regression in this section of the FAQ:\\n\\n**\"Machine Learning for Regression, Deep dive into normal equation for regression\"**\\n\\nHere is the link mentioned in the FAQ:\\n- [Normal Equation Derivation for Regression](#)\\n\\nThis video was found to be very useful for understanding how we arrive at the normal form with linear regression.',\n",
       "  'answer_orig': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'document': 'bd4a1395',\n",
       "  'question': 'Where can I find a useful video for understanding the normal equation in linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a recommended resource for a deeper understanding of the normal equation derivation. The video titled \"Normal Equation Derivation for Regression\" is suggested for understanding how we derive the normal form in linear regression.',\n",
       "  'answer_orig': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'document': 'bd4a1395',\n",
       "  'question': 'Is there a recommended resource for a deeper understanding of the normal equation derivation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To learn about the normal form in linear regression, you should watch the video titled \"Normal Equation Derivation for Regression.\" This video is recommended for understanding how the normal form is derived in the context of linear regression.',\n",
       "  'answer_orig': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'document': 'bd4a1395',\n",
       "  'question': 'What should I watch to learn about the normal form in linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'I recommend the video titled \"Normal Equation Derivation for Regression\" for understanding the normal form derivation for regression. This video provides a clear explanation of how the normal form is derived in the context of linear regression.',\n",
       "  'answer_orig': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'document': 'bd4a1395',\n",
       "  'question': 'Can you suggest a video that explains the normal form derivation for regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, I recommend the video titled \"Normal Equation Derivation for Regression\" for understanding the normal equation used in regression. You can find it useful in grasping how the normal form is derived with linear regression.',\n",
       "  'answer_orig': 'I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression',\n",
       "  'document': 'bd4a1395',\n",
       "  'question': 'Do you have any video recommendations for understanding the normal equation used in regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the learning in public for the midterm project, you need to make 14 posts, one for each day.',\n",
       "  'answer_orig': '(Hrithik Kumar Advani)',\n",
       "  'document': '81b8e8d0',\n",
       "  'question': 'question1',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The CONTEXT provided does not contain information directly related to \"question2\". To give you an accurate answer, could you provide the specific details or content of \"question2\"?',\n",
       "  'answer_orig': '(Hrithik Kumar Advani)',\n",
       "  'document': '81b8e8d0',\n",
       "  'question': 'question2',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The context provided does not include a specific \"question3.\" Without the actual text of the question, I cannot provide an accurate answer. Please provide the text of \"question3\" or check the available contexts again.',\n",
       "  'answer_orig': '(Hrithik Kumar Advani)',\n",
       "  'document': '81b8e8d0',\n",
       "  'question': 'question3',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The question \"question4\" relates to how many models should be trained, as explained in the Projects section for midterm deliverables.\\n\\nBased on the provided CONTEXT:\\n\\n**QUESTION: How many models should I train?**\\n\\n**ANSWER:** Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you\\'re on the right track.',\n",
       "  'answer_orig': '(Hrithik Kumar Advani)',\n",
       "  'document': '81b8e8d0',\n",
       "  'question': 'question4',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The CONTEXT provided does not include an explicit \"question5\" or specific details relevant to it. For a precise answer, please provide the exact wording or context of \"question5\" or clarify your query further.',\n",
       "  'answer_orig': '(Hrithik Kumar Advani)',\n",
       "  'document': '81b8e8d0',\n",
       "  'question': 'question5',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The instruction for applying log transformation to the 'median_house_value' variable can be found before Q3 in the Week-2 homework, under the “Prepare and split the dataset” heading.\",\n",
       "  'answer_orig': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'document': 'a7f6a33c',\n",
       "  'question': \"Where can I find the instruction for applying log transformation to 'median_house_value' in the Week-2 homework?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, the log transformation instruction is not present in all the questions of the Week-2 homework. It is provided before Question 3 under the 'Prepare and split the dataset' heading, but it is absent in the subsequent questions. Remember to apply the log transformation to the target variable for each question.\",\n",
       "  'answer_orig': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'document': 'a7f6a33c',\n",
       "  'question': 'Is the log transformation instruction present in all the questions of the Week-2 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Your RMSE became huge for Q5 in Week-2 homework because you likely forgot to apply the log transformation to the target variable, 'median_house_value'. The instruction to apply this transformation is given before Q3 under the 'Prepare and split the dataset' heading, but it's not explicitly reiterated in subsequent questions. Remember to apply the log transformation to the target variable in each question to avoid inflated RMSE values.\",\n",
       "  'answer_orig': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'document': 'a7f6a33c',\n",
       "  'question': 'Why did my RMSE become huge for Q5 in Week-2 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For each question in the Week-2 homework, you should remember to:\\n\\n1. **Submit your homework**: Use the provided link to submit your Week-2 homework: [Submit HW Week 2](https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform).\\n2. **Access the homework details**: Review and complete the homework assignments listed here: [Week 2 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md).\\n3. **Utilize resources**: Utilize resources like the GitHub repository for the course theory [GitHub for theory](https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp) and the YouTube playlist [Youtube Link: 2.X](https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12) for additional help.\\n4. **Ask questions**: If you have any questions, use the platform to ask questions for live sessions: [Ask questions for Live Sessions](https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions).\\n5. **Check the calendar**: Keep track of weekly meetings and important dates using the calendar link: [Calendar for weekly meetings](https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1).\\n\\nBy following these steps, you can ensure that you have properly completed and submitted your homework for Week-2.',\n",
       "  'answer_orig': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'document': 'a7f6a33c',\n",
       "  'question': 'What should I remember to do for each question in Week-2 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The note about applying log transformation to the target variable in the homework was added by **Soham Mundhada**.',\n",
       "  'answer_orig': 'The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\\n(Added by Soham Mundhada)',\n",
       "  'document': 'a7f6a33c',\n",
       "  'question': 'Who added the note about applying log transformation to the target variable in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Alexey is using version 0.24.2 of sklearn in his YouTube videos.',\n",
       "  'answer_orig': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'document': '129b4ac0',\n",
       "  'question': \"What version of sklearn is used in Alexey's videos?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Alexey uses Python version 3.8.11 in the YouTube course.',\n",
       "  'answer_orig': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'document': '129b4ac0',\n",
       "  'question': 'Which Python version does Alexey use in the YouTube course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The sklearn version mentioned in the course FAQ is 0.24.2.',\n",
       "  'answer_orig': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'document': '129b4ac0',\n",
       "  'question': 'Can you tell me the sklearn version mentioned in the course FAQ?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The required version of Python for the course is Python 3.10.',\n",
       "  'answer_orig': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'document': '129b4ac0',\n",
       "  'question': 'Which version of Python is required for the course based on the FAQ?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In the YouTube videos by Alexey, the versions of sklearn and Python referenced are sklearn version 0.24.2 and Python version 3.8.11.',\n",
       "  'answer_orig': 'Version 0.24.2 and Python 3.8.11\\n(Added by Diego Giraldo)',\n",
       "  'document': '129b4ac0',\n",
       "  'question': 'In the YouTube videos by Alexey, what versions of sklearn and Python are referenced?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the homework for Week 3 at the following link: \\n\\n[Week 3 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md)',\n",
       "  'answer_orig': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'document': 'b8cca8b7',\n",
       "  'question': 'Where can I find the homework for Week 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To submit your Week 3 homework, please use the following link: [Submit HW Week 3](https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform).',\n",
       "  'answer_orig': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'document': 'b8cca8b7',\n",
       "  'question': 'How do I submit my Week 3 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can see all the homework assignments by checking your cohort's specific folder. Each cohort may have different instructions or modules, so it is important to look in the designated folder for your cohort for the most accurate and current information.\",\n",
       "  'answer_orig': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'document': 'b8cca8b7',\n",
       "  'question': 'Where can I see all the homework assignments?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The provided context does not specifically mention an \"evaluation matrix\" being available. However, it does discuss evaluation metrics for model evaluation and how to compute them using scikit-learn. Here are the mentioned metrics:\\n\\n- Accuracy\\n- Precision\\n- Recall\\n- F1-Score\\n- ROC AUC\\n\\nThese metrics can be easily computed without creating an evaluation matrix from scratch. There is also a reference to using the Scikit Learn library for calculating a confusion matrix, which might be what you are looking for when referring to an evaluation matrix. \\n\\nTo get a more detailed classification report, you can use the Yellowbrick library, which combines scikit-learn with matplotlib to produce visualizations for models, including classification reports. \\n\\nSo while there isn\\'t a mention of a specific \"evaluation matrix,\" there are tools and libraries mentioned for evaluation metrics that might fulfill your needs.',\n",
       "  'answer_orig': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'document': 'b8cca8b7',\n",
       "  'question': 'Is there an evaluation matrix available?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The context provided does not contain specific information about the location or link to the theory GitHub repository. For details on where to find the theory GitHub repository, you may need to refer to the course materials or contact the course instructor directly.',\n",
       "  'answer_orig': 'Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n~~Nukta Bhatia~~',\n",
       "  'document': 'b8cca8b7',\n",
       "  'question': 'Where can I find the theory GitHub repository?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The error \"could not convert string to float: \\'Nissan\\'\" occurs when a machine learning model or function expects numerical input but receives a string instead. In this case, it indicates that the model is trying to convert the car brand \\'Nissan\\' into a numerical value, which isn\\'t possible because \\'Nissan\\' is a categorical text string.\\n\\nTo resolve this issue, you need to encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\n\\nHere’s an example of how you can perform one-hot encoding using pandas:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming \\'data\\' is your DataFrame and \\'brand\\' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=[\\'brand\\'])\\n```\\n\\nIn this code, `pd.get_dummies()` creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.',\n",
       "  'answer_orig': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'document': '1091b10f',\n",
       "  'question': \"What does the error 'could not convert string to float: Nissan' mean?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix the error related to converting a string to float in your dataset, you need to encode categorical variables (such as car brands) into numerical values since the machine learning model or function is expecting numerical input. Here's a common method called one-hot encoding, which you can perform using the `pandas` library in Python.\\n\\nHere's how you can do it:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\n```\\n\\nIn this example:\\n- The `pd.get_dummies()` function creates a new DataFrame where the `brand` column is replaced with binary columns for each car brand present in the original column.\\n- Each row in the DataFrame will have a 1 in the column that corresponds to its car brand and 0 in all other brand columns.\\n\\nThis transformation converts categorical data into a numerical format that can be used by machine learning models.\",\n",
       "  'answer_orig': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'document': '1091b10f',\n",
       "  'question': 'How can I fix the error related to converting a string to float in my dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One-hot encoding is a technique used in machine learning to convert categorical variables into a numerical format. Specifically, it creates new binary columns for each category present in the original data column. Each row in the DataFrame has a 1 in the column that corresponds to its category and 0 in all other columns. This method allows machine learning models to interpret categorical data that cannot be directly converted into numerical form. For instance, using pandas in Python, you can one-hot encode a DataFrame column with the `pd.get_dummies()` function.',\n",
       "  'answer_orig': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'document': '1091b10f',\n",
       "  'question': 'What is one-hot encoding in machine learning?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To apply one-hot encoding to a column with car brands, follow these steps using the pandas library in Python:\\n\\n1. **Import the pandas library**:\\n   ```python\\n   import pandas as pd\\n   ```\\n\\n2. **Assuming 'data' is your DataFrame and 'brand' is the column with car brands**, you can use `pd.get_dummies()` to create a new DataFrame where the 'brand' column is replaced with binary columns for each car brand.\\n   ```python\\n   # Example DataFrame\\n   data = pd.DataFrame({'brand': ['Nissan', 'Toyota', 'Honda', 'Toyota', 'Nissan']})\\n   \\n   # Apply one-hot encoding\\n   data_encoded = pd.get_dummies(data, columns=['brand'])\\n   ```\\n\\nThis will convert your DataFrame as follows:\\n- Original DataFrame:\\n  ```\\n  | brand  |\\n  |--------|\\n  | Nissan |\\n  | Toyota |\\n  | Honda  |\\n  | Toyota |\\n  | Nissan |\\n  ```\\n\\n- Encoded DataFrame:\\n  ```\\n  | brand_Honda | brand_Nissan | brand_Toyota |\\n  |-------------|--------------|--------------|\\n  |      0      |      1       |      0       |\\n  |      0      |      0       |      1       |\\n  |      1      |      0       |      0       |\\n  |      0      |      0       |      1       |\\n  |      0      |      1       |      0       |\\n  ```\\n\\nEach row in the encoded DataFrame has a 1 in the column corresponding to its brand and 0 in all other brand columns.\\n\\nThis method ensures that your machine learning model will receive numerical input instead of strings, resolving the error related to string-to-float conversion.\",\n",
       "  'answer_orig': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'document': '1091b10f',\n",
       "  'question': 'How do I apply one-hot encoding to a column with car brands?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Certainly! Here's an example of how you can use pandas for one-hot encoding in Python:\\n\\n```python\\nimport pandas as pd\\n\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata = pd.DataFrame({\\n    'brand': ['Toyota', 'Honda', 'Nissan', 'Toyota', 'Honda', 'Nissan'],\\n    'value': [1, 2, 3, 4, 5, 6]\\n})\\n\\n# Perform one-hot encoding\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\n\\nprint(data_encoded)\\n```\\n\\nIn this example code below:\\n- `data` is a DataFrame containing a column 'brand' with car brands and another column 'value'.\\n- `pd.get_dummies()` is used to perform one-hot encoding on the 'brand' column.\\n- This transformation creates a new DataFrame `data_encoded` where the 'brand' column is replaced with binary columns for each brand (e.g., 'brand_Honda', 'brand_Nissan', 'brand_Toyota').\\n- Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n\\nOutput would be:\\n\\n```\\n   value  brand_Honda  brand_Nissan  brand_Toyota\\n0      1            0             0             1\\n1      2            1             0             0\\n2      3            0             1             0\\n3      4            0             0             1\\n4      5            1             0             0\\n5      6            0             1             0\\n```\\n\\nThis is how you can use pandas to perform one-hot encoding.\",\n",
       "  'answer_orig': \"The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\\nHere’s an example of how you can perform one-hot encoding using pandas:\\nimport pandas as pd\\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\\ndata_encoded = pd.get_dummies(data, columns=['brand'])\\nIn this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\\n-Mohammad Emad Sharifi-\",\n",
       "  'document': '1091b10f',\n",
       "  'question': 'Can you provide an example of using pandas for one-hot encoding?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Mutual information score is used only for categorical or discrete variables because it measures the relationship between such types of variables. If the variables are continuous, the algorithm would need to divide these continuous variables into bins to calculate the mutual information score. This binning process is highly subjective and can vary significantly, making it less reliable. Therefore, continuous variables are typically converted to discrete formats (e.g., binary) to facilitate accurate calculation of mutual information scores.',\n",
       "  'answer_orig': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'document': '0c7715a1',\n",
       "  'question': 'Why is mutual information score used only for categorical or discrete variables?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The modification made to the median_house_value target in the homework was changing it to a binary format. Specifically, its values were converted to discrete values of either 0 or 1. This transformation was necessary because the median_house_value is a continuous variable, and the mutual information score is designed to calculate the relationship between categorical or discrete variables. Hence, to make the values discrete and avoid the subjectivity of binning continuous variables, the median_house_value target was modified to binary format.',\n",
       "  'answer_orig': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'document': '0c7715a1',\n",
       "  'question': 'What modification was made to the median_house_value target in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The mutual information score calculates the relationship between categorical or discrete variables. For continuous variables, mutual information requires the values to be divided into bins, which can be highly subjective. That's why mutual information score does not directly use continuous variables. Instead, continuous variables are often transformed into binary or categorical formats to facilitate this calculation.\",\n",
       "  'answer_orig': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'document': '0c7715a1',\n",
       "  'question': \"Why can't mutual information score directly use continuous variables?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Changing the `median_house_value` to binary format affects its values by converting them from a continuous range of numbers to discrete values of either 0 or 1. This transformation makes the target variable simpler and suitable for calculating the mutual information score, which measures the relationship between categorical or discrete variables. In binary format, the values no longer represent exact house prices but rather fall into one of two categories, allowing for more straightforward mutual information calculations without the need for subjective binning of continuous data.',\n",
       "  'answer_orig': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'document': '0c7715a1',\n",
       "  'question': 'How does changing the median_house_value to binary format affect its values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Allowing continuous variables for mutual information can be subjective because the algorithm needs to divide those continuous variables into discrete bins. The process of selecting how to bin these continuous variables inherently involves subjective decisions, such as the number of bins to use and the range of values for each bin. This subjectivity can significantly influence the mutual information score, thereby making the results less consistent and reliable. Hence, to maintain objectivity and consistency, continuous variables are typically converted into a binary or categorical format before calculating mutual information.',\n",
       "  'answer_orig': 'Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\\n—Odimegwu David—-',\n",
       "  'document': '0c7715a1',\n",
       "  'question': 'Why would allowing continuous variables for mutual information be subjective?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You should use **df_train** for the correlation matrix. As mentioned, the correlation matrix and any transformations on the data should only involve the training dataset (df_train) and not the full training set (df_train_full) which includes validation data. This avoids drawing any conclusions based on the validation data and ensures unbiased testing of your model's performance.\",\n",
       "  'answer_orig': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'document': 'd2043cf5',\n",
       "  'question': 'Should we use df_train or df_train_full for the correlation matrix?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, Q2 involves converting the median_house_value to binary. The context mentions that Q2 asks about the correlation matrix and converting median_house_value from numeric to binary.',\n",
       "  'answer_orig': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'document': 'd2043cf5',\n",
       "  'question': 'Does Q2 involve converting the median_house_value to binary?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, for Q2 about the correlation matrix, you are only dealing with `df_train`. The reason is that `df_train_full` contains both the training and validation datasets, and at this stage, we want to evaluate the training data only without incorporating the validation data.',\n",
       "  'answer_orig': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'document': 'd2043cf5',\n",
       "  'question': 'Are we only dealing with df_train for Q2 about the correlation matrix?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We don't include `df_train_full` for the correlation matrix task because `df_train_full` also contains the validation dataset. At this stage, we want to ensure that our conclusions and any derived metrics are based solely on the training data (`df_train`). Using only the training data helps avoid bias that might be introduced if validation data were included, which is important since we want to test our model's performance independently on the validation data after training.\",\n",
       "  'answer_orig': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'document': 'd2043cf5',\n",
       "  'question': \"Why don't we include df_train_full for the correlation matrix task?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'df_train_full includes the validation data.',\n",
       "  'answer_orig': \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
       "  'document': 'd2043cf5',\n",
       "  'question': 'Which dataset includes the validation data, df_train or df_train_full?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To color the background of a `pandas.DataFrame` based on the numerical values it contains, you can use the `pandas.io.formats.style.Styler.background_gradient` method. Here’s an example of how you can apply this to a DataFrame:\\n\\n```python\\n# Ensure to have the necessary imports\\nimport pandas as pd\\nimport numpy as np\\n\\n# Example DataFrame with numerical values\\nnp.random.seed(3)  # For reproducibility\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\n\\n# Apply background gradient\\nstyled_df = df_random.style.background_gradient(cmap='viridis')\\n\\n# Display the styled DataFrame\\nstyled_df\\n```\\n\\nIn this example, the DataFrame `df_random` contains random values. The `background_gradient` method is then used with the colormap 'viridis' to color the background based on the numerical values. You can replace 'viridis' with any other colormap of your choice.\",\n",
       "  'answer_orig': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'document': '44d22817',\n",
       "  'question': 'How can I color the background of a pandas.DataFrame based on numerical values it contains?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The method that allows coloring the correlation matrix in pandas is `pandas.io.formats.style.Styler.background_gradient`. Here is an example of how to use it:\\n\\n1. Ensure the dataframe contains only numerical values before calling `.corr()`.\\n2. Compute the correlation matrix using the `.corr()` method.\\n3. Apply `background_gradient` to the correlation matrix with a color map of your choice (e.g., 'viridis').\\n\\nExample code:\\n```python\\n# Ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\n```\\n\\nThis will color the background of the correlation matrix based on the numerical values it contains, using the 'viridis' color map.\",\n",
       "  'answer_orig': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'document': '44d22817',\n",
       "  'question': 'What method allows coloring the correlation matrix in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The color map used in the example provided to color the correlation matrix is 'viridis'.\",\n",
       "  'answer_orig': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'document': '44d22817',\n",
       "  'question': 'What color map is used in the example provided to color the correlation matrix?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, it is not strictly necessary to have only numerical values in the dataframe to call 'corr'; however, to get meaningful correlation results, you should ensure that the dataframe contains only numerical values before calling 'corr'. This is because correlation calculations only make sense for numerical data. Here is a brief explanation from the context provided:\\n\\n```plaintext\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\n```\\n\\nIn summary, while the 'corr' method can technically be called on any dataframe, for valid and useful correlation results, you should have a dataframe with only numerical values.\",\n",
       "  'answer_orig': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'document': '44d22817',\n",
       "  'question': \"Is it necessary to have only numerical values in the dataframe before calling 'corr'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Sure! Here's an example of how to color a dataframe containing random values using pandas:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\n# Set the random seed for reproducibility\\nnp.random.seed(3)\\n\\n# Create a dataframe with random values\\ndf_random = pd.DataFrame(data=np.random.random(3 * 3).reshape(3, 3))\\n\\n# Apply background gradient with the 'viridis' colormap\\ndf_random.style.background_gradient(cmap='viridis')\\n```\\n\\nThis code will create a dataframe with random values and apply a color gradient using the 'viridis' colormap to it.\",\n",
       "  'answer_orig': \"The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\\n# ensure to have only numerical values in the dataframe before calling 'corr'\\ncorr_mat = df_numerical_only.corr()\\ncorr_mat.style.background_gradient(cmap='viridis')\\nHere is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\\nnp.random.seed = 3\\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\ndf_random.style.background_gradient(cmap='viridis')\\nAdded by Sylvia Schmitt\",\n",
       "  'document': '44d22817',\n",
       "  'question': 'Can you provide an example of how to color a dataframe containing random values in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To identify highly correlated feature pairs, you can use the following methods:\\n\\n1. **Using Pandas and Correlation Matrix:**\\n   ```python\\n   data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\n   data_corr.head(10)\\n   ```\\n   This code snippet will calculate the correlation matrix of your numerical features, sort them, and display the highest correlations.\\n\\n2. **Using Seaborn to Create a Heatmap:**\\n   ```python\\n   sns.heatmap(df[numerical_features].corr(),\\n               annot=True,\\n               square=True,\\n               fmt=\".2g\",\\n               cmap=\"crest\")\\n   ```\\n   This code will generate a heatmap of the correlations among numerical features, which can help visually identify highly correlated pairs.\\n\\nThese methods allow you to easily spot the feature pairs with the highest degree of correlation.',\n",
       "  'answer_orig': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'document': '1f76dbeb',\n",
       "  'question': 'What method can be used to identify highly correlated feature pairs?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To visualize correlations using seaborn, you can create a heatmap. Here\\'s the code snippet for doing that:\\n\\n```python\\nimport seaborn as sns\\nimport pandas as pd\\n\\n# Load the dataset\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n\\n# Define numerical features if not defined\\nnumerical_features = df.select_dtypes(include=\\'number\\').columns\\n\\n# Create a heatmap of the correlation matrix\\nsns.heatmap(df[numerical_features].corr(), annot=True, square=True, fmt=\".2g\", cmap=\"crest\")\\n```\\n\\nThis heatmap will help you visualize the correlation between numerical features in your dataset. You can refine your heatmap to plot only a triangle and customize the color gradient to enhance readability.',\n",
       "  'answer_orig': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'document': '1f76dbeb',\n",
       "  'question': 'How can I visualize correlations using seaborn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To create a heatmap with seaborn, you can use the following commands:\\n\\n```python\\nimport seaborn as sns\\n\\n# Assuming `df` is your dataframe and `numerical_features` contains your numerical columns\\nsns.heatmap(df[numerical_features].corr(), \\n            annot=True, \\n            square=True, \\n            fmt=\".2g\", \\n            cmap=\"crest\")\\n```\\nThis will create a heatmap with annotations, square cells, formatted values, and a color map \"crest\".',\n",
       "  'answer_orig': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'document': '1f76dbeb',\n",
       "  'question': 'What are the commands for creating a heatmap with seaborn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To customize the seaborn heatmap to avoid redundant information, you can refine the heatmap to plot only a triangle. This approach eliminates the redundant display of the symmetric correlation values.\\n\\nHere is an example function you can use to create such a heatmap:\\n\\n```python\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef plot_triangle_heatmap(df, numerical_features):\\n    # Create a mask to display only the lower triangle of the heatmap\\n    mask = np.triu(np.ones_like(df[numerical_features].corr(), dtype=bool))\\n    \\n    # Set up the matplotlib figure\\n    f, ax = plt.subplots(figsize=(11, 9))\\n    \\n    # Generate a custom diverging colormap\\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\\n    \\n    # Draw the heatmap with the mask and correct aspect ratio\\n    sns.heatmap(df[numerical_features].corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt=\".2g\")\\n    plt.show()\\n\\n# Usage\\n# plot_triangle_heatmap(df, numerical_features)\\n```\\n\\nThis method will help you visualize the correlations between numerical variables without redundant information by displaying only the lower triangle of the correlation matrix.',\n",
       "  'answer_orig': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'document': '1f76dbeb',\n",
       "  'question': 'How can I customize the seaborn heatmap to avoid redundant information?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The heatmap function can be used in the following specific use cases based on the provided context:\\n\\n1. **Identifying Feature Correlations**: Heatmaps can visually represent the correlation matrix of numerical features in a dataset. This helps in easily identifying highly correlated feature pairs, which can be critical in understanding the relationships between variables and in feature selection.\\n   \\n   Example code snippet for creating a heatmap to show correlations:\\n   ```python\\n   sns.heatmap(df[numerical_features].corr(),\\n               annot=True,\\n               square=True,\\n               fmt=\".2g\",\\n               cmap=\"crest\")\\n   ```\\n\\n2. **Refining Visualization**: Heatmaps can be refined to show only a triangular section with a color gradient. This can prevent redundancy and make the visualization clearer by avoiding duplicate information.\\n   \\n   Example of a refined heatmap output (showing only a triangular section):\\n   ```python\\n   # Assume this code is part of a larger function and would produce a more refined output\\n   sns.heatmap(df[numerical_features].corr(),\\n               annot=True,\\n               square=True,\\n               fmt=\".2g\",\\n               cmap=\"crest\")\\n   ``` \\n\\nThese use cases enable data scientists and analysts to effectively visualize and interpret the correlations between multiple numerical variables within a dataset.',\n",
       "  'answer_orig': 'data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\ndata_corr.head(10)\\nAdded by Harish Balasundaram\\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\\nsns.heatmap(df[numerical_features].corr(),\\nannot=True,\\nsquare=True,\\nfmt=\".2g\",\\ncmap=\"crest\")\\nAdded by Cecile Guillot\\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\\nWhich outputs, in the case of churn dataset:\\n(Mélanie Fouesnard)',\n",
       "  'document': '1f76dbeb',\n",
       "  'question': 'What are some specific use cases for the heatmap function?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you should not use the test dataset for Exploratory Data Analysis (EDA) in this course. According to the course guidelines, it is recommended to only rely on the train dataset for EDA. Including the validation dataset might be permissible, but the test dataset should remain untouched to ensure it represents future unseen data.',\n",
       "  'answer_orig': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'document': 'b8071a54',\n",
       "  'question': 'Can we use the test dataset for EDA in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is acceptable to use only the train dataset for EDA. The provided context supports the practice of relying solely on the train dataset for exploratory data analysis (EDA). While including validation data might be okay, you should avoid using the test dataset for EDA to prevent data leakage and maintain the integrity of the testing process.',\n",
       "  'answer_orig': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'document': 'b8071a54',\n",
       "  'question': 'Is it acceptable to use only the train dataset for EDA?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"It's indeed good practice to only rely on the train dataset for Exploratory Data Analysis (EDA). Including the validation dataset might be okay, but you should definitely avoid using the test dataset. The test dataset should be kept as unseen data for future evaluation purposes.\",\n",
       "  'answer_orig': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'document': 'b8071a54',\n",
       "  'question': 'Should we include the validation dataset while performing EDA?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We shouldn't touch the test dataset during Exploratory Data Analysis (EDA) because it's essential to treat the test dataset as unseen future data. This ensures that our model evaluation is based on data that the model has not been exposed to during any part of the training or validation process. Looking at the test dataset, even briefly, can lead to inadvertent data leakage, which can bias the evaluation metrics and present an overly optimistic view of the model's performance on new data. This practice helps to maintain the integrity and effectiveness of the model evaluation process.\",\n",
       "  'answer_orig': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'document': 'b8071a54',\n",
       "  'question': \"Why shouldn't we touch the test dataset during EDA?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Pretending the test dataset is future unseen data means treating the test set as if it represents data that the model has never seen and will encounter in the future once it is deployed. This approach helps simulate a real-world scenario where the model receives new data and ensures its performance is evaluated on truly unseen information.',\n",
       "  'answer_orig': \"Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\\nAlena Kniazeva\",\n",
       "  'document': 'b8071a54',\n",
       "  'question': 'What does pretending the test dataset is future unseen data mean?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The purpose of a validation dataset in model training is to validate models and predictions on unseen data, which helps estimate the model's performance on fresh data. This process enables the optimization of the model.\",\n",
       "  'answer_orig': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'document': 'b8da9037',\n",
       "  'question': 'What is the purpose of a validation dataset in model training?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'We shouldn\\'t apply the fit method of DictVectorizer to the validation dataset because the fit method analyzes the data provided and adjusts itself according to this data. By applying fit to the validation dataset, we would be \"giving the answers\" to the system and it would no longer represent unseen data for validation purposes. This would essentially cause data leakage, where information from the validation set influences the model training, preventing accurate performance estimation on truly unseen data. The correct approach is to fit_transform the training set and only transform the validation and test sets to keep the validation process authentic and unbiased.',\n",
       "  'answer_orig': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'document': 'b8da9037',\n",
       "  'question': \"Why shouldn't we apply the fit method of DictVectorizer to the validation dataset?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The `fit` method of `DictVectorizer` handles categorical and numerical data as follows:\\n\\n- **Categorical data**: The `fit` method applies one-hot encoding to categorical data. This means that it converts each category into a binary vector where only one element is '1' (indicating the presence of that category), and the rest are '0'.\\n  \\n- **Numerical data**: For numerical data, the `fit` method leaves the values as they are without applying any transformation.\\n\\nOverall, when fitting the `DictVectorizer`, it maps the values of the input dictionaries, determining how to transform both categorical and numerical features for further use in machine learning models.\",\n",
       "  'answer_orig': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'document': 'b8da9037',\n",
       "  'question': 'How does the fit method of DictVectorizer handle categorical and numerical data?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The correct way to use `DictVectorizer` on train, validation, and test sets is as follows:\\n\\n1. **Fit and Transform the Train Set**: Use the `fit_transform` method on the train dataset. This will both fit the `DictVectorizer` to the data and transform the training data accordingly. This step determines the mapping and encoding for the features based on the train set.\\n\\n2. **Transform the Validation and Test Sets**: After fitting the `DictVectorizer` on the train set, use the `transform` method on the validation and test datasets. This ensures that the validation and test sets are transformed using the same feature mapping learned from the train set. This practice helps in avoiding data leakage and ensures that the validation and test sets remain unseen by the model until the evaluation phase.\\n\\nHere are the steps in code:\\n```python\\nfrom sklearn.feature_extraction import DictVectorizer\\n\\n# Assume df_train, df_val, and df_test are your datasets in dictionary format\\n\\n# Create an instance of DictVectorizer\\ndv = DictVectorizer()\\n\\n# Fit and transform the train set\\nX_train = dv.fit_transform(df_train)\\n\\n# Transform the validation and test sets using the already fitted DictVectorizer\\nX_val = dv.transform(df_val)\\nX_test = dv.transform(df_test)\\n```\\nThis approach avoids the risk of \"giving the answers\" to the model by only allowing the train set to influence the fit stage, and ensures consistency in transformation across all datasets.',\n",
       "  'answer_orig': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'document': 'b8da9037',\n",
       "  'question': 'What is the correct way to use DictVectorizer on train, validation, and test sets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"After fitting a DictVectorizer instance on the train set, it is unnecessary to initialize another instance because doing so would overwrite what it learned from being fit on the train data. The correct approach is to use the `fit_transform` method on the train set and then only use the `transform` method for the validation and test sets. This ensures that the transformation applied to the validation and test sets aligns with the transformation learned during training, allowing for accurate assessment of the model's performance without introducing data leakage.\",\n",
       "  'answer_orig': 'Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\\nEdidiong Esu\\nBelow is an extract of Alexey\\'s book explaining this point. Hope is useful\\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven\\'t seen. By not applying the fit to the validation model we can know how well it was trained.\\nBelow is an extract of Alexey\\'s book explaining this point.\\nHumberto Rodriguez\\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\\nMemoona Tahira',\n",
       "  'document': 'b8da9037',\n",
       "  'question': 'Why is it unnecessary to initialize another DictVectorizer instance after fitting it on the train set?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"For Q5 in the homework, the smallest accuracy difference should be calculated in absolute values. This means you consider the magnitude of the differences without regard to their sign. For example, between differences of -0.001 and -0.0002, you would compare 0.001 and 0.0002, and 0.0002 would be considered smaller.\\n\\nIf the difference is negative, it indicates that the model's performance improved when the feature was removed.\",\n",
       "  'answer_orig': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'document': '467e0cec',\n",
       "  'question': 'Should the smallest accuracy difference for Q5 in homework be calculated in absolute values or real values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, a negative difference in accuracy means the model improved after feature removal. This is because the difference in accuracy is calculated between the original model (with all features) and the model after removing one feature. If the resulting difference is negative, it indicates that the accuracy increased when the feature was removed, implying an improvement in the model's performance.\",\n",
       "  'answer_orig': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'document': '467e0cec',\n",
       "  'question': 'Does a negative difference in accuracy mean the model improved after feature removal?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For Q5, we should reason in absolute values when calculating the smallest difference. This means comparing the absolute values of the differences in accuracy to determine which is smallest, regardless of whether the differences are positive or negative.',\n",
       "  'answer_orig': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'document': '467e0cec',\n",
       "  'question': 'For Q5, are we supposed to reason in absolute values or lowest values when calculating the smallest difference?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"A negative accuracy difference when removing a feature in Question 5 (Q5) indicates that the model's performance has improved upon the removal of that feature. In other words, the model's accuracy has increased when the feature was removed, suggesting that the feature might have been detrimental to the model's performance.\",\n",
       "  'answer_orig': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'document': '467e0cec',\n",
       "  'question': 'When removing a feature in Q5, what does a negative accuracy difference indicate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In homework Q5, you should calculate the smallest difference in accuracy using absolute values.',\n",
       "  'answer_orig': 'For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\\nIf the difference is negative, it means that the model actually became better when we removed the feature.',\n",
       "  'document': '467e0cec',\n",
       "  'question': 'In homework Q5, what metric should we use for the smallest accuracy difference, absolute or real values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When `get_feature_names` is deprecated in machine learning for classification, you should use the `.get_feature_names_out()` method from the `DictVectorizer` function. This change is recommended to resolve the deprecation warning. You can update your code as follows:\\n\\nOld Code:\\n```python\\ndv.get_feature_names()\\n```\\n\\nNew Code:\\n```python\\nlist(dv.get_feature_names_out())\\n```\\n\\nThis method will provide you with a list of feature names in the desired format. If you are using `DictVectorizer` for one-hot encoding or feature extraction, make sure to fit the predictor and response arrays before accessing the feature names.',\n",
       "  'answer_orig': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'document': 'b69f32f6',\n",
       "  'question': 'What should I do when get_feature_names is deprecated in machine learning for classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the `get_feature_names` function is going to be removed in a future release. Specifically, it is deprecated as of version 1.0 and will be fully removed in version 1.2. Instead, you should use the `get_feature_names_out` method.',\n",
       "  'answer_orig': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'document': 'b69f32f6',\n",
       "  'question': 'Is get_feature_names going to be removed in a future release?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To resolve the FutureWarning about `get_feature_names` in section 3, you should use the method `.get_feature_names_out()` from the `DictVectorizer` function. This will eliminate the warning:\\n\\n```python\\n# Instead of using dv.get_feature_names(), use:\\nfeature_names = dv.get_feature_names_out()\\n```\\n\\nYou can then convert this to a list if needed:\\n\\n```python\\nfeature_names_list = list(dv.get_feature_names_out())\\n```\\n\\nThis approach should ensure that the warning is resolved. However, it is important to note that the warning can be ignored as it does not affect the functionality.',\n",
       "  'answer_orig': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'document': 'b69f32f6',\n",
       "  'question': 'How can I resolve the FutureWarning about get_feature_names in section 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To replace the `get_feature_names` method in `DictVectorizer`, you should use the method `get_feature_names_out()`. This method serves the same purpose but is updated to align with newer versions of the library. Here is how you can use it:\\n\\n```python\\ndv.get_feature_names_out()  # This will return a numpy array of feature names\\n```\\n\\nIf you need the result as a standard list of strings, you can convert the numpy array to a list:\\n\\n```python\\nlist(dv.get_feature_names_out())  # Converts the numpy array to a list of strings\\n```\\n\\nMake sure that you have already fitted the `DictVectorizer` instance with the predictor and response arrays to access the feature names.\\n\\nReference: Quinn Avila, Santhosh Kumar, Ibai Irastorza, George Chizhmak.',\n",
       "  'answer_orig': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'document': 'b69f32f6',\n",
       "  'question': 'What method can replace get_feature_names in DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, you don't need to be concerned about the FutureWarning for `get_feature_names`. Instead of using the deprecated `get_feature_names` method, you should use `get_feature_names_out()` from the `DictVectorizer` function. This will resolve the warning. Additionally, the warning itself does not indicate an immediate issue that requires urgent attention, so there's no need to worry.\",\n",
       "  'answer_orig': \"Instead use the method “.get_feature_names_out()” from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\\nSanthosh Kumar\",\n",
       "  'document': 'b69f32f6',\n",
       "  'question': 'Do I need to be concerned about the FutureWarning for get_feature_names?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Fitting logistic regression in Jupyter takes a long time or might cause the kernel to crash because the logistic regression model is likely not being provided with a binary target variable. Ensuring that the target variable is binary is crucial for the model to work efficiently.',\n",
       "  'answer_orig': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'document': '3b3b1989',\n",
       "  'question': 'Why does fitting logistic regression take a long time in Jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your logistic regression model crashes during prediction, you should make sure that the target variable for the logistic regression is binary.',\n",
       "  'answer_orig': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'document': '3b3b1989',\n",
       "  'question': 'What should I check if my logistic regression model crashes during prediction?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Your logistic regression model might be crashing in Jupyter because the target variable for the logistic regression is not binary. Make sure that the target variable is binary to avoid this issue.',\n",
       "  'answer_orig': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'document': '3b3b1989',\n",
       "  'question': 'What could be causing my logistic regression model to crash in Jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To prevent logistic regression from crashing Jupyter, make sure that the target variable for the logistic regression is binary. This is a common issue that can cause the fitting process to take a long time or even crash the kernel when calling `predict()` with the fitted model.\\n\\nKonrad Muehlberg',\n",
       "  'answer_orig': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'document': '3b3b1989',\n",
       "  'question': 'How can I prevent logistic regression from crashing Jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the target variable in logistic regression, a necessary condition is that it must be binary.\\n\\nThis is based on the FAQ section on logistic regression where it mentions:\\n\"Make sure that the target variable for the logistic regression is binary.\"',\n",
       "  'answer_orig': 'Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\\nMake sure that the target variable for the logistic regression is binary.\\nKonrad Muehlberg',\n",
       "  'document': '3b3b1989',\n",
       "  'question': 'What is a necessary condition for the target variable in logistic regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Ridge regression is used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and to prevent overfitting in predictive modeling. It does this by adding a regularization term to the linear regression cost function, which penalizes large coefficients.',\n",
       "  'answer_orig': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'document': 'eb5771a0',\n",
       "  'question': 'What is Ridge regression used for?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The sag solver in Ridge regression stands for \"Stochastic Average Gradient.\" It is optimized for large datasets by using stochastic gradient descent (SGD) to optimize the regularization term, potentially making it faster than other solvers.',\n",
       "  'answer_orig': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'document': 'eb5771a0',\n",
       "  'question': 'What does the sag solver stand for in Ridge regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The sag solver, which stands for \"Stochastic Average Gradient,\" is particularly suitable for large datasets because it optimizes the regularization term using stochastic gradient descent (SGD). This allows the sag solver to be faster than some other solvers when working with large datasets, making it efficient and practical for such use cases.',\n",
       "  'answer_orig': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'document': 'eb5771a0',\n",
       "  'question': 'Why is the sag solver suitable for large datasets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The alpha parameter controls the strength of regularization in Ridge regression. When you increase the alpha value, it leads to stronger regularization. This means the model's coefficients will become smaller, which helps in reducing the risk of overfitting. Conversely, a lower alpha value results in weaker regularization, allowing the model to potentially fit more closely to the training data, but potentially increasing the risk of overfitting.\",\n",
       "  'answer_orig': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'document': 'eb5771a0',\n",
       "  'question': 'How does the alpha parameter affect Ridge regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To initialize and train a Ridge regression model using scikit-learn, follow these steps:\\n\\n1. **Import the Ridge regression class:**\\n    ```python\\n    from sklearn.linear_model import Ridge\\n    ```\\n\\n2. **Prepare your data:**\\n   - Ensure that all features used in the Ridge regression model are numerical.\\n\\n3. **Preprocess your data:**\\n   - **Normalize numerical features:** Use techniques like `MinMaxScaler` or `StandardScaler` to normalize your numeric features to help with convergence.\\n   - **Encode categorical features:** If your dataset includes categorical features, convert them into a numerical format using techniques like `OneHotEncoder`. For example:\\n     ```python\\n     from sklearn.preprocessing import OneHotEncoder, StandardScaler\\n     from sklearn.compose import ColumnTransformer\\n     from sklearn.pipeline import Pipeline\\n     import numpy as np\\n\\n     # Assuming X_train is your input feature matrix which includes both numerical and categorical data\\n     numerical_features = ['num_feature1', 'num_feature2']\\n     categorical_features = ['cat_feature1', 'cat_feature2']\\n\\n     preprocessor = ColumnTransformer(\\n         transformers=[\\n             ('num', StandardScaler(), numerical_features),\\n             ('cat', OneHotEncoder(sparse=True), categorical_features)])\\n\\n     X_train_preprocessed = preprocessor.fit_transform(X_train)\\n     ```\\n\\n4. **Combine the preprocessed features:**\\n   - Make sure to combine the normalized numerical features and the encoded categorical features to form a single feature matrix:\\n     ```python\\n     X_train_combined = np.hstack([X_train_numerical_normalized, X_train_categorical_encoded])\\n     ```\\n\\n5. **Initialize the Ridge regression model:**\\n    ```python\\n    alpha_value = 1.0  # You can adjust this value.\\n    ridge = Ridge(alpha=alpha_value, solver='sag', random_state=42)\\n    ```\\n\\n6. **Train the Ridge regression model:**\\n    ```python\\n    ridge.fit(X_train_preprocessed, y_train)\\n    ```\\n\\nBy following these steps, you will have successfully initialized and trained a Ridge regression model using scikit-learn with appropriate feature preprocessing to avoid convergence issues.\",\n",
       "  'answer_orig': 'Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It\\'s particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\\nfrom sklearn.linear_model import Ridge\\nridge = Ridge(alpha=alpha, solver=\\'sag\\', random_state=42)\\nridge.fit(X_train, y_train)\\nAminat Abolade',\n",
       "  'document': 'eb5771a0',\n",
       "  'question': 'How do you initialize and train a Ridge regression model using scikit-learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, pandas.get_dummies() and DictVectorizer(sparse=False) do not produce the same type of one-hot encodings. Here are the main differences:\\n\\n1. **Format**: \\n   - `pandas.get_dummies()` produces a dense format by default.\\n   - `DictVectorizer(sparse=False)` also produces a dense matrix, but its default configuration is sparse (DictVectorizer(sparse=True)).\\n\\n2. **Efficiency and Performance**:\\n   - The default sparse format of `DictVectorizer` (sparse=True) is more memory-efficient as it stores only the non-zero values and their indices, which is beneficial for large datasets with many zero or missing values. This leads to better convergence during fitting and avoids high memory usage that comes with dense format.\\n   - Using dense formats with large amounts of classes (like in the case of car models) can be slower and lead to performance issues and convergence warnings, as observed in linear or ridge regression tasks.\\n\\nIn summary, while both can achieve one-hot encoding, the format they produce and their subsequent performance characteristics differ notably, especially with large feature sets.',\n",
       "  'answer_orig': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'document': 'bca10281',\n",
       "  'question': 'Do pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using `DictVectorizer(sparse=True)` is recommended over `pandas.get_dummies()` for high cardinality features for several reasons:\\n\\n1. **Memory Efficiency**: `DictVectorizer(sparse=True)` produces a Compressed Sparse Row (CSR) format which is more memory-efficient. It only stores the non-zero values and their indices, unlike `pandas.get_dummies()` which would create a new column for each possible value of the categorical feature.\\n\\n2. **Performance**: Models trained with data in sparse format converge better and do not generate convergence warnings as often. In contrast, using dense formats can be slower and might produce worse results, particularly for high-cardinality features.\\n\\n3. **Scalability**: The CSR format is better suited for handling large datasets with many unique values in categorical features, such as different models of cars, where using dense formats would result in an infeasibly large number of columns.\\n\\nThese factors combined make `DictVectorizer(sparse=True)` a better option for efficient and effective model training with high cardinality categorical features.',\n",
       "  'answer_orig': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'document': 'bca10281',\n",
       "  'question': 'Why is using DictVectorizer(sparse=True) recommended over pandas.get_dummies() for high cardinality features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'DictVectorizer(sparse=True) produces data in CSR (Compressed Sparse Row) format. This format is more memory-efficient and tends to converge better during the fitting process because it stores only the non-zero values and their indices, rather than adding a column for each class of each feature.',\n",
       "  'answer_orig': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'document': 'bca10281',\n",
       "  'question': 'What format does DictVectorizer(sparse=True) produce?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using \"sparse\" format with `pandas.get_dummies()` for tasks like Linear/Ridge Regression can cause several problems:\\n\\n1. **Slower Processing:** When dealing with a high number of classes (e.g., multiple car models producing over 900 columns), the use of non-sparse, i.e., dense, format is slower. For instance, it takes around 6-8 minutes for such tasks.\\n\\n2. **Worse Results:** The models tend to yield poorer results in Linear/Ridge Regression when using a sparse format with `pandas.get_dummies()` compared to when a more memory-efficient representation like CSR format is used.\\n\\n3. **Convergence Warnings:** There are convergence warnings observed during Linear/Ridge Regression, indicating that the optimization process is struggling to find a solution with the given representation of the data.\\n',\n",
       "  'answer_orig': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'document': 'bca10281',\n",
       "  'question': 'What problems arise when using sparse format with pandas.get_dummies() for tasks like Linear/Ridge Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The performance and accuracy of the sparse format for Logistic Regression are generally better compared to the dense format, especially when working with large datasets having many zero or missing values. Sparse matrices are more memory-efficient and make the training process more performant without causing errors that might occur with the dense format. However, for high amounts of classes, using a dense format can be slower and yield marginally worse results, along with convergence warnings during regression tasks.',\n",
       "  'answer_orig': 'DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\\nUsing “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\\nLarkin Andrii',\n",
       "  'document': 'bca10281',\n",
       "  'question': 'How does the performance and accuracy of sparse format compare to dense format for Logistic Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Convergence problems in W3Q6 when using Ridge with the sag solver are caused by the requirement that features must be of the same scale. If they are not properly scaled, you may encounter the warning: \"ConvergenceWarning: The max_iter was reached which means the coef_ did not converge.\" To address this, you can normalize your numerical features using techniques like StandardScaler and use OneHotEncoder for categorical features, then combine these scaled and encoded features before training the model.',\n",
       "  'answer_orig': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'document': '34a8edb0',\n",
       "  'question': 'What causes convergence problems in W3Q6 when using Ridge with sag solver?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If the features are not scaled properly in Ridge with the sag solver, you might encounter the following warning: \\n\\n\"ConvergenceWarning: The max_iter was reached which means the coef_ did not converge.\"',\n",
       "  'answer_orig': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'document': '34a8edb0',\n",
       "  'question': 'What warning might appear if the features are not scaled properly in Ridge with sag solver?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To address convergence issues in W3Q6 by experimenting with different scalers, you should consult the file **notebook-scaling-ohe.ipynb**. This notebook contains examples and techniques for scaling numerical features and encoding categorical features, which are crucial for resolving convergence problems in your Ridge regression model.\\n\\nMake sure to:\\n1. Use a **StandardScaler** for numerical features.\\n2. Apply **OneHotEncoder (sparse=False)** for categorical features.\\n3. Separate and normalize the numerical and categorical features appropriately before using them in your model.\\n\\nThis approach will help mitigate the convergence warnings you might be facing.',\n",
       "  'answer_orig': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'document': '34a8edb0',\n",
       "  'question': 'What file should I consult to play with different scalers for W3Q6 convergence issues?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To avoid convergence issues, you should use a **StandardScaler** for numeric fields and **OneHotEncoder (with sparse=False)** for categorical features. This approach helps to ensure that features are on a similar scale and properly encoded for the Ridge regression model.',\n",
       "  'answer_orig': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'document': '34a8edb0',\n",
       "  'question': 'Which scalers should be used for numeric and categorical fields to avoid convergence issues?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Before using an encoder for separating numeric and categorical features, the initial step is to identify and differentiate these features within your dataset. This ensures that the appropriate encoding technique can be applied to the categorical features. Once identified, you can then proceed with encoding techniques using tools like sklearn's OneHotEncoder or OrdinalEncoder to convert categorical data into numerical values.\",\n",
       "  'answer_orig': 'Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\\nPlay with different scalers. See notebook-scaling-ohe.ipynb\\nDmytro Durach\\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.',\n",
       "  'document': '34a8edb0',\n",
       "  'question': 'What initial step should be taken before using an encoder for separating numeric and categorical features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The recommended method to prevent convergence issues in Ridge regression during Week 3 involves several steps:\\n\\n1. **Feature Normalization**: Normalize your numerical features using scaling techniques like MinMaxScaler or StandardScaler. Normalizing ensures that numerical features are on a similar scale, which helps in preventing convergence issues.\\n\\n2. **Categorical Feature Encoding**: If your dataset includes categorical features, encode them into a numerical format using OneHotEncoder (OHE). OneHotEncoder is useful for converting categorical variables into binary vectors, making them compatible with Ridge regression models.\\n\\n3. **Combine Features**: After normalizing the numerical features and encoding the categorical features using OHE, combine both types of features to form a single feature matrix (\\\\(X_{\\\\text{train}}\\\\)). This combined dataset will serve as the input for training the Ridge regression model.\\n\\n4. **Proper Solver and Scalers**: Using a suitable solver like 'sag' and ensuring features are of the same scale can help. Playing with different scalers can also help resolve convergence warnings regarding max iterations not being reached.\\n\\nBy following these steps, you can address convergence errors and improve the stability of the Ridge regression model training process.\",\n",
       "  'answer_orig': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'document': 'f625307b',\n",
       "  'question': 'What is the recommended method to prevent convergence issues in Ridge regression during Week 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To handle numerical features when training a Ridge regression model, you should follow these steps:\\n\\n1. **Normalize Numerical Features**: Ensure that numerical features are on a similar scale to prevent convergence issues. Techniques like `MinMaxScaler` or `StandardScaler` can be used for normalization.\\n\\n2. **Encode Categorical Features**: If your dataset includes categorical features, transform them using one-hot encoding. This converts categorical variables into a numerical format, making them compatible with Ridge regression models. \\n\\n3. **Combine Features**: After normalizing numerical features and encoding categorical features, combine them to form a single feature matrix (`X_train`). \\n\\n4. **Additional Tips**:\\n    - When using one-hot encoding, set the parameter `sparse=True` to avoid errors of non-convergence.\\n    - For the Ridge regression with a `sag` solver, which optimizes using stochastic gradient descent, ensure that all features are on the same scale to prevent convergence warnings or errors.\\n\\nBy following these steps, you will prepare your features appropriately for training a Ridge regression model and address potential convergence issues.',\n",
       "  'answer_orig': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'document': 'f625307b',\n",
       "  'question': 'How should I handle numerical features when training a Ridge regression model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For encoding categorical features for Ridge regression, the suggested technique is to use **OneHotEncoder**. To ensure proper convergence, set the parameter `sparse=True` when applying OneHotEncoder. Additionally, ensure that all features are numerical by first transforming categorical features using OneHotEncoder, then combining them with normalized numerical features before proceeding with model training. Here is a detailed step-by-step recommendation:\\n\\n1. **Transform Categorical Features**: Use OneHotEncoder to transform categorical features into a numerical format.\\n2. **Normalize Numerical Features**: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler.\\n3. **Combine Features**: Combine the normalized numerical features and the encoded categorical features to form a single feature matrix (X_train).\\n4. **Model Training**: Use the combined feature matrix to train your Ridge regression model.\\n\\nThis approach helps in handling categorical data appropriately and ensuring that your Ridge regression model converges successfully.',\n",
       "  'answer_orig': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'document': 'f625307b',\n",
       "  'question': 'What technique is suggested for encoding categorical features for Ridge regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To combine numerical and categorical features before training a Ridge regression model, follow these steps:\\n\\n1. **Normalize Numerical Features:** Use normalization techniques such as MinMaxScaler or StandardScaler to ensure that all numerical features are on a similar scale. This step is crucial to prevent convergence issues during model training.\\n\\n2. **Encode Categorical Features:** Transform categorical features into a numerical format using one-hot encoding (OneHotEncoder). It's important to set `sparse=True` when performing one-hot encoding to avoid errors related to non-convergence.\\n\\n3. **Combine Normalized and Encoded Features:** After normalizing the numerical features and encoding the categorical features, combine them to form a single feature matrix (e.g., `X_train`). This combined dataset will serve as the input for training the Ridge regression model.\\n\\nBy following these steps, you can effectively combine numerical and categorical features and enhance the stability and performance of your Ridge regression model.\\n\\nHere’s a pseudo-code outline for these steps:\\n\\n```python\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import Ridge\\n\\n# Assume X_train includes both numerical and categorical features\\nnumerical_features = ['numerical_feature1', 'numerical_feature2']\\ncategorical_features = ['categorical_feature1', 'categorical_feature2']\\n\\n# Create transformations for numerical and categorical features\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('num', StandardScaler(), numerical_features),\\n        ('cat', OneHotEncoder(sparse=True), categorical_features)\\n    ])\\n\\n# Create the Ridge Regression model pipeline\\nridge_pipeline = Pipeline(steps=[\\n    ('preprocessor', preprocessor),\\n    ('classifier', Ridge(alpha=1.0, solver='sag'))\\n])\\n\\n# Fit the model\\nridge_pipeline.fit(X_train, y_train)\\n\\n# Predict using the trained model\\ny_pred = ridge_pipeline.predict(X_test)\\n```\\n\\nMake sure to adjust the feature lists and model parameters as per your specific dataset and requirements.\",\n",
       "  'answer_orig': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'document': 'f625307b',\n",
       "  'question': 'How can I combine numerical and categorical features before training the Ridge regression model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Using OneHotEncoder is appropriate for categorical features in Ridge regression because Ridge regression accepts only numerical values, and categorical features need to be transformed into a numerical format to be used in the model. One-hot encoding converts categorical features into binary vectors, which makes them compatible with Ridge regression. This transformation helps in combining both numerical and categorical features into a single feature matrix, thereby enhancing the model's stability and addressing convergence errors during the training process. Additionally, when dealing with convergence problems, it's recommended to set `sparse=True` in OneHotEncoder to avoid non-convergence issues.\",\n",
       "  'answer_orig': \"When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \\tsimilar scale, preventing convergence issues.\\nCategorical Feature Encoding: If your dataset includes categorical features, apply \\tcategorical encoding techniques such as OneHotEncoder (OHE) to \\tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\\nCombine Features: After \\tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\\nYou can find an example here.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali\",\n",
       "  'document': 'f625307b',\n",
       "  'question': 'Why is using OneHotEncoder appropriate for categorical features in Ridge regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using a **sparse matrix** instead of a dense matrix offers several benefits:\\n\\n1. **Memory Efficiency**: Sparse matrices only store non-zero values and their positions, which makes them significantly more memory-efficient. This is particularly useful with large datasets that contain many zero or missing values.\\n\\n2. **Performance Improvements**: Training a machine learning model with a sparse matrix can be more performant and faster compared to using a dense matrix. Dense matrices can lead to larger memory requirements and could potentially cause errors, whereas sparse matrices are more manageable in size.\\n\\n3. **Better Convergence**: Sparse matrices tend to converge better during model fitting processes. Models that use sparse matrices can avoid the pitfalls of dense matrices, such as excessive memory use and poor scaling with large numbers of classes or features.\\n\\nOverall, the use of sparse matrices can result in a more efficient and faster training process while also saving on memory usage.',\n",
       "  'answer_orig': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'document': '7fa98526',\n",
       "  'question': 'What are the benefits of using a sparse matrix instead of a dense matrix?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This efficiency is particularly important when working with large datasets that contain numerous zero or missing values.',\n",
       "  'answer_orig': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'document': '7fa98526',\n",
       "  'question': 'Why is a sparse matrix memory-efficient?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The default configuration of `DictVectorizer` involves converting categorical features into numerical variables via one-hot encoding, while leaving numerical features as they are. There is no explicit mention of any other specific default parameters or configurations in the provided context.',\n",
       "  'answer_orig': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'document': '7fa98526',\n",
       "  'question': 'What is the default configuration of DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using a sparse matrix was beneficial for Week 3 Question 6 because it was more memory-efficient and allowed the model to train more effectively without errors. Sparse matrices only store non-zero values and their positions in memory, making them ideal for handling large datasets with many zero or missing values. Specifically, the default DictVectorizer configuration, which produces a sparse matrix, helped manage the size of the matrix, improve training performance, and avoid errors that occurred when using a dense matrix.',\n",
       "  'answer_orig': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'document': '7fa98526',\n",
       "  'question': 'Why was using a sparse matrix beneficial for week3 Q6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using a sparse matrix for training in Week 3 Q6 avoided the issue of high memory consumption and performance degradation that comes with using a dense matrix. Sparse matrices are more memory-efficient because they only store non-zero values and their positions, which is crucial for large datasets with many zero or missing values. This made training the model more performant and avoided any error messages that would occur with a dense matrix.',\n",
       "  'answer_orig': 'A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\\n \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila',\n",
       "  'document': '7fa98526',\n",
       "  'question': 'What issue was avoided by using a sparse matrix for training in week3 Q6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To disable warnings in Jupyter Notebooks, you can use the following commands:\\n\\n```python\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\n```\\n\\nThis will suppress the warning messages in your notebook.',\n",
       "  'answer_orig': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'document': '0807f0f3',\n",
       "  'question': 'How can I disable warnings in Jupyter Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To avoid warnings in Jupyter notebooks, you can disable them by using the following comments:\\n\\n```python\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\n```\\n\\nThis method will suppress warning messages that can clutter your notebook, making it easier to focus on your primary output and code.',\n",
       "  'answer_orig': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'document': '0807f0f3',\n",
       "  'question': 'What is the process to avoid warnings in Jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution to disable warnings in Jupyter was provided by Krishna Anand.',\n",
       "  'answer_orig': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'document': '0807f0f3',\n",
       "  'question': 'Who provided the solution to disable warnings in Jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The section discussing disabling warnings in Jupyter Notebooks is **section 3. Machine Learning for Classification**.',\n",
       "  'answer_orig': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'document': '0807f0f3',\n",
       "  'question': 'Which section discusses disabling warnings in Jupyter Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The library used to filter warnings in Jupyter Notebooks is the `warnings` library. You can disable warnings with the following commands:\\n\\n```python\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\n```\\n\\nThis information is provided by Krishna Anand in the FAQ database.',\n",
       "  'answer_orig': 'The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\\nImport warnings\\nwarnings.filterwarnings(“ignore”)\\nKrishna Anand',\n",
       "  'document': '0807f0f3',\n",
       "  'question': 'What library is used to filter warnings in Jupyter Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To choose the alpha parameter in Q6, you need to find the RMSE (Root Mean Square Error) for each possible alpha value. If you encounter scenarios where the RMSE scores are equal for different alpha values, you should select the lowest alpha.',\n",
       "  'answer_orig': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'document': '6d0fb418',\n",
       "  'question': 'How do we choose the alpha parameter in Q6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If RMSE scores for different alphas are the same, you should select the lowest alpha.',\n",
       "  'answer_orig': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'document': '6d0fb418',\n",
       "  'question': 'What should we do if RMSE scores for different alphas are the same?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To determine the correct RMSE (Root Mean Squared Error) score, you can use one of the following methods using Python libraries like `numpy` and `sklearn`:\\n\\n1. **Using numpy and sklearn:**\\n   ```python\\n   import numpy as np\\n   from sklearn.metrics import mean_squared_error\\n\\n   mse = mean_squared_error(actual_values, predicted_values)\\n   rmse = np.sqrt(mse)\\n   print(\"Root Mean Squared Error (RMSE):\", rmse)\\n   ```\\n\\n2. **Using sklearn directly with squared argument set to False:**\\n   ```python\\n   from sklearn.metrics import mean_squared_error\\n\\n   rms = mean_squared_error(y_actual, y_predicted, squared=False)\\n   print(\"Root Mean Squared Error (RMSE):\", rms)\\n   ```\\n\\nThese methods calculate the RMSE by measuring the average deviation between the observed values (actual_values or y_actual) and the predicted values (predicted_values or y_predicted). The lower the RMSE score, the better the model\\'s predictive accuracy.',\n",
       "  'answer_orig': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'document': '6d0fb418',\n",
       "  'question': 'How do we determine the correct RMSE score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The criteria for selecting the lowest alpha is that if the RMSE (Root Mean Square Error) scores for multiple alpha values are equal, you will select the lowest alpha among them.',\n",
       "  'answer_orig': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'document': '6d0fb418',\n",
       "  'question': 'What is the criteria for selecting the lowest alpha?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The study group in week two's homework misunderstood the instruction regarding the application of log transformation to the ‘median_house_value’ variable. They missed applying the log transformation to the target variable for each question, which could result in significantly different RMSE values. The instruction for this transformation is provided before Q3 under the ‘Prepare and split the dataset’ heading, and it's crucial to remember this step throughout the homework.\",\n",
       "  'answer_orig': 'Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\\nAsia Saeed',\n",
       "  'document': '6d0fb418',\n",
       "  'question': \"What was misunderstood by the study group in week two's homework?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The second variable used to calculate the mutual information score in HW3 Q3 is **ocean_proximity**.',\n",
       "  'answer_orig': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'document': 'fbda1f40',\n",
       "  'question': 'What is the second variable used to calculate the mutual information score in HW3 Q3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To calculate the mutual information score in HW3 Q3, you need to use two variables: the binarized price variable (above_average) and the categorical variable ocean_proximity. This means you will be assessing the mutual information score between these two specific variables using the training set only.',\n",
       "  'answer_orig': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'document': 'fbda1f40',\n",
       "  'question': 'Can you specify the variables for calculating the mutual info score in HW3 Q3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The categorical variable that should be used with the binarized price for HW3 Q3 is **ocean_proximity**, as it is the only original categorical variable in the dataset.',\n",
       "  'answer_orig': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'document': 'fbda1f40',\n",
       "  'question': 'Which categorical variable should be used with binarized price for HW3 Q3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In HW3 Q3, you should use the categorical variable **ocean_proximity** alongside the binarized price (above_average) to calculate the mutual information score.',\n",
       "  'answer_orig': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'document': 'fbda1f40',\n",
       "  'question': 'In HW3 Q3, what should we use alongside the binarized price for mutual information?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The second variable needed to calculate the mutual information score in HW3 Q3 is \"ocean_proximity,\" the only original categorical variable in the dataset.',\n",
       "  'answer_orig': 'Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\\nAsia Saeed',\n",
       "  'document': 'fbda1f40',\n",
       "  'question': 'For the mutual info calculation in HW3 Q3, what is the second variable needed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Given the context provided, you should initially train the model with all available features. This process involves evaluating the model's accuracy with all features, then systematically removing one feature at a time and retraining the model each time to compare the accuracy. The goal is to identify which features have the smallest absolute impact on accuracy when removed, which helps in selecting the most predictive features for your final model.\\n\\nSo, to answer your question directly: **You need to start by training the model with all available features initially.**\",\n",
       "  'answer_orig': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'document': '0f88b7ac',\n",
       "  'question': 'Do we need to train the model with only total_rooms, total_bedrooms, population, and households or with all available features initially?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To make an accuracy comparison when removing features for your homework (Q5), follow these steps:\\n\\n1. **List All Features**: Start by listing all available features in your dataset.\\n\\n2. **Calculate Original Accuracy**: Train your model using all the features and evaluate its accuracy.\\n\\n3. **Remove Features One at a Time**:\\n    - For each iteration, remove one feature from the dataset.\\n    - Train the model with the remaining features.\\n    - Evaluate the model's accuracy without the removed feature.\\n\\n4. **Calculate Accuracy Differences**:\\n    - Compute the difference between the original accuracy (from step 2) and the accuracy obtained after each feature is removed.\\n    - Use absolute values for these differences to ensure you are considering the magnitude of the changes rather than their direction.\\n\\n5. **Identify the Smallest Absolute Difference**:\\n    - Determine which feature's removal results in the smallest absolute difference in accuracy.\\n\\nBy following these steps, you can effectively identify the impact of each feature on the model's accuracy and make an accurate comparison.\",\n",
       "  'answer_orig': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'document': '0f88b7ac',\n",
       "  'question': 'How should we remove the features to make an accuracy comparison?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To find the original accuracy, you can follow these steps based on the provided context:\\n\\n1. **Create a List of All Features**: Start by creating a list that includes all features you will be using in the model.\\n\\n2. **Train the Model**: Train your model using all the features listed.\\n   \\n3. **Evaluate the Accuracy**: Use the `accuracy_score` function from the scikit-learn library to evaluate the accuracy of the model. This will be your original accuracy.\\n\\nHere's an example code snippet to illustrate the process:\\n\\n```python\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\n\\n# Assuming df is your DataFrame and 'target' is your target variable\\nX = df.drop(columns=['target'])  # Features\\ny = df['target']  # Target variable\\n\\n# Split the data\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Train the model\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions\\ny_pred = model.predict(X_val)\\n\\n# Calculate the original accuracy\\noriginal_accuracy = accuracy_score(y_val, y_pred)\\nprint(f'Original Accuracy: {original_accuracy}')\\n```\\n\\nThis code will train your model using all available features and compute the original accuracy. Subsequently, you can follow the steps to drop one feature at a time, retrain the model, and compare the resulting accuracies to identify the impact of each feature.\",\n",
       "  'answer_orig': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'document': '0f88b7ac',\n",
       "  'question': 'What is the process to find the original accuracy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To determine which feature has the smallest absolute accuracy difference, you should follow these steps based on the provided context:\\n\\n1. **Evaluate the Original Model Accuracy**: Train your model using all available features and note the accuracy of the model (this is the original accuracy).\\n\\n2. **Remove One Feature at a Time**: For each feature, train a new model where you exclude that particular feature. \\n\\n3. **Measure Accuracy for Modified Models**: After training each new model, note the accuracy of the model without the specific feature.\\n\\n4. **Calculate Accuracy Differences**: For each feature removed, compute the difference between the original model's accuracy and the accuracy of the model without that feature. Specifically, let’s call the original accuracy `accuracy_original` and the accuracy without the i-th feature `accuracy_i`.\\n   \\\\[ \\\\text{difference}_i = \\\\text{accuracy_original} - \\\\text{accuracy_i} \\\\]\\n\\n5. **Determine Absolute Differences**: Convert these differences to their absolute values.\\n   \\\\[ |\\\\text{difference}_i| = \\\\left|\\\\text{accuracy_original} - \\\\text{accuracy_i}\\\\right| \\\\]\\n\\n6. **Identify the Smallest Absolute Difference**: Compare all these absolute values and identify the smallest one. The feature corresponding to this smallest absolute difference is the one you are looking for.\\n\\nIn essence, you should be using absolute values for accuracy differences to ensure you are comparing magnitudes regardless of direction (i.e., whether the difference is positive or negative). This approach focuses on the smallest impact a feature's removal has on the model's accuracy, whether the accuracy increases or decreases.\",\n",
       "  'answer_orig': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'document': '0f88b7ac',\n",
       "  'question': 'How do you determine which feature has the smallest absolute accuracy difference?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'We consider the smallest absolute accuracy difference. This means calculating the absolute value of the difference in accuracy scores when a feature is removed from the model, rather than simply the lowest numerical difference. This ensures that we are measuring the magnitude of the impact on accuracy irrespective of whether it is a positive or negative change.',\n",
       "  'answer_orig': 'Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)',\n",
       "  'document': '0f88b7ac',\n",
       "  'question': 'Do we consider the smallest absolute accuracy difference or the lowest difference value?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In Machine Learning for Classification, the primary distinction between OneHotEncoder and DictVectorizer lies in their input types. OneHotEncoder takes an array as input, while DictVectorizer works with a dictionary. Both methods convert categorical features into numerical variables for model training, and they produce similar results. However, OneHotEncoder sorts features alphabetically by default, whereas DictVectorizer allows you to stack features according to your preference.',\n",
       "  'answer_orig': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'document': '9ffcc895',\n",
       "  'question': 'In Machine Learning for Classification, what distinguishes OneHotEncoder from DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'OneHotEncoder and DictVectorizer differ primarily in the type of input they accept for converting categorical features to numerical variables.\\n\\n- **OneHotEncoder** uses an **array** as input.\\n- **DictVectorizer** uses a **dictionary** as input.\\n\\nBoth methods will produce the same result in terms of converting categorical data to numerical form. However, with OneHotEncoder, the features are sorted alphabetically, whereas with DictVectorizer, you can stack features as you want.',\n",
       "  'answer_orig': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'document': '9ffcc895',\n",
       "  'question': 'How does OneHotEncoder differ from DictVectorizer in terms of input?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'OneHotEncoder and DictVectorizer are both used to convert categorical features into numerical variables suitable for training machine learning models. The primary difference between the two lies in their input format:\\n\\n- **OneHotEncoder**: This requires an array as input for encoding the categorical features.\\n- **DictVectorizer**: This requires a dictionary as input for encoding the categorical features.\\n\\nAdditionally, using OneHotEncoder results in alphabetically sorted features, whereas DictVectorizer allows for stacking features according to your preference.',\n",
       "  'answer_orig': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'document': '9ffcc895',\n",
       "  'question': 'Can you explain the input formats for OneHotEncoder and DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The primary difference between OneHotEncoder and DictVectorizer lies in their input types. OneHotEncoder uses an array as input, while DictVectorizer uses a dictionary. Additionally, OneHotEncoder sorts features alphabetically, whereas DictVectorizer allows you to stack features as desired. Both tools convert categorical features to numerical variables for model training and produce the same result in terms of encoding.',\n",
       "  'answer_orig': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'document': '9ffcc895',\n",
       "  'question': 'What is the primary difference between OneHotEncoder and DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'OneHotEncoder and DictVectorizer handle feature ordering differently. When using OneHotEncoder, the features are sorted alphabetically. In contrast, DictVectorizer allows you to stack features in the order you want.',\n",
       "  'answer_orig': 'Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\\nTanya Mard',\n",
       "  'document': '9ffcc895',\n",
       "  'question': 'How do OneHotEncoder and DictVectorizer handle feature ordering?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The main practical advantage of using `pandas.get_dummies` over `sklearn.OneHotEncoder` is convenience when you are already working with Pandas DataFrames. `get_dummies` integrates seamlessly with Pandas, making it a straightforward choice within a Pandas-centric workflow. On the other hand, `OneHotEncoder` is a more suitable option if you are building a machine learning pipeline using scikit-learn and need to handle categorical data within that pipeline.',\n",
       "  'answer_orig': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'document': '94a3b2fb',\n",
       "  'question': 'What is the main practical advantage of using pandas get_dummies over sklearn OneHotEncoder?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"It is best to use sklearn's OneHotEncoder instead of pandas' get_dummies when you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline. While both techniques produce identical results in one-hot encoding categorical variables, OneHotEncoder is more suitable for integration within a scikit-learn workflow.\",\n",
       "  'answer_orig': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'document': '94a3b2fb',\n",
       "  'question': 'When is it best to use sklearn OneHotEncoder instead of pandas get_dummies?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the results are identical when using `pandas.get_dummies` and `sklearn.OneHotEncoder`. Both methods are techniques to one-hot-encode categorical variables, and they produce identical results in terms of encoding. The primary differences lie in their input/output types and additional functionalities, such as handling missing values and being part of a pipeline in scikit-learn-based machine learning workflows.',\n",
       "  'answer_orig': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'document': '94a3b2fb',\n",
       "  'question': 'Are the results identical when using pandas get_dummies and sklearn OneHotEncoder?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Some key differences between the input/output types of pandas `get_dummies` and sklearn `OneHotEncoder` are related to their integration within different workflows and their handling of data types:\\n\\n1. **Integration & Workflow:**\\n   - **pandas `get_dummies`:** This function is particularly well-suited for use with pandas DataFrames. It directly converts categorical columns into one-hot encoded columns, simplifying operations when you're working within a pandas-centric workflow.\\n   - **sklearn `OneHotEncoder`:** This encoder is designed to fit better within a scikit-learn-based machine learning pipeline. It is more suitable when you need a consistent preprocessing approach integrated into scikit-learn's model training and evaluation process.\\n\\n2. **Input Types:**\\n   - **pandas `get_dummies`:** Accepts a pandas DataFrame or Series as input.\\n   - **sklearn `OneHotEncoder`:** Accepts an array-like input, typically numpy arrays, making it versatile for various input types beyond pandas DataFrames.\\n\\n3. **Output Types:**\\n   - **pandas `get_dummies`:** Outputs a pandas DataFrame, maintaining the ease of use within the pandas ecosystem.\\n   - **sklearn `OneHotEncoder`:** Outputs a numpy array by default. However, it can return a sparse matrix if the `sparse` parameter is set to `True`, which can be more memory efficient for large datasets with many categories.\\n\\nBy understanding these differences, you can better choose between them based on the specific needs of your data processing workflow and the environment (pandas vs. scikit-learn pipeline) you are working within.\",\n",
       "  'answer_orig': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'document': '94a3b2fb',\n",
       "  'question': 'What are some key differences between the input/output types of pandas get_dummies and sklearn OneHotEncoder?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Pandas `get_dummies` and `sklearn` `OneHotEncoder` handle missing values differently, although the CONTEXT does not provide specific details on exactly how each method treats missing values. However, it does note that there are key differences between the two methods, including their handling of missing values. \\n\\nIn summary, both `get_dummies` and `OneHotEncoder` can one-hot-encode categorical variables but may treat missing values differently, making `get_dummies` convenient for Pandas DataFrames and `OneHotEncoder` more suitable for scikit-learn pipelines.',\n",
       "  'answer_orig': 'They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]',\n",
       "  'document': '94a3b2fb',\n",
       "  'question': 'How do pandas get_dummies and sklearn OneHotEncoder handle missing values differently?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, in the test_train_split homework question, you should set `random_state` to 42 in both splits.',\n",
       "  'answer_orig': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'document': 'fb9a45d8',\n",
       "  'question': 'In the test_train_split homework question, should random_state be set to 42 in both splits?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you should use the same random state value (random_state = 42) for both splits in week 3's homework.\",\n",
       "  'answer_orig': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'document': 'fb9a45d8',\n",
       "  'question': \"Do we use the same random state value for both splits in week 3's homework?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For HW3, the random_state 42 should be used in both splits.',\n",
       "  'answer_orig': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'document': 'fb9a45d8',\n",
       "  'question': 'Regarding HW3, is random_state 42 used once or in both splits?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, for the splitting question in week 3\\'s homework, you should use `random_state=42` for each split. This ensures reproducibility and consistency as mentioned in the context:\\n\\n\"For the test_train_split question on week 3\\'s homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\"',\n",
       "  'answer_orig': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'document': 'fb9a45d8',\n",
       "  'question': \"For the splitting question in week 3's homework, should random_state be 42 for each split?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the `random_state` should be set to 42 for both splits in the `test_train_split` question from homework 3.',\n",
       "  'answer_orig': \"For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\\nAnswer: for both splits random_state = 42 should be used\\n(Bhaskar Sarma)\",\n",
       "  'document': 'fb9a45d8',\n",
       "  'question': 'Is random_state set to 42 for both splits in the test_train_split question from homework 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'It is appropriate to compute the correlation matrix after splitting the data. The correlation matrix should be calculated using the training dataset (df_train) and not the entire dataset, which includes validation data (df_train_full). This ensures that the conclusions drawn are based only on the training portion of the data and not influenced by the validation set, which is meant to test the performance of the model.',\n",
       "  'answer_orig': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'document': 'e31051f7',\n",
       "  'question': 'When is it appropriate to compute the correlation matrix?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find the most correlated features in your dataset, you should follow these steps:\\n\\n1. **Create the Correlation Matrix**: Calculate the correlation matrix for your training dataset. This means you need to split your data first and then calculate the correlations on the training set.\\n\\n2. **Identify the Two Most Correlated Features**:\\n   - **Option 1**: Use the correlation matrix to find the pair of features with the highest absolute correlation coefficient.\\n   - **Option 2**: Use pandas and seaborn for an easy visualization:\\n     - **Pandas**: \\n       ```python\\n       import pandas as pd\\n       data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\n       data_corr.head(10)\\n       ```\\n     - **Seaborn Heatmap**:\\n       ```python\\n       import seaborn as sns\\n       import matplotlib.pyplot as plt\\n       sns.heatmap(df[numerical_features].corr(), annot=True, square=True, fmt=\".2g\", cmap=\"crest\")\\n       plt.show()\\n       ```\\n\\nFollowing these steps will help you visualize and identify the two most correlated features from your dataset.',\n",
       "  'answer_orig': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'document': 'e31051f7',\n",
       "  'question': 'How should I find the most correlated features in my dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is necessary to calculate the correlation after splitting the dataset. This is to avoid data leakage, ensuring that no data from the validation or test sets influences the training stage. You should compute the correlation matrix on your train dataset only, as mentioned in the context, \"Correlation matrix of your train dataset. Thus, after splitting.\"',\n",
       "  'answer_orig': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'document': 'e31051f7',\n",
       "  'question': 'Is it necessary to calculate the correlation before splitting the dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To identify high correlation between features, you can use the following methods:\\n\\n1. **Using Pandas DataFrame and `unstack` method**:\\n   ```python\\n   data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\n   data_corr.head(10)\\n   ```\\n\\n2. **Using Seaborn to create a heatmap**:\\n   ```python\\n   sns.heatmap(df[numerical_features].corr(), \\n               annot=True, \\n               square=True, \\n               fmt=\".2g\", \\n               cmap=\"crest\")\\n   ```\\n\\nThese methods help efficiently identify and visualize high correlations between numerical features.',\n",
       "  'answer_orig': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'document': 'e31051f7',\n",
       "  'question': 'What is the best method to identify high correlation between features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the correlation coefficient should be in absolute terms when identifying correlated features. The two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'answer_orig': 'Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.',\n",
       "  'document': 'e31051f7',\n",
       "  'question': 'Should the correlation coefficient be in absolute terms when identifying correlated features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In a Ridge Regression model, the type of data that can be used directly is numerical data. Categorical data must first be transformed into a numerical format using techniques like one-hot encoding before it can be used in the model. This transformation ensures compatibility with the Ridge Regression model, which requires numerical input.',\n",
       "  'answer_orig': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'document': '493b7b59',\n",
       "  'question': 'What type of data can be used directly in a Ridge Regression model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Before using Ridge Regression, categorical features should be handled as follows:\\n\\n1. **Transform Categorical Features**: Convert categorical features into numerical format by using one-hot encoding. Both `OneHotEncoder` and `pandas.get_dummies()` can be used for this purpose. Ensure that `sparse=True` is set when utilizing `OneHotEncoder` to avoid non-convergence errors.\\n\\n2. **Normalize Numerical Features**: Use normalization techniques such as `MinMaxScaler` or `StandardScaler` on your numerical features to ensure they are on a similar scale.\\n\\n3. **Combine the Transformed Features**: After encoding the categorical features and normalizing the numerical features, combine them to form a single feature matrix (`X_train`). This combined dataset will serve as the input for training the Ridge regression model.\\n\\nSo, the steps are:\\n1. Drop the categorical features from the dataset.\\n2. Use `OneHotEncoder` with `sparse=True` to transform categorical features into numerical format.\\n3. Normalize numerical features.\\n4. Combine the transformed categorical and numerical features into a single feature matrix.\\n\\nBy following these procedures, you can ensure that the categorical data is correctly handled and that your Ridge regression model is trained effectively without encountering convergence issues.',\n",
       "  'answer_orig': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'document': '493b7b59',\n",
       "  'question': 'How should categorical features be handled before using Ridge Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One-hot encoding is necessary for categorical features in Ridge Regression because Ridge Regression only accepts numerical features, not categorical ones. By transforming categorical features into a numerical format using one-hot encoding, we make them compatible with the Ridge Regression model. Additionally, one-hot encoding ensures that the categorical data is represented in a binary vector format, allowing the model to process it effectively and avoid non-convergence errors.',\n",
       "  'answer_orig': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'document': '493b7b59',\n",
       "  'question': 'Why do we need to use one-hot encoding for categorical features in Ridge Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The parameter that should be set to true when using one-hot encoding to avoid non-convergence errors is `sparse`.',\n",
       "  'answer_orig': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'document': '493b7b59',\n",
       "  'question': 'What parameter should be set to true when using one-hot encoding to avoid non-convergence errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The advice on how to handle categorical features in Ridge Regression was provided by Erjon.',\n",
       "  'answer_orig': 'Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\\nDrop all categorical features first before proceeding.\\n(Aileah Gotladera)\\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\\n(Erjon)',\n",
       "  'document': '493b7b59',\n",
       "  'question': 'Who provided the advice on how to handle categorical features in Ridge Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For Homework 3 Question 6, you need to use **all features and price for the target**. Do not include the average variable created previously. If you use a `DictVectorizer`, make sure to set `sparse=True` to avoid convergence errors. Additionally, using `StandardScaler` for numerical variables is recommended, although you can try running it with or without this scaler. \\n\\n(Peter Pan)',\n",
       "  'answer_orig': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'document': '4a55c510',\n",
       "  'question': 'Which features should I use for Homework 3 Question 6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In Homework 3 Question 6, the target should be the price. You need to use all features and exclude the average variable created before.',\n",
       "  'answer_orig': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'document': '4a55c510',\n",
       "  'question': 'What should be the target in Homework 3 Question 6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you should not include the average variable created previously for Homework 3 Question 6. You need to use all features and the price as the target.',\n",
       "  'answer_orig': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'document': '4a55c510',\n",
       "  'question': 'Should I include the average variable created previously for Homework 3 Question 6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When using DictVectorizer, you should ensure that you fit and transform the train set and only transform the validation and test sets. This is to avoid overwriting what the DictVectorizer has learned from the train data and to properly validate the performance of your model on unseen data. \\n\\nHere’s how you should handle it:\\n\\n1. Fit and transform the training dataset:\\n   ```python\\n   X_train = dv.fit_transform(train_dicts)\\n   ```\\n\\n2. Only transform the validation and test datasets:\\n   ```python\\n   X_val = dv.transform(val_dicts)\\n   X_test = dv.transform(test_dicts)\\n   ```\\n\\nThis way, you maintain the integrity of the model training and validation process. Additionally, use the `.get_feature_names_out()` method instead of the deprecated `.get_feature_names()` method to resolve any FutureWarnings.',\n",
       "  'answer_orig': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'document': '4a55c510',\n",
       "  'question': 'When using DictVectorizer, what setting should I ensure?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you should use StandardScaler for numerical variables in Homework 3 Question 6. According to the COURSE FAQ, using a StandardScaler for the numeric fields can help with convergence issues when training your model. It is also mentioned that playing with different scalers might be beneficial, but using StandardScaler is specifically recommended for numerical variables.',\n",
       "  'answer_orig': \"You need to use all features. and price for target. Don't include the average variable we created before.\\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\\nI also used StandardScalar for numerical variable you can try running with or without this\\n(Peter Pan)\",\n",
       "  'document': '4a55c510',\n",
       "  'question': 'Should I use StandardScalar for numerical variables in Homework 3 Question 6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In classification tasks, you can use the following sklearn tools to convert non-numerical data to numerical data:\\n\\n- `OneHotEncoder`\\n- `OrdinalEncoder`\\n\\nThese tools are part of the `sklearn.preprocessing` module.',\n",
       "  'answer_orig': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'document': '3ca0b489',\n",
       "  'question': 'Which sklearn tools can convert non-numerical data to numerical in classification tasks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To transform categorical data to numerical values in machine learning, you can use `sklearn.preprocessing` encoders such as `OneHotEncoder` and `OrdinalEncoder`. Additionally, you can use `StandardScaler` for scaling numerical values. \\n\\nHere’s an example of transforming a categorical column using pandas for one-hot encoding:\\n\\n``` python\\nimport pandas as pd\\n\\n# Assuming 'data' is your DataFrame and 'category_column' is the column with categorical data\\ndata_encoded = pd.get_dummies(data, columns=['category_column'])\\n```\\n\\nIn this example, `pd.get_dummies()` creates a new DataFrame where the categorical column is replaced with binary columns for each category present in the original column. Each row in the DataFrame has a 1 in the column that corresponds to its category and 0 in all other category columns.\",\n",
       "  'answer_orig': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'document': '3ca0b489',\n",
       "  'question': 'How do I transform categorical data to numerical values in machine learning?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In sklearn, you can use the following techniques available in the `sklearn.preprocessing` module for encoding non-numerical columns:\\n\\n- **OneHotEncoder**: This encoder transforms categorical features to a format that works better with classification and regression algorithms by creating one binary column per category.\\n- **OrdinalEncoder**: This encoder converts categorical values into integers, preserving the ordinal relationship among them.\\n\\nYou can also use other preprocessing tools like `StandardScaler`, although it is not specifically for encoding categorical data.',\n",
       "  'answer_orig': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'document': '3ca0b489',\n",
       "  'question': 'What techniques are available in sklearn for encoding non-numerical columns?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For transforming categorical features into numerical features in sklearn, you can use `sklearn.preprocessing` encoders such as `OneHotEncoder` and `OrdinalEncoder`. Additionally, `DictVectorizer` and `FeatureHasher` are also available for dealing with categorical data, each with its own use case depending on the nature and requirements of your data.\\n\\n- **OneHotEncoder**: Converts categorical values into a one-hot numeric array.\\n- **OrdinalEncoder**: Encodes categorical values as ordinal integers.\\n- **DictVectorizer**: Transforms lists of feature-value mappings to vectors. Efficient for small unique value sets.\\n- **FeatureHasher**: Converts categorical variables into a hashed representation, useful for high cardinality features.\\n\\nYou can find more information and encoding techniques [here](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02).',\n",
       "  'answer_orig': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'document': '3ca0b489',\n",
       "  'question': 'Can you suggest methods for numerical transformation of categorical features in sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To preprocess non-numerical columns for machine learning using sklearn, you can use the encoding tools available in `sklearn.preprocessing` such as `OneHotEncoder` and `OrdinalEncoder`. Additionally, for numerical preprocessing, consider using `StandardScaler`. Here’s a brief overview of these tools:\\n\\n- **OneHotEncoder**: This encoder transforms categorical (non-numerical) values into a binary (0 or 1) matrix. Each category is converted into a separate column with binary representation.\\n- **OrdinalEncoder**: This encoder converts categorical data into numerical format. Each unique category is assigned an integer value.\\n- **StandardScaler**: This scaler standardizes features by removing the mean and scaling to unit variance. It is typically used for numerical features.\\n\\nFor an example use case, you could:\\n\\n```python\\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\\nimport pandas as pd\\n\\n# Sample DataFrame\\ndata = {\\n    'category': ['cat', 'dog', 'bird'],\\n    'value': [1, 2, 3]\\n}\\ndf = pd.DataFrame(data)\\n\\n# OneHotEncoder example\\none_hot_encoder = OneHotEncoder(sparse=False)\\nencoded_categories = one_hot_encoder.fit_transform(df[['category']])\\n\\n# OrdinalEncoder example\\nordinal_encoder = OrdinalEncoder()\\nencoded_categories_ord = ordinal_encoder.fit_transform(df[['category']])\\n\\n# StandardScaler example\\nscaler = StandardScaler()\\nscaled_values = scaler.fit_transform(df[['value']])\\n\\nprint(encoded_categories)\\nprint(encoded_categories_ord)\\nprint(scaled_values)\\n```\\n\\nIn the context provided, these encoders and scaler can be used as appropriate preprocessing steps before feeding the data into a machine learning model.\",\n",
       "  'answer_orig': 'Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.',\n",
       "  'document': '3ca0b489',\n",
       "  'question': 'How to preprocess non-numerical columns for machine learning using sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you need to preserve feature names in transformed data for categorical features, you should use the DictVectorizer. This tool is appropriate especially if you have a small number of unique values.',\n",
       "  'answer_orig': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'document': '690d97f1',\n",
       "  'question': 'What should I use if I need to preserve feature names in transformed data for categorical features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For categorical features with high cardinality, the suitable method is FeatureHasher. FeatureHasher creates vectors with predefined length and is more memory-efficient in handling high cardinality compared to DictVectorizer, which stores a large vocabulary and uses more memory.',\n",
       "  'answer_orig': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'document': '690d97f1',\n",
       "  'question': 'Which method is suitable for categorical features with high cardinality, FeatureHasher or DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, DictVectorizer consumes more memory compared to FeatureHasher. DictVectorizer will store the entire vocabulary, which can take up significant memory space, especially with high cardinality categorical features. In contrast, FeatureHasher creates vectors with predefined lengths, making it more memory efficient.',\n",
       "  'answer_orig': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'document': '690d97f1',\n",
       "  'question': 'Does DictVectorizer consume more memory compared to FeatureHasher?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, both FeatureHasher and DictVectorizer can handle categorical features. However, the choice between them depends on the specific requirements of your data, such as memory usage and whether preserving feature names is important.\\n\\n- **FeatureHasher** is more suitable for handling categorical features with high cardinality as it creates vectors with a predefined length, which is more memory-efficient.\\n- **DictVectorizer** is better when you need to preserve feature names and you have a small number of unique values, though it consumes more memory because it stores the entire vocabulary.\\n\\nYou can explore more about their usage [here](https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html).',\n",
       "  'answer_orig': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'document': '690d97f1',\n",
       "  'question': 'Can both FeatureHasher and DictVectorizer handle categorical features?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can read more about the differences between FeatureHasher and DictVectorizer by following this link: [FeatureHasher vs DictVectorizer](https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html).',\n",
       "  'answer_orig': 'These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\nOlga Rudakova',\n",
       "  'document': '690d97f1',\n",
       "  'question': 'Where can I read more about the differences between FeatureHasher and DictVectorizer?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'It is recommended to use DictVectorizer or `get_dummies` after splitting the data to avoid data leakage. Data leakage occurs when information from the test set influences the training process, which can lead to over-optimistic performance estimates and generalization errors. By ensuring that these encodings are applied after splitting the dataset, you prevent the test set from inadvertently affecting the training or validation stages, resulting in a more robust and accurate model evaluation. This is a recommended practice highlighted in the scikit-learn documentation on common pitfalls and recommended practices.',\n",
       "  'answer_orig': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'document': 'eb5a25cb',\n",
       "  'question': 'Why is it recommended to use DictVectorizer or get_dummies after splitting the data?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you use `DictVectorizer` or `pandas.get_dummies` before splitting the data into train/validation/test sets, you can encounter a major issue: **data leakage**. This happens because data from the test set could influence the training stage, thus distorting the performance evaluations and leading to overly optimistic results. \\n\\nIn terms of efficiency and model performance, `DictVectorizer(sparse=False)` and `pandas.get_dummies(sparse=True)` can also lead to slower processing times (e.g., 6-8 minutes for a high number of classes) and potentially worse results due to high memory usage and convergence issues during fitting.\\n\\nTo avoid these problems, it's recommended to perform these transformations after splitting the dataset to ensure that no information from the validation or test sets leaks into the training process.\",\n",
       "  'answer_orig': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'document': 'eb5a25cb',\n",
       "  'question': 'What issues arise if we use DictVectorizer or get_dummies before splitting?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For more information about avoiding data leakage in machine learning, you can refer to the section on Decision Trees and Ensemble Learning. Specifically, data leakage can occur when you fill in missing values using an entire dataset before splitting it for training, testing, or validation. This can lead to biased performance estimates and poor model generalization.',\n",
       "  'answer_orig': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'document': 'eb5a25cb',\n",
       "  'question': 'Where can I find more information about avoiding data leakage in machine learning?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Using DictVectorizer or get_dummies incorrectly, especially before splitting data into train and test sets, can introduce data leakage. Data leakage occurs when information from the test set influences the training process, leading to overly optimistic performance estimates. This incorrect handling violates the principle that the test set should remain completely unseen during training to provide an unbiased evaluation of the model.\\n\\nThe proper approach is to first split the data into train, validation, and test sets. Then you should fit_transform the train set and only transform the validation and test sets using the fitted DictVectorizer or get_dummies model. This ensures that no information from the validation or test set influences the training process, thereby maintaining the model's integrity and providing an accurate measure of its predictive performance on unseen data.\",\n",
       "  'answer_orig': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'document': 'eb5a25cb',\n",
       "  'question': 'How does using DictVectorizer or get_dummies incorrectly influence train and test sets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The answer regarding the recommended practice for using DictVectorizer or get_dummies was provided by Larkin Andrii.',\n",
       "  'answer_orig': '(Question by Connie S.)\\nThe reason it\\'s good/recommended practice to do it after splitting is to avoid data leakage - you don\\'t want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\\nAnswered/added by Rileen Sinha',\n",
       "  'document': 'eb5a25cb',\n",
       "  'question': 'Who provided the answer regarding the recommended practice for using DictVectorizer or get_dummies?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In HW3Q4, an accuracy of 1.0 typically suggests that you have likely overfitted the model. One way to address this issue is by dropping the column `msrp/price` from your dataset.\\n\\n',\n",
       "  'answer_orig': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'document': '6d9e0a6f',\n",
       "  'question': 'What might cause an accuracy of 1.0 in HW3Q4?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you are getting an accuracy of 1.0 in HW3Q4, it suggests that your model might be overfitting. To address overfitting, you can drop the column msrp/price from your dataset. This adjustment should help in resolving the issue.',\n",
       "  'answer_orig': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'document': '6d9e0a6f',\n",
       "  'question': 'How can I address overfitting if I get an accuracy of 1.0 in HW3Q4?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your HW3Q4 accuracy is 1.0, you should not simply select the nearest option. Instead, this high accuracy suggests that you may have overfitted your model. To address this issue, consider dropping the column \"msrp/price\" to see if it resolves the overfitting problem.',\n",
       "  'answer_orig': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'document': '6d9e0a6f',\n",
       "  'question': 'Should I select the nearest option if my HW3Q4 accuracy is 1.0?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To improve your model accuracy in HW3Q4, you should consider that if you are currently achieving an accuracy of 1.0, your model may be overfitted. A suggested adjustment to address this issue is to drop the column \"msrp/price\" from your features. This change can help in reducing overfitting and potentially improve your model\\'s accuracy.',\n",
       "  'answer_orig': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'document': '6d9e0a6f',\n",
       "  'question': 'What adjustment can improve my model accuracy in HW3Q4?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix overfitting in HW3Q4, you should drop the column **msrp/price**.',\n",
       "  'answer_orig': 'If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\\nAdded by Akshar Goyal',\n",
       "  'document': '6d9e0a6f',\n",
       "  'question': 'Which column should be dropped to fix overfitting in HW3Q4?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The packages recommended for calculating Root Mean Squared Error (RMSE) are `sklearn` (scikit-learn) and `numpy`. Examples of their usage include:\\n\\n1. Using `np.sqrt` and `sklearn.metrics.mean_squared_error`:\\n   ```python\\n   from sklearn.metrics import mean_squared_error\\n   import numpy as np\\n   rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest))\\n   ```\\n\\n2. Using `mean_squared_error` with the `squared` parameter set to `False`:\\n   ```python\\n   from sklearn.metrics import mean_squared_error\\n   rms = mean_squared_error(y_actual, y_predicted, squared=False)\\n   ``` \\n\\nFeel free to refer to the specific examples and additional details provided in the context.',\n",
       "  'answer_orig': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'document': '618ad97a',\n",
       "  'question': 'What packages are recommended for calculating Root Mean Squared Error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a notebook reference available for further understanding of RMSE calculation. You can refer to Alexey’s notebook for Week 2 at the following link:\\n\\n[https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb)\\n\\nThis notebook includes detailed code and explanation related to RMSE.',\n",
       "  'answer_orig': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'document': '618ad97a',\n",
       "  'question': 'Is there a notebook reference available for further understanding of RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The alternative code snippet for calculating RMSE using the `mean_squared_error` function with the `squared` parameter set to `False` was contributed by Ahmed Okka.',\n",
       "  'answer_orig': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'document': '618ad97a',\n",
       "  'question': 'Who contributed the alternative code snippet for calculating RMSE?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a practical example of RMSE calculation in Alexey’s notebook for Week 2. The notebook includes code demonstrating the RMSE calculation process:\\n\\n```python\\ndef rmse(y, y_pred):\\n    error = y_pred - y\\n    mse = (error ** 2).mean()\\n    return np.sqrt(mse)\\n```\\n\\nYou can access the full notebook here: [Alexey’s notebook for Week 2](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb)\\n\\nAdditionally, here is a simplified example using the `sklearn` and `numpy` packages:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\n\\nactual_values = [/* actual values here */]\\npredicted_values = [/* predicted values here */]\\n\\nmse = mean_squared_error(actual_values, predicted_values)\\nrmse = np.sqrt(mse)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```\\n\\nFor more details, you can refer to the [Stack Overflow thread](https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python).',\n",
       "  'answer_orig': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'document': '618ad97a',\n",
       "  'question': 'Where can I find a practical example of RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The command from `sklearn` to import the required function for RMSE calculation is:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\n```',\n",
       "  'answer_orig': 'We can use sklearn & numpy packages to calculate Root Mean Squared Error\\nfrom sklearn.metrics import mean_squared_error\\nimport numpy as np\\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\\nAdded by Radikal Lukafiardi\\nYou can also refer to Alexey’s notebook for Week 2:\\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\nwhich includes the following code:\\ndef rmse(y, y_pred):\\nerror = y_pred - y\\nmse = (error ** 2).mean()\\nreturn np.sqrt(mse)\\n(added by Rileen Sinha)',\n",
       "  'document': '618ad97a',\n",
       "  'question': 'What is the command from sklearn to import the required function for RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Instead of using 'get_feature_names' for 'DictVectorizer', you should use the method 'get_feature_names_out()'. This method returns a numpy array of the feature names, which you can convert to a list if needed by using `list(dv.get_feature_names_out())`.\",\n",
       "  'answer_orig': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'document': '683495d2',\n",
       "  'question': \"What should I use instead of 'get_feature_names' for 'DictVectorizer'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To resolve the AttributeError related to 'DictVectorizer', you should use the method `get_feature_names_out()` instead of the deprecated `get_feature_names()`. This change is necessary to avoid the AttributeError and comply with updated library versions.\\n\\nFor more details, you can visit the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html).\\n\\nHope this helps! If you have any more questions, feel free to ask.\",\n",
       "  'answer_orig': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'document': '683495d2',\n",
       "  'question': \"How do I resolve the AttributeError related to 'DictVectorizer'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can find details about the 'get_feature_names_out' method by visiting the following link:\\n[sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names).\\n\\nAdditionally, the 'get_feature_names_out' method is mentioned in various questions within the context:\\n- It's a replacement for the deprecated `get_feature_names()` function.\\n- It is used with `DictVectorizer` and returns feature names as a NumPy array of string objects.\\n- You might need to convert the resulting NumPy array to a list using `list(dv.get_feature_names_out())`.\\n\\nFor more detailed usage, you can refer to the specific sections in the provided context or visit the link above to the official scikit-learn documentation.\",\n",
       "  'answer_orig': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'document': '683495d2',\n",
       "  'question': \"Where can I find details about the 'get_feature_names_out' method?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The solution for the 'DictVectorizer' attribute error was provided by George Chizhmak.\",\n",
       "  'answer_orig': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'document': '683495d2',\n",
       "  'question': \"Who provided the solution for the 'DictVectorizer' attribute error?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The 'DictVectorizer' error is discussed in section 3. Machine Learning for Classification.\",\n",
       "  'answer_orig': 'The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\nGeorge Chizhmak',\n",
       "  'document': '683495d2',\n",
       "  'question': \"In which section is the 'DictVectorizer' error discussed?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To use Root Mean Squared Error (RMSE) without involving mathematical calculations or numpy, you can use the `mean_squared_error` function from the `sklearn.metrics` library with the `squared` parameter set to `False`:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\n\\n# Example usage:\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\n```',\n",
       "  'answer_orig': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'document': 'dc1897b5',\n",
       "  'question': 'How can I use RMSE without involving mathematical calculations or numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The library in Python that provides a function for calculating RMSE easily is `sklearn.metrics`. You can use the `mean_squared_error` function with the `squared` kwarg set to `False`, as shown below:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\n```\\n\\nThis will provide the RMSE directly without the need for additional calculations.',\n",
       "  'answer_orig': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'document': 'dc1897b5',\n",
       "  'question': 'Which library in Python provides a function for calculating RMSE easily?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The default behavior of the `mean_squared_error` function in `sklearn.metrics` is to return the Mean Squared Error (MSE). This is because the `squared` keyword argument defaults to `True`. If you want to return the Root Mean Squared Error (RMSE), you would set the `squared` argument to `False`.\\n\\nReference:\\n```python\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\n```',\n",
       "  'answer_orig': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'document': 'dc1897b5',\n",
       "  'question': 'What is the default behavior of the mean_squared_error function in sklearn.metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To modify the `mean_squared_error` function to return RMSE instead of MSE, you can use the `squared=False` argument in the function call. Here is how you can do it:\\n\\n```python\\nfrom sklearn.metrics import mean_squared_error\\n\\n# Assuming y_actual and y_predicted are your actual and predicted values respectively\\nrmse = mean_squared_error(y_actual, y_predicted, squared=False)\\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\\n```\\n\\nBy setting `squared=False`, the function will return the Root Mean Squared Error (RMSE) directly.',\n",
       "  'answer_orig': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'document': 'dc1897b5',\n",
       "  'question': 'How do I modify the mean_squared_error function to return RMSE instead of MSE?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find more details about using the `mean_squared_error` function for RMSE in Python, you can refer to the following instructions and code snippet:\\n\\n1. The `mean_squared_error` function from `sklearn.metrics` can be used to calculate RMSE by setting the `squared` kwarg to `False`.\\n2. Example usage:\\n   ```python\\n   from sklearn.metrics import mean_squared_error\\n\\n   rms = mean_squared_error(y_actual, y_predicted, squared=False)\\n   ```\\n\\nFor additional detailed information and examples, you can visit the following Stack Overflow link:\\n[https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python](https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python)\\n\\nAhmed Okka provided this information, and you can also check Alexey’s notebook for practical examples:\\n[https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb)\\n\\nThis notebook includes a function for RMSE calculation:\\n```python\\ndef rmse(y, y_pred):\\n    error = y_pred - y\\n    mse = (error ** 2).mean()\\n    return np.sqrt(mse)\\n```',\n",
       "  'answer_orig': 'To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\\nfrom sklearn.metrics import mean_squared_error\\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\nAhmed Okka',\n",
       "  'document': 'dc1897b5',\n",
       "  'question': 'Where can I find more details about using the mean_squared_error function for RMSE in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The article that explains different encoding techniques used in Machine Learning for Classification can be found at the following link: [All About Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02).',\n",
       "  'answer_orig': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'document': '826098f2',\n",
       "  'question': 'What article explains different encoding techniques used in Machine Learning for Classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a detailed explanation of categorical variable encoding by reading the article linked here: [All About Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02).\\n\\nHrithik Kumar Advani',\n",
       "  'answer_orig': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'document': '826098f2',\n",
       "  'question': 'Where can I find a detailed explanation of categorical variable encoding?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The article about encoding techniques in this course section was authored by Hrithik Kumar Advani.',\n",
       "  'answer_orig': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'document': '826098f2',\n",
       "  'question': 'Who authored the article about encoding techniques in this course section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The section that contains information about encoding techniques is **section 3. Machine Learning for Classification**.',\n",
       "  'answer_orig': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'document': '826098f2',\n",
       "  'question': 'Which section contains information about encoding techniques?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Certainly! You can find an article explaining different encoding techniques for categorical variables at this link: [All About Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02).',\n",
       "  'answer_orig': 'This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\nHrithik Kumar Advani',\n",
       "  'document': '826098f2',\n",
       "  'question': 'Could you provide a link to an article on categorical variable encoding?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To use `accuracy_score` from `sklearn` for classification evaluation correctly, you can follow the given example:\\n\\n```python\\nfrom sklearn.metrics import accuracy_score\\n\\n# Assuming y_val are the true labels and y_pred are the predicted labels\\naccuracy = accuracy_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\n```\\n\\nIf you encounter the error `TypeError: 'numpy.float64' object is not callable`, ensure that you are not mistakenly calling a numpy float as a function. For instance, the following code should be corrected as shown below:\\n\\nIncorrect:\\n```python\\n# This might raise an error\\naccuracy_score(y_val, y_pred >= 0.5)\\n```\\n\\nCorrect:\\n```python\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_val, y_pred >= 0.5)\\n```\\n\\nIn summary, make sure to import `accuracy_score` from `sklearn.metrics` and pass the true labels and predicted labels directly to it. If comparing probabilities or thresholded values, ensure that logical conditions (e.g., `y_pred >= 0.5`) are correctly handled.\",\n",
       "  'answer_orig': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'document': '821dfc08',\n",
       "  'question': 'What is the correct way to use accuracy_score in sklearn for classification evaluation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix the TypeError when using `accuracy_score` in sklearn, you should import the `metrics` module from sklearn and avoid using any callable notation with the accuracy score. Here’s how you can resolve the issue:\\n\\nReplace:\\n```python\\naccuracy_score(y_val, y_pred >= 0.5)\\nTypeError: 'numpy.float64' object is not callable\\n```\\n\\nWith:\\n```python\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred >= 0.5)\\n```\\n\\nThis solution should help you avoid the `TypeError` caused by the incorrect usage of `accuracy_score`.\",\n",
       "  'answer_orig': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'document': '821dfc08',\n",
       "  'question': 'How can I fix the TypeError when using accuracy_score in sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To use `accuracy_score` correctly, you should import it from the `sklearn.metrics` module. Here is the proper import statement:\\n\\n```python\\nfrom sklearn.metrics import accuracy_score\\n```',\n",
       "  'answer_orig': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'document': '821dfc08',\n",
       "  'question': 'Which sklearn module should I import to use accuracy_score correctly?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"A common mistake that might cause a `TypeError` with `accuracy_score` in Jupyter is using parentheses incorrectly, for instance trying to call a NumPy float as if it were a function. The error mentioned is:\\n\\n```python\\nTypeError: 'numpy.float64' object is not callable\\n```\\n\\nThis issue can be resolved by using `metrics.accuracy_score` correctly as shown:\\n\\n```python\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred >= 0.5)\\n```\",\n",
       "  'answer_orig': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'document': '821dfc08',\n",
       "  'question': 'What common mistake might cause a TypeError with accuracy_score in jupyter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Certainly! Here's a code example for correctly using `accuracy_score` from `sklearn`:\\n\\n```python\\nfrom sklearn.metrics import accuracy_score\\n\\n# Example ground truth labels and predicted labels\\ny_val = [0, 1, 1, 0, 1, 0]  # Replace with your actual data\\ny_pred = [0, 1, 0, 0, 1, 1]  # Replace with your actual predictions\\n\\n# Computing the accuracy score\\naccuracy = accuracy_score(y_val, y_pred)\\n\\nprint(f'Accuracy: {accuracy}')\\n```\\n\\nEnsure you have the following import statement at the beginning of your code:\\n```python\\nfrom sklearn.metrics import accuracy_score\\n```\",\n",
       "  'answer_orig': \"I got this error multiple times here is the code:\\n“accuracy_score(y_val, y_pred >= 0.5)”\\nTypeError: 'numpy.float64' object is not callable\\nI solve it using\\nfrom sklearn import metrics\\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\\nOMAR Wael\",\n",
       "  'document': '821dfc08',\n",
       "  'question': 'Can you provide a code example for correctly using accuracy_score in sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the Week 4 homework at the following link: [Week 4 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md).',\n",
       "  'answer_orig': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'document': '27c8d5da',\n",
       "  'question': 'Where can I find the Week 4 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can access all the course's homework assignments by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/). Scroll down and start going through the course materials.\",\n",
       "  'answer_orig': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'document': '27c8d5da',\n",
       "  'question': \"Where can I access all the course's homework assignments?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, I can provide the Evaluation Matrix link. Here it is: [Evaluation Matrix](https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml).',\n",
       "  'answer_orig': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'document': '27c8d5da',\n",
       "  'question': 'Can you provide the Evaluation Matrix link?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The GitHub repository for the course theory is not explicitly mentioned in the provided context. Therefore, if you're looking for the original course repo to fork and add your community notes to, it would likely be shared by the course instructors or found within the course materials. You may need to check your course syllabus or ask an instructor directly for the specific GitHub URL associated with the course theory.\",\n",
       "  'answer_orig': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'document': '27c8d5da',\n",
       "  'question': 'Where can I find the GitHub repository for the course theory?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the provided CONTEXT does not mention a YouTube video covering Evaluation Metrics for Classification.',\n",
       "  'answer_orig': 'Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\nSci-Kit Learn on Evaluation:\\nhttps://scikit-learn.org/stable/model_selection.html\\n~~Nukta Bhatia~~',\n",
       "  'document': '27c8d5da',\n",
       "  'question': 'Do you have a YouTube video covering Evaluation Metrics for Classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find information on using a variable to score in classification by visiting the following link: [https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119](https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119). Metrics can be applied to a series or a dataframe. \\n\\n~~Ella Sahnan~~',\n",
       "  'answer_orig': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'document': 'a52d4739',\n",
       "  'question': 'Where can I find information on using a variable to score in classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, evaluation metrics can be used on both a series and a dataframe.',\n",
       "  'answer_orig': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'document': 'a52d4739',\n",
       "  'question': 'Are evaluation metrics applicable to both a series and a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The platform providing detailed instructions on scoring with variables is Slack, with more information available at the following link: [https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119](https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119).',\n",
       "  'answer_orig': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'document': 'a52d4739',\n",
       "  'question': 'Which platform provides detailed instructions on scoring with variables?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, metrics can be applied to dataframes according to the course.\\n\\nMetrics can be used on both a series and a dataframe.\\n\\n~~Ella Sahnan~~',\n",
       "  'answer_orig': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'document': 'a52d4739',\n",
       "  'question': 'Can metrics be applied to dataframes according to the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about using metrics on a series or dataframe in section 4 was contributed by Ella Sahnan.',\n",
       "  'answer_orig': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\nMetrics can be used on a series or a dataframe\\n~~Ella Sahnan~~',\n",
       "  'document': 'a52d4739',\n",
       "  'question': 'Who contributed the information about using metrics on a series or dataframe in section 4?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Some classification tasks require setting the `random_state` parameter to ensure the reproducibility of the randomness used in the process, such as shuffling datasets or initializing the random number generator. This reproducibility is crucial for making consistent and comparable results, particularly when you are sharing your results or debugging your code. For example, in cases like `train_test_split` or `KFold` cross-validation, setting the `random_state` ensures that the splits of the dataset are consistent across different runs, allowing you to reliably compare the performance of your models.',\n",
       "  'answer_orig': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'document': 'dc55359c',\n",
       "  'question': 'Why do some classification tasks require setting the random_state parameter?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Both `random_state` and `shuffle` parameters should be set in classification tasks when you want to ensure that the \"randomness\" used to shuffle the dataset is reproducible. This is particularly useful when using techniques like KFold cross-validation, where having a consistent random state allows for reproducibility and fair comparison of results. By setting both parameters, you control the shuffling process and ensure that the results can be replicated.',\n",
       "  'answer_orig': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'document': 'dc55359c',\n",
       "  'question': 'When should both random_state and shuffle parameters be set in classification tasks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The reproducibility of randomness in datasets during classification is ensured by setting the `random_state` parameter, often in conjunction with the `shuffle` parameter. This approach is used to ensure that the randomness applied to shuffle the dataset remains consistent each time the code is executed. Consequently, this allows for reproducibility in experiments and results. \\n\\nFor example, in scikit-learn, setting these parameters ensures that the dataset is shuffled in the same way across different runs, as per the details mentioned in the sklearn documentation.',\n",
       "  'answer_orig': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'document': 'dc55359c',\n",
       "  'question': 'What ensures the reproducibility of randomness in datasets during classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Randomness is a crucial aspect in various stages of machine learning, and the `random_state` parameter controls this randomness to ensure reproducibility of results. However, its necessity depends on the specific task being performed.\\n\\nIn the context of your module-04 homework:\\n\\n- The `random_state` parameter ensures reproducibility by setting the seed of the random number generator. This is particularly important when dealing with operations that involve randomness, such as shuffling the dataset or splitting the dataset into training and validation sets. By fixing the `random_state`, you ensure that every time you run the code, the splitting or shuffling produces the same result.\\n\\n- Specifically, for module-04:\\n  - In some questions, such as Qn2, using `random_state` might not be explicitly required if the task doesn't involve any operation where the randomness affects the outcome directly, or reproducibility is not a critical concern for understanding the core concept being taught.\\n  - In cases like Qn5, or when using methods like KFold cross-validation, where the random splitting of data can lead to different outcomes, setting `random_state` helps in generating consistent results, facilitating easier debugging and comparison of model performance across runs.\\n\\nAccording to the provided context:\\n- The use of `random_state` varies depending on the specific requirements of the task. For instance:\\n  - When using methods like `KFold(n_splits=n_splits, shuffle=True, random_state=1)`, the `random_state` ensures that the dataset is split in a consistent manner across different runs, which is important for reproducibility.\\n  - While creating or evaluating models like `DecisionTreeClassifier`, setting `random_state` is crucial to get consistent metrics, such as AUC, on re-runs.\\n\\nTherefore, `random_state` is used selectively based on whether reproducibility of that particular step is required.\",\n",
       "  'answer_orig': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'document': 'dc55359c',\n",
       "  'question': 'Can you clarify why random_state is not always needed in module-04 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For more information about the use of `random_state` in scikit-learn (sklearn), you can refer to the official sklearn documentation. One relevant link provided in the context is: \\n\\n- [Sklearn KFold Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\\n\\nAdditionally, the context indicates that `random_state` is used to ensure the randomness used to shuffle datasets is reproducible, often in conjunction with shuffle parameters. This reproducibility helps achieve consistent results across different runs of your code.\\n\\nFor another detailed example, you can look at:\\n\\n- [Sklearn RandomForestRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\\n\\nThese resources should provide you with comprehensive information about the usage of `random_state` in sklearn.',\n",
       "  'answer_orig': 'Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\nRefer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\\n~~Ella Sahnan~~',\n",
       "  'document': 'dc55359c',\n",
       "  'question': 'Where can I find more information about the use of random_state in sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To obtain precision, recall, F1 score, and accuracy all at once in your classification task, you can use the `classification_report` function from the scikit-learn library. \\n\\nHere's how you can do it:\\n\\n```python\\nfrom sklearn.metrics import classification_report\\n\\n# Assuming y_true are the true labels and y_pred are the predicted labels\\nreport = classification_report(y_true, y_pred)\\nprint(report)\\n```\\n\\nThis function will provide a detailed report including precision, recall, F1 score, and support for each class, as well as an overall accuracy.\\n\\nFor more information, you can refer to the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).\",\n",
       "  'answer_orig': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'document': '2ab49e43',\n",
       "  'question': 'How can I obtain precision, recall, f1 score, and accuracy at once in my classification task?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The function in sklearn that provides various classification metrics simultaneously is `classification_report`.',\n",
       "  'answer_orig': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'document': '2ab49e43',\n",
       "  'question': 'Which function in sklearn provides various classification metrics simultaneously?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can find more information about getting classification metrics in sklearn by using the `classification_report` from sklearn. For more detailed documentation, you can check the official sklearn documentation on this method. Here's a reference on how to get all classification metrics, such as precision, recall, f1 score, and accuracy simultaneously:\\n\\n```python\\nfrom sklearn.metrics import classification_report\\n\\n# Example usage\\nreport = classification_report(y_val, y_pred)\\nprint(report)\\n```\\n\\nAdditionally, sklearn provides specific functions to compute individual metrics:\\n\\n- `accuracy_score`\\n- `precision_score`\\n- `recall_score`\\n- `f1_score`\\n- `roc_auc_score`\\n\\nExamples:\\n```python\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\\n\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\n\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n```\\n\\nFor additional ways to compute these metrics, you can use `precision_recall_fscore_support`:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_pred, zero_division=0)\\n```\\n\\nIf you need visualizations of the classification metrics, consider using the Yellowbrick library:\\n\\n```python\\n# Example code for using Yellowbrick to get a beautiful classification report\\n# from yellowbrick.classifier import ClassificationReport\\n```\\n\\nPlease refer to the sklearn and Yellowbrick documentation for more comprehensive details and examples.\",\n",
       "  'answer_orig': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'document': '2ab49e43',\n",
       "  'question': 'Where can I find more information about getting classification metrics in sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, there is a tool in sklearn to get a report on classification metrics. You can use the `classification_report` function from sklearn. This function gives you a comprehensive report on metrics like precision, recall, f1-score, and accuracy simultaneously.\\n\\nFor more information, you can refer to the relevant documentation on sklearn's website.\\n\\nHere's an example of how to use it:\\n```python\\nfrom sklearn.metrics import classification_report\\n\\n# assuming y_true and y_pred are your ground truth and predicted labels\\nreport = classification_report(y_true, y_pred)\\nprint(report)\\n```\\n\\nThis will provide you with a detailed report on the mentioned classification metrics.\",\n",
       "  'answer_orig': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'document': '2ab49e43',\n",
       "  'question': 'Is there a tool in sklearn to get a report on classification metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information on how to get classification metrics using sklearn was provided by Abhishek N.',\n",
       "  'answer_orig': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'document': '2ab49e43',\n",
       "  'question': 'Who provided the information on how to get classification metrics using sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you get multiple thresholds with the same F1 score, you should choose the threshold that is closest to any of the options you have. Using scikit-learn or other standard libraries/packages can help verify the results. You might consider using “classification_report” from scikit-learn to obtain precision, recall, and F1-score.',\n",
       "  'answer_orig': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'document': 'b431e7eb',\n",
       "  'question': 'In the evaluation metrics, what should I do if I get multiple thresholds with the same F1 score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Based on the context from the FAQ database, it is suggested to \"choose the one closest to any of the options\" when multiple thresholds yield the same F1 score. This implies that among the multiple thresholds, select the one that is nearest to any predefined options or criteria you have in place.',\n",
       "  'answer_orig': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'document': 'b431e7eb',\n",
       "  'question': 'Is it correct to pick the lowest threshold when multiple ones yield the same F1 score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can verify the results of your own code for evaluation metrics by using the scikit-learn library. Scikit-learn provides ready-to-use functions for calculating various evaluation metrics, which are more precise and save time compared to manual calculations. Here is a sample code on how to compute some of these metrics:\\n\\n```python\\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\\n\\n# Assuming y_val contains the true labels and y_pred contains the predicted labels\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\n\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n```\\n\\nThese functions will give you the accuracy, precision, recall, F1 score, and ROC AUC score for your model, helping you to verify the results effectively.\",\n",
       "  'answer_orig': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'document': 'b431e7eb',\n",
       "  'question': 'How can I verify the results of my own code for evaluation metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can use the Scikit-learn library to verify precision, recall, and F1-score. Specifically, the following tools are available:\\n\\n1. `precision_recall_fscore_support`\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\\n\\n2. `precision_score`, `recall_score`, and `f1_score`\\n```python\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\n```\\n\\n3. `classification_report`\\n```python\\nfrom sklearn.metrics import classification_report\\nclassification_report(y_true, y_pred)\\n```\\n\\nThese tools and libraries make it easy to compute and verify these evaluation metrics without manually calculating them.\",\n",
       "  'answer_orig': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'document': 'b431e7eb',\n",
       "  'question': 'Which tools or libraries can be used to verify precision, recall, and F1-score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can find a method to obtain precision, recall, and F1-score using scikit-learn in the following way:\\n\\n1. **Using `precision_recall_fscore_support`**:\\n   ```python\\n   from sklearn.metrics import precision_recall_fscore_support\\n   precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n   ```\\n\\n2. **Using individual scoring functions**:\\n   ```python\\n   from sklearn.metrics import precision_score, recall_score, f1_score\\n   precision = precision_score(y_true, y_pred, average='binary')\\n   recall = recall_score(y_true, y_pred, average='binary')\\n   f1_score = f1_score(y_true, y_pred, average='binary')\\n   ```\\n\\nBoth methods can be used depending on your specific needs and preference for getting these metrics.\",\n",
       "  'answer_orig': 'I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\\nChoose the one closest to any of the options\\nAdded by Azeez Enitan Edunwale\\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\\nAdded by Rileen Sinha',\n",
       "  'document': 'b431e7eb',\n",
       "  'question': 'Where can I find a method to obtain precision, recall, and F1-score using scikit-learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The ValueError indicating that samples of at least 2 classes are needed in the data is caused because the data contains only one class, specifically only 0's. This can happen due to the way the target variable ('churn' in this case) is being processed. For example, if the command `df.churn = (df.churn == 'yes').astype(int)` is executed improperly, it might result in having only 0's in the `churn` column, leading to the error you're seeing. The solution would be to ensure that your target variable contains more than one class before fitting the model.\",\n",
       "  'answer_orig': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'document': 'c5fdeba9',\n",
       "  'question': 'What causes the ValueError indicating samples of at least 2 classes in the data?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The error saying the data contains only one class: 0 occurs because of the following line in your code:\\n\\n```python\\ndf.churn = (df.churn == 'yes').astype(int)\\n```\\n\\nThis line is incorrectly converting the 'churn' column such that it only contains 0's. When a classification model requires at least two classes in your dataset to function properly, having only one class (0 in this case) causes the `ValueError`. You need to ensure that your 'churn' column has a mix of classes (both 0's and 1's) to avoid this error. Adjusting how you process this column should resolve the issue.\",\n",
       "  'answer_orig': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'document': 'c5fdeba9',\n",
       "  'question': 'Why am I getting an error saying the data contains only one class: 0?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The error message about needing samples of at least 2 classes typically means that your dataset for a classification task must include at least two different classes (or categories) in order for the model to properly function and learn to differentiate between them. If you're encountering this error, it's likely that your dataset contains samples from only one class, which makes it impossible for the classification model to train correctly. You'll need to ensure that your training data includes examples from at least two distinct classes.\",\n",
       "  'answer_orig': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'document': 'c5fdeba9',\n",
       "  'question': 'What does the error message about needing samples of at least 2 classes mean?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To solve the `ValueError` related to having only 0's in the churn column, you need to ensure that your data contains samples of at least two classes. This issue can arise if the operation `df.churn = (df.churn == 'yes').astype(int)` results in a column that contains only 0's. You can address this by ensuring that your data has at least some samples that are classified as 1 (indicating churn). Here’s how you can approach this:\\n\\n1. **Check the Data**: Verify whether there are any records in your dataset that should be classified as churn (1). If not, you will need to add or collect more data that includes churn samples.\\n\\n2. **Data Correction**: If the churn column is mistakenly encoded, correct the data to accurately reflect the churn status. For example:\\n   ```python\\n   df.churn = (df.churn == 'yes').astype(int)\\n   ```\\n\\n3. **Balance the Classes**: In case your dataset is heavily imbalanced (with almost all 0's and very few 1's), consider techniques like oversampling the minority class or undersampling the majority class to balance your dataset.\\n\\nBy ensuring that your dataset contains both 0's and 1's in the churn column, you can resolve the ValueError that requires at least two classes in the data.\\n\\nSteps to verify and correct:\\n1. Print the counts of each class:\\n   ```python\\n   print(df['churn'].value_counts())\\n   ```\\n\\n2. If needed, add or correct data to ensure both classes are present:\\n   ```python\\n   # Example: Adding a row with churn = 1 for illustration purposes\\n   else:\\n       new_row = {'churn': 1, ...other column data...}\\n       df = df.append(new_row, ignore_index=True)\\n   ```\\n\\nMake sure to always work with a representative and balanced dataset to avoid such errors.\",\n",
       "  'answer_orig': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'document': 'c5fdeba9',\n",
       "  'question': \"How can I solve the ValueError related to having only 0's in the churn column?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If your churn column contains only 0's and you're receiving a ValueError, the issue is that your data contains only one class (0). This is problematic because many classification algorithms require at least two classes to function properly. \\n\\nYou can resolve this by ensuring that your churn column includes both classes (0 and 1). The issue may stem from a previous operation where you converted the churn column using:\\n\\n```python\\ndf.churn = (df.churn == 'yes').astype(int)\\n```\\n\\nThis conversion can result in a column with only 0's if there are no 'yes' values in the original column. To fix this, you should check your data to make sure it contains both 'yes' and 'no' values before performing the conversion. If it does, ensure the conversion is done correctly.\\n\\nOnce you have both 0's and 1's in the churn column, the ValueError should no longer occur:\\n\\n```python\\ndf.churn = (df.churn == 'yes').astype(int)\\n```\",\n",
       "  'answer_orig': \"Solution description: duplicating the\\ndf.churn = (df.churn == 'yes').astype(int)\\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\\nIt is telling us that it only contains 0's.\\nDelete one of the below cells and you will get the accuracy\\nHumberto Rodriguez\",\n",
       "  'document': 'c5fdeba9',\n",
       "  'question': \"What should I do if my churn column contains only 0's and I'm receiving a ValueError?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Use Yellowbrick. Yellowbrick is a library that combines scikit-learn with matplotlib to produce visualizations for your models, including colorful classification reports.',\n",
       "  'answer_orig': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'document': 'b8c9eaf1',\n",
       "  'question': 'What tool can produce a beautiful classification report?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The library that combines scikit-learn with matplotlib for model visualizations is Yellowbrick.',\n",
       "  'answer_orig': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'document': 'b8c9eaf1',\n",
       "  'question': 'Which library combines scikit-learn with matplotlib for model visualizations?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To generate colorful classification reports, you should use Yellowbrick. Yellowbrick is a library that combines scikit-learn with matplotlib to produce visualizations for your models, including colorful classification reports.',\n",
       "  'answer_orig': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'document': 'b8c9eaf1',\n",
       "  'question': 'How can I generate colorful classification reports?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yellowbrick is used to produce colorful and visually appealing classification reports for your models. It combines scikit-learn with matplotlib to create these visualizations.',\n",
       "  'answer_orig': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'document': 'b8c9eaf1',\n",
       "  'question': 'What is Yellowbrick used for in evaluation metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Krishna Annad mentioned the usage of Yellowbrick for classification reports in the course.',\n",
       "  'answer_orig': 'Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\\nKrishna Annad',\n",
       "  'document': 'b8c9eaf1',\n",
       "  'question': 'Who mentioned the usage of Yellowbrick for classification reports in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you have questions about the course, you can ask them during the live stream as office hours questions are covered, ask them in Slack, or refer to the course FAQ document for frequently asked technical questions. Further information and resources can be found in the course GitHub repository, which includes a link to additional materials: [https://airtable.com/shryxwLd0COOEaqXo](https://airtable.com/shryxwLd0COOEaqXo).',\n",
       "  'answer_orig': 'That’s fine, use the closest option',\n",
       "  'document': 'c54058a1',\n",
       "  'question': 'questions',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find how to use AUC for feature importance by using the `roc_auc_score` function from the `sklearn.metrics` module. You need to pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second argument. This function will return the AUC score for the feature that was passed as the second argument. Here is a brief example:\\n\\n```python\\nfrom sklearn.metrics import roc_auc_score\\n\\n# Example usage:\\nauc_score = roc_auc_score(y_train, df_train[col])\\n```\\n\\nMake sure to pass the target variable and feature values in the correct order to avoid errors.',\n",
       "  'answer_orig': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'document': 'b4b85c4b',\n",
       "  'question': 'Where can I find how to use AUC for feature importance?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should check the solutions from the 2021 iteration of the course for using AUC.',\n",
       "  'answer_orig': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'document': 'b4b85c4b',\n",
       "  'question': \"Which year's course solutions should I check for using AUC?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To evaluate numerical variables using AUC, you can use the `roc_auc_score` function from the `sklearn.metrics` module. You need to pass the target variable vector (e.g., `above_average`) as the first argument (`y_true`) and the vector of feature values as the second argument (`y_score`). This function will return the AUC score for the feature passed as the second argument.\\n\\nHere's a step-by-step guide:\\n\\n1. Import the `roc_auc_score` function:\\n   ```python\\n   from sklearn.metrics import roc_auc_score\\n   ```\\n\\n2. Pass the target variable and the feature values to the function:\\n   ```python\\n   auc_score = roc_auc_score(y_true, feature_values)\\n   ```\\n\\nWhere `y_true` is a vector containing the target variable (e.g., whether a value is above average), and `feature_values` is a vector containing the numerical values of the feature you want to evaluate.\\n\\nFor example:\\n```python\\nauc_score = roc_auc_score(y_train, df_train['feature_column'])\\n```\\n\\nThis will compute and return the AUC score for `'feature_column'`.\",\n",
       "  'answer_orig': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'document': 'b4b85c4b',\n",
       "  'question': 'How do I evaluate numerical variables using AUC?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The recommended metric for evaluating feature importance in numerical variables is the AUC (Area Under the Curve) score. You can calculate this by using the `roc_auc_score` function from the `sklearn.metrics` module. Pass the vector of the target variable (e.g., 'above_average') as the first argument and the vector of feature values as the second argument. This function will return the AUC score for the given numerical feature.\",\n",
       "  'answer_orig': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'document': 'b4b85c4b',\n",
       "  'question': 'What metric is recommended for feature importance in numerical variables?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should use the `roc_auc_score` function for AUC evaluation in this course. This function can be found in the `sklearn.metrics` module.',\n",
       "  'answer_orig': 'Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.',\n",
       "  'document': 'b4b85c4b',\n",
       "  'question': 'What function should I use for AUC evaluation in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To compute the Area Under the Curve (AUC) using numerical values, you can utilize the `roc_auc_score` function from the `sklearn.metrics` module. Here are the steps to follow:\\n\\n1. **Prepare your data**: Ensure you have two vectors ready:\\n   - `y_true`: The true binary labels or target values.\\n   - `y_score`: The predicted scores or numerical values from your model for each instance.\\n\\n2. **Compute AUC**: Pass these vectors to the `roc_auc_score` function. The true labels should be the first argument, and the predicted scores should be the second argument.\\n\\nHere is an example in Python code:\\n\\n```python\\nfrom sklearn.metrics import roc_auc_score\\n\\n# Assuming y_true is your target variable and y_score is your numerical score/predicted values\\ny_true = [0, 1, 0, 1, 1]  # Example true values\\ny_score = [0.1, 0.4, 0.35, 0.8, 0.9]  # Example predicted scores\\n\\n# Compute AUC\\nauc = roc_auc_score(y_true, y_score)\\n\\nprint(\"AUC Score:\", auc)\\n```\\n\\nBy following this process, you can compute the AUC for any set of numerical predictions against the true labels.\\n\\nMake sure to pass the correct arguments in the right order to avoid errors, as outlined by the FAQs:\\n- `roc_auc_score(y_true, y_score)`',\n",
       "  'answer_orig': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'document': '7d40f6f6',\n",
       "  'question': 'Can you explain how to compute AUC using numerical values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"When calculating the ROC AUC score using `sklearn.metrics.roc_auc_score`, you'll need to use two parameters: `y_true` and `y_score`. \\n\\n- `y_true` should be the actual target variable (the true labels).\\n- `y_score` should be the score or predicted probability for each instance.\\n\\nFor each numerical value in the dataframe you intend to evaluate, you should pass it as the `y_score` to the `roc_auc_score` function while passing the target variable as `y_true`.\\n\\nFor example:\\n\\n```python\\nroc_auc_score(y_true, y_score)\\n```\\n\\nMake sure to pass `y_true` as the first argument and `y_score` as the second argument to avoid errors.\",\n",
       "  'answer_orig': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'document': '7d40f6f6',\n",
       "  'question': 'How should y_true and y_score be used when calculating ROC AUC?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The function `sklearn.metrics.roc_auc_score` expects two parameters: \\n\\n1. `y_true`: The true binary labels.\\n2. `y_score`: Target scores, which can be either probability estimates of the positive class, confidence values, or binary decisions.\\n\\nIn practice, you should pass the vector of the true target values (`y_true`) as the first argument and the vector of predicted scores or probabilities (`y_score`) as the second argument.',\n",
       "  'answer_orig': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'document': '7d40f6f6',\n",
       "  'question': 'What parameters does sklearn.metrics.roc_auc_score expect?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To clarify the use of `y_score` in the ROC AUC calculation:\\n\\nIn the process of calculating the ROC AUC score using the function `sklearn.metrics.roc_auc_score`, the function expects two parameters: `y_true` and `y_score`.\\n\\n- **y_true**: This parameter should be the actual target values (i.e., the true labels) from your dataset.\\n- **y_score**: This parameter is the score or probability estimates predicted by your model. Essentially, for each numerical value in your dataframe, it will be passed as the `y_score` to the function, and the actual target variable will be passed as `y_true`.\\n\\nIn other words, `y_score` represents the predicted scores, which might be continuous-valued probability estimates, and `y_true` represents the actual labels of the data points.\\n\\nThis approach helps in determining how well the classifier can distinguish between the classes, making ROC AUC a valuable metric for evaluating binary classification models.\\n\\nFor example, if your predictions are `y_pred`, the correct function call would be:\\n```python\\nroc_auc_score(y_train, y_pred)\\n```\\nwhere `y_train` is your actual labels and `y_pred` is the prediction scores.\\n\\nReference:\\n- \"When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters \\'y_true\\' and \\'y_score\\'.\" (Sylvia Schmitt)',\n",
       "  'answer_orig': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'document': '7d40f6f6',\n",
       "  'question': 'Can you clarify the use of y_score in the ROC AUC calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In calculating the AUC score with `sklearn`, the role of `y_true` is to serve as the target variable. It represents the true class labels of the data points. When using the `roc_auc_score` function from `sklearn.metrics`, you need to pass `y_true` as the first argument and the predicted scores (`y_score`) as the second argument. The AUC score is then computed based on these inputs.\\n\\nSo, for each numerical value used as a score, it will be passed to the `roc_auc_score` function as `y_score`, while the target variable is passed as `y_true`.\\n\\nFor example:\\n```python\\nfrom sklearn.metrics import roc_auc_score\\n\\nauc_score = roc_auc_score(y_true, y_score)\\n```\\nHere, `y_true` contains the actual class labels and `y_score` contains the predicted scores.',\n",
       "  'answer_orig': 'When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\\nSylvia Schmitt',\n",
       "  'document': '7d40f6f6',\n",
       "  'question': 'What is the role of y_true in calculating the AUC score with sklearn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The dataset required to calculate metrics in Question 3 is the `dt_val` dataset.',\n",
       "  'answer_orig': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'document': 'f5dc446c',\n",
       "  'question': 'Which dataset is required to calculate metrics in Question 3?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards.',\n",
       "  'answer_orig': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'document': 'f5dc446c',\n",
       "  'question': 'When finding metrics in Question 3, what dataset do I use?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards.',\n",
       "  'answer_orig': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'document': 'f5dc446c',\n",
       "  'question': 'What is the dataset for computing Question 3 metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To compute the metrics for Question 3, you should use the `dt_val` dataset, as specified.',\n",
       "  'answer_orig': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'document': 'f5dc446c',\n",
       "  'question': 'To compute metrics for Question 3, which dataset should be used?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The dataset essential for calculating metrics in Question 3 is the `dt_val` dataset.',\n",
       "  'answer_orig': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'document': 'f5dc446c',\n",
       "  'question': 'For calculating metrics in Question 3, which dataset is essential?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The `KFold` function in the context of `n_splits`, `shuffle`, and `random_state` creates a generator object that is used to split your dataset into `n_splits` number of folds. Here is a breakdown of what each parameter does:\\n\\n- `n_splits`: This specifies the number of folds (or splits) to create in the dataset. For instance, if `n_splits=5`, the dataset will be divided into 5 different folds.\\n- `shuffle`: When set to `True`, the dataset will be shuffled before being split into folds. This can help ensure that the folds are more representative of the overall dataset.\\n- `random_state`: This acts as a seed to the random number generator used when `shuffle` is `True`. This ensures that the shuffling is reproducible; the same seed will always result in the same shuffle.\\n\\nThe KFold function does not perform the actual split when it is called. It merely sets up the parameters for the split. The actual splitting into training and validation sets happens in the loop where the KFold's `split` method is called. Hence, it doesn't matter whether you instantiate `KFold` before or inside a loop, as long as the parameters remain unchanged, the splits will be consistent.\\n\\nUsing a consistent `random_state` ensures that the splits are reproducible, which is crucial for comparing model performance across different runs.\",\n",
       "  'answer_orig': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'document': 'd30fc29d',\n",
       "  'question': 'What does the KFold function do in the context of n_splits, shuffle, and random_state?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The placement of KFold inside or outside the loop does not affect the results in HW04, Q6. This is because KFold is simply a generator object that holds the parameters `n_splits`, `shuffle`, and `random_state`, and the actual k-fold splitting of the dataset occurs in the subsequent loop `for train_idx, val_idx in kFold.split(df_full_train):`. Thus, it will generate the same splits regardless of whether it is placed inside or outside the loop. However, from a programming efficiency standpoint, it is recommended to place KFold outside the loop to avoid creating the object multiple times.',\n",
       "  'answer_orig': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'document': 'd30fc29d',\n",
       "  'question': 'Does the placement of KFold inside or outside the loop affect the results in HW04, Q6?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Changing the `random_state` parameter in KFold affects the results because it alters the randomness used to shuffle the dataset. This randomness affects how the data is split into training and validation sets. Ensuring you set a specific `random_state` allows for reproducibility in experiments, meaning each time you run your code, the data splits will be the same. Conversely, changing the `random_state` value will result in different train/validation splits, leading to variations in the results of your model.',\n",
       "  'answer_orig': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'document': 'd30fc29d',\n",
       "  'question': 'Why might changing the random_state parameter in KFold affect the results?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Creating the KFold generator object before the loop is better from a programming efficiency perspective. Here's why:\\n\\nThe KFold generator object contains the parameters `n_splits`, `shuffle`, and `random_state`, but the actual k-fold splitting occurs within the loop when calling `kFold.split(df_full_train)`. Thus, generating the KFold object inside the loop repeatedly is redundant, as it carries the same configuration information each time it is created. By creating the KFold object before the loop, you avoid unnecessary repetition and potentially improve the readability and efficiency of your code. \\n\\nIn summary, while it doesn't affect the results in terms of the split generated, doing this setup outside the loop is a more efficient practice.\",\n",
       "  'answer_orig': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'document': 'd30fc29d',\n",
       "  'question': 'Why is it better to create the KFold generator object before the loop rather than inside it?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The loop should iterate through the different C values because C is a hyperparameter that affects the regularization of the model. As described in the CONTEXT, smaller C values introduce more regularization, leading to a simpler decision boundary and potentially underfitting the data, whereas larger C values reduce regularization, allowing the model to fit the training data more closely and potentially overfitting. Cross-validation is used to evaluate the performance of the model and choose the best hyperparameters.\\n\\nBy iterating through different C values and using KFold for cross-validation, you can systematically evaluate how each value of C impacts the model's performance. This helps in selecting the C value that results in the best performance based on the evaluation metrics derived from the cross-validation process. So, the loop over different C values ensures that you properly assess the impact of various degrees of regularization on your model, ultimately aiding in finding the most optimal hyperparameter settings.\",\n",
       "  'answer_orig': \"What does this line do?\\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\nIn my case changing random state changed results\\n(Arthur Minakhmetov)\\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\\n(Bhaskar Sarma)\\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\\n(Ani Mkrtumyan)\",\n",
       "  'document': 'd30fc29d',\n",
       "  'question': 'In the context of KFold and different C values, why should the loop iterate through the C values as described in the video?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You are getting a \"ValueError: multi_class must be in (\\'ovo\\', \\'ovr\\')\" because you are passing the parameters to the `roc_auc_score` function incorrectly. Instead of passing `df_train[col]` and `y_train` separately, you should pass the target variable as the first argument and the feature values as the second argument, like this:\\n\\n```python\\nroc_auc_score(y_train, df_train[col])\\n```\\n\\nMake sure to follow this order to avoid the ValueError.',\n",
       "  'answer_orig': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'document': '8eca9f73',\n",
       "  'question': \"Why do I get a ValueError: multi_class must be in ('ovo', 'ovr') when evaluating feature importance with roc_auc_score?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To avoid the multi_class ValueError when using `roc_auc_score`, you should ensure that the parameters are passed in the correct order. Specifically, you should pass the target variable (e.g., `y_train`) as the first argument and the feature values (e.g., `df_train[col]`) as the second argument. The correct way to call the function is:\\n\\n```python\\nroc_auc_score(y_train, df_train[col])\\n```\\n\\nThis method guarantees that the function receives the parameters in the expected format and avoids the `multi_class` ValueError.',\n",
       "  'answer_orig': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'document': '8eca9f73',\n",
       "  'question': 'How should I correctly pass parameters to roc_auc_score to avoid the multi_class ValueError?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The error 'multi_class must be in ('ovo', 'ovr')' occurs because you are passing the parameters to the `roc_auc_score` function incorrectly. The correct way to pass the parameters is `roc_auc_score(y_train, df_train[col])`, where `y_train` is the target variable and `df_train[col]` is the feature column. This error likely arises from reversing the order of the arguments. \\n\\nAsia Saeed\",\n",
       "  'answer_orig': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'document': '8eca9f73',\n",
       "  'question': \"What causes the error 'multi_class must be in ('ovo', 'ovr')' while using roc_auc_score in question 1?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To avoid errors when using `roc_auc_score` from `sklearn.metrics`, the correct parameter order is crucial. You should pass the vector of the target variable (referred to as `y_true`) as the first argument and the vector of feature values or predicted scores (referred to as `y_score`) as the second argument. Here is an example:\\n\\n```python\\nroc_auc_score(y_true, y_score)\\n```\\n\\nIn a practical scenario, if you are using data from, say, a training set, the call would look something like this:\\n\\n```python\\nroc_auc_score(y_train, df_train[col])\\n```\\n\\nMake sure to pass `y_train` (the actual labels) as `y_true` and the feature values (e.g., `df_train[col]`) as `y_score`.',\n",
       "  'answer_orig': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'document': '8eca9f73',\n",
       "  'question': 'Can you explain the correct parameter order for roc_auc_score to avoid errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The proper syntax for using `roc_auc_score` with `y_train` and `df_train` in question 1 is:\\n\\n```python\\nroc_auc_score(y_train, df_train[col])\\n```\\n\\nThis ensures that the target variable `y_train` is passed as the first argument (`y_true`) and the numerical feature `df_train[col]` is passed as the second argument (`y_score`).',\n",
       "  'answer_orig': \"I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\\nAsia Saeed\",\n",
       "  'document': '8eca9f73',\n",
       "  'question': 'What is the proper syntax for using roc_auc_score with y_train and df_train in question 1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can monitor wait times and code execution progress using the `tqdm` library. Here's how you can do it:\\n\\n```python\\nfrom tqdm.auto import tqdm\\n```\\n\\nTqdm provides a terminal progress bar that can help you visualize the progress of your code execution.\",\n",
       "  'answer_orig': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'document': '7b9eb7f7',\n",
       "  'question': 'How can I monitor wait times and code execution progress?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To track code execution in Python, you can use the `tqdm` module. Specifically, you can import it using:\\n\\n```python\\nfrom tqdm.auto import tqdm\\n```\\n\\nThis will allow you to monitor wait times and progress of your code execution with a terminal progress bar.',\n",
       "  'answer_orig': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'document': '7b9eb7f7',\n",
       "  'question': 'What module is imported for tracking code execution in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The library that provides a terminal progress bar is `tqdm`. Specifically, you can monitor wait times and progress of code execution with `from tqdm.auto import tqdm`.',\n",
       "  'answer_orig': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'document': '7b9eb7f7',\n",
       "  'question': 'Which library provides a terminal progress bar?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about monitoring wait times was provided by Krishna Anand.',\n",
       "  'answer_orig': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'document': '7b9eb7f7',\n",
       "  'question': 'Who provided the information about monitoring wait times?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To import `tqdm` for auto progress monitoring, use the following command:\\n\\n```python\\nfrom tqdm.auto import tqdm\\n```',\n",
       "  'answer_orig': 'from tqdm.auto import tqdm\\nTqdm - terminal progress bar\\nKrishna Anand',\n",
       "  'document': '7b9eb7f7',\n",
       "  'question': 'What is the command to import tqdm for auto progress monitoring?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you have additional questions related to the course that are not covered in the FAQ, please use the Slack channel to ask them. This platform is available for course-related inquiries and ensures you get swift responses either from the instructor or fellow students.',\n",
       "  'answer_orig': 'Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\\nAileah Gotladera',\n",
       "  'document': 'c4aaeed9',\n",
       "  'question': 'questions',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The difference between using `predict(X)` and `predict_proba(X)[:, 1]` for classification is as follows:\\n\\n- `predict(X)`: Returns the binary classification predictions, which are either 0 or 1. This approach directly provides the class labels based on the model's decision threshold (usually 0.5). Using this method may result in less precise evaluation metrics because it does not provide the confidence levels of the predictions.\\n\\n- `predict_proba(X)[:, 1]`: Returns the probabilities that the instance belongs to one of the classes (class 1 in this case). This method provides the likelihood or confidence that a particular instance belongs to class 1. Using these probability values allows for more nuanced evaluation metrics and better understanding of the model's certainty regarding its predictions.\\n\\nThus, `predict(X)` gives you the finalized class labels, while `predict_proba(X)[:, 1]` gives you the probability estimates for class 1.\",\n",
       "  'answer_orig': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'document': '3af31e2a',\n",
       "  'question': 'What is the difference between using predict(X) and predict_proba(X)[:, 1] for classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Using `predict(X)` for evaluation may lead to incorrect values because it provides binary classification predictions (0 or 1), which might not capture the nuances necessary for accurate evaluation. To get better evaluation values, you should use `predict_proba(X)[:,1]`, which gives the probability that the value belongs to one of the classes, offering a more fine-grained and informative metric for evaluation.',\n",
       "  'answer_orig': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'document': '3af31e2a',\n",
       "  'question': 'Why might using predict(X) lead to incorrect evaluation values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In a classification task, `predict_proba(X)[:, 1]` returns the probabilities that the samples in `X` belong to class 1. This gives you the likelihood of each sample being classified as class 1, which can be useful for evaluating or making decisions based on the confidence of the predictions rather than just the binary outcome.',\n",
       "  'answer_orig': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'document': '3af31e2a',\n",
       "  'question': 'What does predict_proba(X)[:, 1] return in a classification task?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'According to the context, it was Vladimir Yesipov who explained that predict_proba shows probabilities per class.',\n",
       "  'answer_orig': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'document': '3af31e2a',\n",
       "  'question': 'Who explained that predict_proba shows probabilities per class?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution to use `predict_proba(X)[:,1]` instead of `predict(X)` was provided by Vladimir Yesipov.',\n",
       "  'answer_orig': 'In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\\nVladimir Yesipov\\nPredict_proba shows probailites per class.\\nAni Mkrtumyan',\n",
       "  'document': '3af31e2a',\n",
       "  'question': 'Who provided the solution to use predict_proba(X)[:,1] instead of predict(X)?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When the threshold is set to 1.0, both the False Positive Rate (FPR) and the True Positive Rate (TPR) are equal to 0.0. This occurs because the condition for a prediction to belong to the positive class (e.g., churn) is \\\\( g(x) \\\\geq 1.0 \\\\). However, since \\\\( g(x) \\\\) is a sigmoid function with values between 0 and 1, it never actually reaches 1.0. Consequently, no examples satisfy the condition \\\\( g(x) \\\\geq 1.0 \\\\), resulting in no positive predictions being made (neither true positives nor false positives). Therefore, both the FPR and TPR are 0.0.',\n",
       "  'answer_orig': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'document': '746342ff',\n",
       "  'question': 'What happens to FPR and TPR when the threshold is set to 1.0?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"When the threshold is set to 1.0 in a binary classification problem using a sigmoid function, the function's output values range between 0 and 1, but never actually reach 1. This means that no instance's predicted value will satisfy the condition \\\\( g(x) \\\\geq 1.0 \\\\). As a result, no instances are classified as positive (churn), meaning there are no true positives (TP) or false positives (FP). Consequently, both the True Positive Rate (TPR) and the False Positive Rate (FPR) are 0.0, as there are no positive predictions to consider.\",\n",
       "  'answer_orig': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'document': '746342ff',\n",
       "  'question': 'Why does a threshold of 1.0 result in FPR being 0.0?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When the threshold is set to 1.0, the condition for being classified as part of the positive class (in this case, the churn class) is that the output of the classifier, denoted as g(x), must be greater than or equal to 1.0. However, g(x) is a sigmoid function, which in the context of binary classification produces values strictly between 0 and 1. Since the sigmoid function never actually reaches the value of 1.0, no data points will meet this condition.\\n\\nAs a result, no positive classifications are made—meaning there are no true positives (TP) or false positives (FP). Consequently, the True Positive Rate (TPR), which is the ratio of TP to the sum of TP and false negatives (FN), is 0.0 because TP is zero. Similarly, the False Positive Rate (FPR) is also 0.0. This explains why a threshold of 1.0 results in a TPR of 0.0.',\n",
       "  'answer_orig': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'document': '746342ff',\n",
       "  'question': 'Why does a threshold of 1.0 result in TPR being 0.0?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'There are no positive predicted values when the threshold is 1.0 in a churn prediction model because the sigmoid function used in binary classification outputs values strictly between 0 and 1, and never reaches 1.0. Therefore, no instance can meet the condition for belonging to the positive class (churn class), which is \\\\( g(x) \\\\geq 1.0 \\\\). Consequently, there are no positive predicted values (neither true positive nor false positive) at this threshold.',\n",
       "  'answer_orig': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'document': '746342ff',\n",
       "  'question': 'Why are there no positive predicted values when the threshold is 1.0 in a churn prediction model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When we say the sigmoid function never reaches 0 or 1, it means that the values produced by the sigmoid function \\\\(\\\\sigma(z) = \\\\frac{1}{1 + e^{-z}}\\\\) always lie strictly within the range (0, 1). In other words, the sigmoid function asymptotically approaches 0 as the input \\\\(z\\\\) approaches negative infinity, and it asymptotically approaches 1 as \\\\(z\\\\) approaches positive infinity, but it never actually reaches these exact values. Therefore, when setting the threshold for classification to exactly 1.0, the output \\\\(\\\\sigma(z)\\\\) will never meet this condition, leading to no data points being classified as positive (churn) and consequently, both the False Positive Rate (FPR) and True Positive Rate (TPR) will be 0.0.',\n",
       "  'answer_orig': 'For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\\nThe threshold is 1.0\\nFPR is 0.0\\nAnd TPR is 0.0\\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\\nAlena Kniazeva',\n",
       "  'document': '746342ff',\n",
       "  'question': 'What does it mean when we say the sigmoid function never reaches 0 or 1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Matplotlib offers the `annotate` method for annotating a graph. You can provide an X,Y point and annotate with an arrow and text. For example, the following code will show an arrow pointing to the (x, y) point specified:\\n\\n```python\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\n             xy=(optimal_threshold, optimal_f1_score),\\n             xytext=(0.3, 0.5),\\n             textcoords='axes fraction',\\n             arrowprops=dict(facecolor='black', shrink=0.05))\\n```\\n\\nThis method is useful for adding detailed annotations to highlight important data points in your plots.\",\n",
       "  'answer_orig': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'document': 'bda2c9b3',\n",
       "  'question': 'What method does Matplotlib offer for annotating a graph?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Certainly! Here's an example of how to annotate a graph in Matplotlib:\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# Example values for the optimal threshold and F1 score\\noptimal_threshold = 0.4\\noptimal_f1_score = 0.75\\n\\n# Annotating the graph with an arrow pointing to the (optimal_threshold, optimal_f1_score) point\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\n             xy=(optimal_threshold, optimal_f1_score), # Point to annotate\\n             xytext=(0.3, 0.5), # Annotation location (fraction of the axes)\\n             textcoords='axes fraction',\\n             arrowprops=dict(facecolor='black', shrink=0.05)) # Arrow properties\\n\\n# Dummy data for plotting\\nthresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\\nf1_scores = [0.5, 0.6, 0.65, 0.75, 0.7]\\n\\n# Plotting the data\\nplt.plot(thresholds, f1_scores, label='F1 Score vs Threshold')\\nplt.xlabel('Threshold')\\nplt.ylabel('F1 Score')\\nplt.legend()\\nplt.show()\\n```\\n\\nThis code will create a plot with an arrow pointing to the `optimal_threshold` and `optimal_f1_score` point, annotated with the threshold value and F1 score.\",\n",
       "  'answer_orig': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'document': 'bda2c9b3',\n",
       "  'question': 'Can you give an example of how to annotate a graph in Matplotlib?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To annotate a graph with an arrow and text in Matplotlib, you can use the `annotate` method. Here is an example that demonstrates how to annotate a graph by pointing an arrow to specific (x, y) coordinates and including a text annotation:\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# Example values\\noptimal_threshold = 0.5\\noptimal_f1_score = 0.8\\n\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\n             xy=(optimal_threshold, optimal_f1_score),\\n             xytext=(0.3, 0.5),\\n             textcoords='axes fraction',\\n             arrowprops=dict(facecolor='black', shrink=0.05))\\n\\n# Example plot (you can replace this part with your actual data)\\nplt.plot([0.1, 0.2, 0.3, 0.4, 0.5], [0.5, 0.6, 0.7, 0.8, 0.8])\\nplt.xlabel('Threshold')\\nplt.ylabel('F1 Score')\\nplt.title('Threshold vs F1 Score')\\nplt.show()\\n```\\n\\nIn this example:\\n- `xy=(optimal_threshold, optimal_f1_score)` specifies the point to annotate.\\n- `xytext=(0.3, 0.5)` specifies the position of the text annotation.\\n- `textcoords='axes fraction'` interprets xytext as a fraction of the axes dimensions.\\n- `arrowprops=dict(facecolor='black', shrink=0.05)` customizes the arrow, such as its color and shrink factor.\",\n",
       "  'answer_orig': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'document': 'bda2c9b3',\n",
       "  'question': 'How can I use Matplotlib to annotate with an arrow and text?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"An optimal way to annotate the optimal threshold and F1 score on a graph is to use Matplotlib's annotation method. Here is an example of how you can do it:\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# Assuming optimal_threshold and optimal_f1_score are defined\\nplt.annotate(\\n    f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\n    xy=(optimal_threshold, optimal_f1_score),\\n    xytext=(0.3, 0.5),\\n    textcoords='axes fraction',\\n    arrowprops=dict(facecolor='black', shrink=0.05)\\n)\\n```\\n\\nThis will show an arrow pointing to the `x,y` point of the optimal threshold and display the values of the optimal threshold and F1 score.\",\n",
       "  'answer_orig': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'document': 'bda2c9b3',\n",
       "  'question': 'What is an optimal way to annotate the optimal threshold and F1 score on a graph?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To annotate a graph in Matplotlib, you need to provide the following parameters:\\n\\n1. `s` (string): The annotation text.\\n2. `xy` (tuple of float): The point (x, y) to annotate.\\n3. `xytext` (tuple of float): The position (x, y) to place the text.\\n4. `textcoords` (string or object): The coordinate system for `xytext`.\\n5. `arrowprops` (dict): Properties for the arrow, such as `facecolor` and `shrink`.\\n\\nHere is an example with these parameters:\\n\\n```python\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\n             xy=(optimal_threshold, optimal_f1_score),\\n             xytext=(0.3, 0.5),\\n             textcoords='axes fraction',\\n             arrowprops=dict(facecolor='black', shrink=0.05))\\n```\",\n",
       "  'answer_orig': \"Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\\\nOptimal F1 Score: {optimal_f1_score:.2f}',\\nxy=(optimal_threshold, optimal_f1_score),\\nxytext=(0.3, 0.5),\\ntextcoords='axes fraction',\\narrowprops=dict(facecolor='black', shrink=0.05))\\nQuinn Avila\",\n",
       "  'document': 'bda2c9b3',\n",
       "  'question': 'What parameters are needed to annotate a graph in Matplotlib?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The ROC AUC (Receiver Operating Characteristic Area Under the Curve) is important in Binary Classification models because it is one of the most vital metrics used to evaluate the performance of such models. The ROC AUC score helps to measure the ability of a classifier to distinguish between positive and negative classes. It provides a comprehensive view of the trade-off between the true positive rate and the false positive rate across different threshold settings. This makes it a valuable tool for evaluating models, especially when the dataset is balanced between each class.',\n",
       "  'answer_orig': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'document': '41521c92',\n",
       "  'question': 'Why is the ROC AUC important in Binary Classification models?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is recommended to rewatch the video on the ROC curve. The ROC AUC is one of the most important metrics used in Binary Classification models, and rewatching the video or exploring additional resources could be beneficial for your understanding.',\n",
       "  'answer_orig': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'document': '41521c92',\n",
       "  'question': 'Is it recommended to rewatch the video on the ROC curve?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can proceed with the course without fully understanding the ROC curve. However, since the ROC AUC is an important metric in binary classification models, it might be helpful for you to revisit the topic later. You could rewatch the course video on this topic or seek additional resources from other lectures or notes to strengthen your understanding.',\n",
       "  'answer_orig': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'document': '41521c92',\n",
       "  'question': 'Can I proceed with the course without fully understanding the ROC curve?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To improve your understanding of the ROC curve, here are some effective strategies:\\n\\n1. **Rewatch the Course Video**: Since the ROC curve is complex and abstract, it might help to rewatch the video lecture from your course. Repetition can often lead to new insights.\\n\\n2. **Seek Additional Resources**: Consider watching videos or lectures, or reading notes by other experts on the topic. Different explanations and perspectives might clarify points that were confusing initially.\\n\\n3. **Understand When to Use ROC Curves**: Understand the contexts in which ROC curves are more appropriate compared to Precision-Recall curves. For instance, ROC curves are more suited for balanced datasets, whereas Precision-Recall curves are better for imbalanced datasets. Knowing this can give you a better conceptual foundation.\\n\\n4. **Practical Application and Computation**: Try computing the ROC AUC score using practical examples. Use tools like `sklearn.metrics.roc_auc_score` to pass different sets of actual and predicted values. Familiarizing yourself with the function parameters (`y_true` and `y_score`) and ensuring you're passing the correct arguments can deepen your understanding.\\n\\n5. **Linking Theory and Practice**: Pay attention to how the ROC curve uses true positives (TP) and false positives (FP) to assess model performance. Working through scenarios where you manually compute these rates might make the abstract concept more concrete.\\n\\nBy combining these strategies, you will likely find your understanding of the ROC curve improving over time.\",\n",
       "  'answer_orig': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'document': '41521c92',\n",
       "  'question': 'What are some ways I can improve my understanding of the ROC curve?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it might be useful for you to watch other resources to grasp the ROC AUC concept. Although you can move on without fully understanding it, the ROC AUC is one of the most important metrics used in Binary Classification models. Therefore, rewatching the course video or watching videos, lectures, or notes from other sources can help you better understand this complex and abstract topic.',\n",
       "  'answer_orig': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'document': '41521c92',\n",
       "  'question': 'Should I watch other resources to grasp the ROC AUC concept?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Your accuracy may differ from the homework options due to the way you split the data. The method of splitting data can affect the results even if the overall train/validation/test ratios are the same. For example, splitting the data as follows:\\n\\n1) \\n```python\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n```\\n2)\\n```python\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\n```\\n\\nBoth methods result in the same ratio of 60% train, 20% validation, and 20% test data, but the actual composition of the data in each split may be slightly different. This can lead to variations in accuracy. It is recommended to use the second method (the one aligned with the lessons) to maintain consistency with the homework.',\n",
       "  'answer_orig': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'document': '25481ce5',\n",
       "  'question': 'Why does my accuracy differ from the homework options?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One main reason for the different accuracy values is the way the data is split into train, validation, and test sets. For example, consider two data splitting methods that end up with the same ratio (60% train, 20% validation, 20% test):\\n\\n1. First, split the data into a training set and a temporary set with a ratio of 60%/40%, then further split the temporary set into validation and test sets with a ratio of 50%/50%.\\n2. First, split the data into a full training set and a test set with a ratio of 80%/20%, then further split the full training set into training and validation sets with a ratio of 75%/25%.\\n\\nAlthough both methods end up with the same final ratios, the underlying data composition in each set is slightly different, which can lead to different accuracy values.',\n",
       "  'answer_orig': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'document': '25481ce5',\n",
       "  'question': 'What is one main reason for different accuracy values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When splitting data into train, validation, and test sets, it is crucial to do the splitting before performing preprocessing tasks such as filling in missing values or one-hot encoding to avoid data leakage. This means performing data preprocessing like one-hot encoding *after* splitting the data. Here are the steps to properly split the data:\\n\\n1. **Initial Split for Test Data**: Split the data into training+validation set and test set. For example:\\n   ```python\\n   df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\n   ```\\n\\n2. **Split Training+Validation Data**: Further split the training+validation set into individual training and validation sets. For example:\\n   ```python\\n   df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\n   ```\\n\\nBy following this method, you ensure a clear separation between training, validation, and test sets, reducing the risk of data leakage and ensuring that the test set remains an unbiased representation of future unseen data. Additionally, performing exploratory data analysis (EDA) should be based primarily on the training dataset to prevent any influence from the validation or test sets.',\n",
       "  'answer_orig': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'document': '25481ce5',\n",
       "  'question': 'How should data be split into train, validation, and test sets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The recommended data splitting method is the second one, which is more consistent with the lessons and homeworks. This method involves:\\n\\n1. Splitting the full dataset into training and test sets:\\n   ```python\\n   df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\n   ```\\n\\n2. Splitting the training set further into training and validation sets:\\n   ```python\\n   df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\n   ```\\n\\nThis approach ensures that your training, validation, and test sets are correctly partitioned and is less prone to data leakage.',\n",
       "  'answer_orig': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'document': '25481ce5',\n",
       "  'question': 'Which data splitting method is recommended?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The two data splitting results are different because the methods used to achieve the splits handle the data partitioning in different ways, leading to slightly different datasets for training, validation, and testing even though the overall ratios remain the same. \\n\\nIn the first method:\\n1. `df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)`\\n2. `df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)`\\n\\nThis approach first removes 60% of the data for training and then splits the remaining 40% evenly between validation and test sets.\\n\\nIn the second method:\\n1. `df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)`\\n2. `df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)`\\n\\nHere, 20% of the total data is first set aside for testing, and the remaining 80% is then split into training and validation sets in a way that 20% of the original data constitutes the validation set.\\n\\nEven though both methods aim for a 60%/20%/20% training/validation/testing split, the intermediate divisions cause the data partitions to differ slightly. Therefore, it is recommended to use the second method as it ensures consistency with the lessons and homeworks.',\n",
       "  'answer_orig': 'One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\\n1)\\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\\n2)\\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\\nIbraheem Taha',\n",
       "  'document': '25481ce5',\n",
       "  'question': 'Why are the two data splitting results different?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find the intercept between precision and recall curves using numpy, you can follow these steps:\\n\\n1. Ensure your dataframe (let\\'s call it `df_scores`) contains the columns `threshold`, `precision`, and `recall`.\\n2. Compute the difference between the precision and recall values.\\n3. Determine where the sign of this difference changes, which indicates an intercept.\\n\\nHere\\'s a snippet of code to achieve this:\\n\\n```python\\nimport numpy as np\\n\\n# Assuming df_scores is your dataframe containing columns `threshold`, `precision`, and `recall`\\n# Calculate the index at which the precision and recall curves intersect\\nidx = np.argwhere(\\n    np.diff(\\n        np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n    )\\n).flatten()\\n\\n# Print the result\\nprint(f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\")\\n```\\n\\nThis code will output the threshold at which the precision and recall curves intersect.',\n",
       "  'answer_orig': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'document': '1427d567',\n",
       "  'question': 'What is the method to find the intercept between precision and recall curves using numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find the intercept between precision and recall curves, you can use the `numpy.diff` and `numpy.sign` functions. Here\\'s how you can do it:\\n\\n1. **Prepare your data**: Ensure you have a dataframe (`df_scores`) with the columns `\\'threshold\\'`, `\\'precision\\'`, and `\\'recall\\'`.\\n\\n2. **Compute the indices where precision and recall intersect**:\\n   ```python\\n   import numpy as np\\n\\n   idx = np.argwhere(\\n       np.diff(\\n           np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n       )\\n   ).flatten()\\n   ```\\n\\n3. **Print the result** to easily read the indices where the intercept occurs:\\n   ```python\\n   print(\\n       f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n   )\\n   ```\\n\\n(Source: Mélanie Fouesnard)',\n",
       "  'answer_orig': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'document': '1427d567',\n",
       "  'question': 'Which numpy functions are necessary to find the intercept between precision and recall curves?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To determine the index of the intercept between precision and recall in a DataFrame, you can use the `numpy` library. Here\\'s a step-by-step method to find where the precision and recall curves intersect:\\n\\n1. Ensure your DataFrame `df_scores` has the columns \\'threshold\\', \\'precision\\', and \\'recall\\'.\\n2. Use `numpy.diff` and `numpy.sign` functions to determine where the sign of the difference between precision and recall changes.\\n\\nHere is the code you can follow:\\n\\n```python\\nimport numpy as np\\n\\n# Assume df_scores is your DataFrame with \\'threshold\\', \\'precision\\', and \\'recall\\' columns\\n# Convert the \\'precision\\' and \\'recall\\' columns to numpy arrays\\nprecision_array = np.array(df_scores[\"precision\"])\\nrecall_array = np.array(df_scores[\"recall\"])\\n\\n# Compute the difference and find where the sign changes\\nidx = np.argwhere(\\n    np.diff(np.sign(precision_array - recall_array))\\n).flatten()\\n\\n# Print the result\\nprint(f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\")\\n```\\n\\nThis script will output the index where the precision and recall curves intersect in the DataFrame.',\n",
       "  'answer_orig': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'document': '1427d567',\n",
       "  'question': 'How can I determine the index of the intercept between precision and recall in a DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The role of `np.sign` in determining the intercept between precision and recall curves is to identify where the sign of the difference between the precision and recall values changes. This change in sign indicates where the precision and recall curves intersect. Here’s how it works:\\n\\n1. **Compute the Difference:** Subtract the recall values from the precision values.\\n   ```python\\n   np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"])\\n   ```\\n\\n2. **Get the Signs:** Apply `np.sign` to this difference to get the sign (-1, 0, or 1) of each difference value.\\n   ```python\\n   np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n   ```\\n\\n3. **Find Changes in Sign:** Use `np.diff` to find where these signs change (indicating an intersection).\\n   ```python\\n   np.diff(np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"])))\\n   ```\\n\\n4. **Identify Intercept Indices:** Use `np.argwhere` to identify the indices where the sign changes.\\n   ```python\\n   idx = np.argwhere(np.diff(np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))))\\n   ```\\n\\nThis process helps to pinpoint the threshold at which the precision and recall curves intersect by detecting the change in sign in their difference.',\n",
       "  'answer_orig': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'document': '1427d567',\n",
       "  'question': 'What is the role of np.sign in determining the intercept between precision and recall curves?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To print the threshold value where precision and recall curves intersect in Python, you can use numpy functions `diff` and `sign` to find the intercept, as demonstrated in the CONTEXT. Here\\'s the specific procedure:\\n\\n1. Ensure you have your DataFrame (`df_scores`) ready with columns for \\'threshold\\', \\'precision\\', and \\'recall\\'.\\n2. Use numpy to find the index (or indices) where the precision and recall curves intersect.\\n3. Print the threshold value at the intersection.\\n\\nHere is the relevant code:\\n\\n```python\\nimport numpy as np\\n\\n# Assuming you have a DataFrame df_scores with \\'threshold\\', \\'precision\\', and \\'recall\\' columns\\n# Example:\\n# df_scores = pd.DataFrame({\\n#     \\'threshold\\': [0.1, 0.2, 0.3, ...],\\n#     \\'precision\\': [0.9, 0.85, 0.8, ...],\\n#     \\'recall\\': [0.8, 0.85, 0.9, ...]\\n# })\\n\\n# Find the index where the sign of the difference between precision and recall changes\\nidx = np.argwhere(\\n    np.diff(\\n        np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n    )\\n).flatten()\\n\\n# Print the threshold values at the intersection points\\nprint(f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\")\\n```\\nThis code will output the threshold values where the precision and recall curves intersect.',\n",
       "  'answer_orig': 'You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\nI suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\\nidx = np.argwhere(\\nnp.diff(\\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\\n)\\n).flatten()\\nYou can print the result to easily read it:\\nprint(\\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx][\\'threshold\\']}.\"\\n)\\n(Mélanie Fouesnard)',\n",
       "  'document': '1427d567',\n",
       "  'question': 'How do you print the threshold value where precision and recall curves intersect in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To calculate precision and recall manually, you'll need to start by computing the values for true positives (TP), false positives (FP), and false negatives (FN). Here's a step-by-step guide:\\n\\n1. **Define True Positives (TP), False Positives (FP), and False Negatives (FN):**\\n   - **True Positives (TP):** The number of correct positive predictions.\\n   - **False Positives (FP):** The number of incorrect positive predictions.\\n   - **False Negatives (FN):** The number of incorrect negative predictions.\\n\\n2. **Calculate Precision:**\\n   \\\\[\\n   \\\\text{Precision} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FP}}\\n   \\\\]\\n   Precision measures the accuracy of the positive predictions.\\n\\n3. **Calculate Recall:**\\n   \\\\[\\n   \\\\text{Recall} = \\\\frac{\\\\text{TP}}{\\\\text{TP} + \\\\text{FN}}\\n   \\\\]\\n   Recall measures the ability of the model to find all the relevant cases (true positives).\\n\\n### Example\\nSuppose we have the confusion matrix as follows:\\n- True Positives (TP) = 50\\n- False Positives (FP) = 10\\n- False Negatives (FN) = 20\\n\\nUsing these values:\\n- Precision:\\n  \\\\[\\n  \\\\text{Precision} = \\\\frac{50}{50 + 10} = \\\\frac{50}{60} \\\\approx 0.833\\n  \\\\]\\n- Recall:\\n  \\\\[\\n  \\\\text{Recall} = \\\\frac{50}{50 + 20} = \\\\frac{50}{70} \\\\approx 0.714\\n  \\\\]\\n\\nThese are the manual calculations for precision and recall. Alternatively, you can use scikit-learn to calculate these metrics, as mentioned in the CONTEXT:\\n\\n```python\\nfrom sklearn.metrics import precision_score, recall_score\\n\\n# Assuming y_true and y_pred are your true labels and predicted labels respectively\\nprecision = precision_score(y_true, y_pred, average='binary')\\nrecall = recall_score(y_true, y_pred, average='binary')\\n```\",\n",
       "  'answer_orig': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'document': '76c91dfb',\n",
       "  'question': 'How can I calculate precision and recall manually?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can use Scikit Learn to get precision without defining true positive (TP), true negative (TN), false positive (FP), and false negative (FN). You can use the `precision_score` function from the Scikit Learn library as shown in the example below:\\n\\n```python\\nfrom sklearn.metrics import precision_score\\nprecision = precision_score(y_true, y_pred, average='binary')\\n```\\n\\nYou can also get recall and F1 scores in a similar way without manually defining TP, TN, FP, and FN:\\n\\n```python\\nfrom sklearn.metrics import recall_score, f1_score\\nrecall = recall_score(y_true, y_pred, average='binary')\\nf1 = f1_score(y_true, y_pred, average='binary')\\n```\\n\\nAdditionally, you can also use `precision_recall_fscore_support` to obtain precision, recall, and F1 score together:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\\n```\\n\\nThese functions allow you to compute precision directly from the true and predicted labels without the need to compute the individual components of the confusion matrix.\",\n",
       "  'answer_orig': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'document': '76c91dfb',\n",
       "  'question': 'Can I use Scikit Learn to get precision without defining TP, TN, FP, and FN?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In Scikit Learn, the following functions can be used for computing recall:\\n\\n1. `recall_score`\\n   ```python\\n   from sklearn.metrics import recall_score\\n   recall = recall_score(y_true, y_pred, average='binary')\\n   ```\\n\\n2. `precision_recall_fscore_support`\\n   ```python\\n   from sklearn.metrics import precision_recall_fscore_support\\n   precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n   ```\\n\\n3. `precision_recall_curve` (for obtaining recall values to plot)\\n   ```python\\n   from sklearn.metrics import precision_recall_curve\\n   precision, recall, thresholds = precision_recall_curve(y_val, y_predict)\\n   ```\\n\\nThese functions are all capable of computing recall in different contexts, such as a single score or as part of a more extensive metrics calculation.\",\n",
       "  'answer_orig': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'document': '76c91dfb',\n",
       "  'question': 'What functions from Scikit Learn are used for computing recall?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, Scikit Learn offers an option to directly compute the F1 Score. You can use the `f1_score` function from the `sklearn.metrics` module:\\n\\n```python\\nfrom sklearn.metrics import f1_score\\nf1 = f1_score(y_true, y_pred, average='binary')\\n```\\n\\nAdditionally, you can also use the `precision_recall_fscore_support` function to compute precision, recall, F1 score, and support:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\",\n",
       "  'answer_orig': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'document': '76c91dfb',\n",
       "  'question': 'Is there an option in Scikit Learn to directly compute F1 Score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"For binary classification in Scikit Learn metrics, you should use the parameter `average='binary'`. Here's how you can calculate the precision, recall, and F1 score using this parameter:\\n\\n```python\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\n\\nprecision = precision_score(y_true, y_pred, average='binary')\\nrecall = recall_score(y_true, y_pred, average='binary')\\nf1 = f1_score(y_true, y_pred, average='binary')\\n```\\n\\nThis parameter setting is specifically used for binary classification to get the appropriate metrics.\",\n",
       "  'answer_orig': \"In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\\nfrom sklearn.metrics import precision_score, recall_score, f1_score\\nprecision_score(y_true, y_pred, average='binary')\\nrecall_score(y_true, y_pred, average='binary')\\nf1_score(y_true, y_pred, average='binary')\\nRadikal Lukafiardi\",\n",
       "  'document': '76c91dfb',\n",
       "  'question': 'What parameter should be used for binary classification in Scikit Learn metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We use cross-validation when evaluating model performance because it allows for the assessment of the model's effectiveness and helps in selecting the best hyperparameters. Cross-validation works by splitting the dataset into multiple parts (folds), typically 5 or 10, and then repeatedly training and validating the model on different folds. In each iteration, a different fold is used as the validation set while the remaining folds are used for training. This process ensures that the model is evaluated on various subsets of the data, leading to more reliable and generalizable performance estimates.\",\n",
       "  'answer_orig': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'document': 'e4dd91cf',\n",
       "  'question': 'Why do we use cross-validation when evaluating model performance?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Cross-validation chooses the best hyperparameters by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates the model multiple times, each time using a different fold as the validation set and the remaining folds as the training set. This method allows the model to be tested on various partitions of the data to ensure robust performance and generalization. For example, in models like Support Vector Machines (SVM) and logistic regression, the \"C\" hyperparameter can be tuned via cross-validation to find the optimal balance between underfitting and overfitting.',\n",
       "  'answer_orig': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'document': 'e4dd91cf',\n",
       "  'question': 'How does cross-validation choose the best hyperparameters?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"When you use smaller 'C' values in regularization, more regularization is introduced to the model. This means that the model is penalized more for misclassifying training examples, encouraging it to find a simpler decision boundary. As a result, the model might underfit the data because it is heavily penalizing incorrect predictions. This approach is typically applied in algorithms like Support Vector Machines (SVM) and logistic regression.\",\n",
       "  'answer_orig': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'document': 'e4dd91cf',\n",
       "  'question': \"What happens when you use smaller 'C' values in regularization?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Larger 'C' values in models like Support Vector Machines (SVM) and logistic regression reduce the regularization effect. This allows the model to fit the training data more closely, which can potentially lead to overfitting. This happens because larger 'C' values penalize misclassification less severely, enabling the model to prioritize getting training examples correct.\",\n",
       "  'answer_orig': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'document': 'e4dd91cf',\n",
       "  'question': \"What is the effect of larger 'C' values in models like SVM and logistic regression?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Cross-validation splits the dataset into multiple parts, commonly referred to as folds, typically 5 or 10. During model training, the process involves the following steps:\\n\\n1. **Splitting**: The dataset is divided into 'k' folds.\\n2. **Training and Validation**: The model is trained and evaluated 'k' times. In each iteration, one different fold is used as the validation set while the remaining 'k-1' folds are used as the training set.\\n3. **Rotation**: This process is rotated such that each fold serves as the validation set once.\\n\\nThis method ensures that each part of the data is used for validation at least once, leading to a more reliable evaluation of model performance and helping in hyperparameter tuning.\",\n",
       "  'answer_orig': 'Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\\nAminat Abolade',\n",
       "  'document': 'e4dd91cf',\n",
       "  'question': 'How does cross-validation split the dataset during model training?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To evaluate a model using scikit learn metrics, you can compute various model evaluation metrics using readily available functions in the scikit learn library. Here are the steps and functions you can use:\\n\\n1. **Accuracy Score:**\\n   ```python\\n   from sklearn.metrics import accuracy_score\\n   accuracy = accuracy_score(y_val, y_pred)\\n   print(f'Accuracy: {accuracy}')\\n   ```\\n\\n2. **Precision Score:**\\n   ```python\\n   from sklearn.metrics import precision_score\\n   precision = precision_score(y_val, y_pred)\\n   print(f'Precision: {precision}')\\n   ```\\n\\n3. **Recall Score:**\\n   ```python\\n   from sklearn.metrics import recall_score\\n   recall = recall_score(y_val, y_pred)\\n   print(f'Recall: {recall}')\\n   ```\\n\\n4. **F1 Score:**\\n   ```python\\n   from sklearn.metrics import f1_score\\n   f1 = f1_score(y_val, y_pred)\\n   print(f'F1-Score: {f1}')\\n   ```\\n\\n5. **ROC AUC Score:**\\n   ```python\\n   from sklearn.metrics import roc_auc_score\\n   roc_auc = roc_auc_score(y_val, y_pred)\\n   print(f'ROC AUC: {roc_auc}')\\n   ```\\n\\nAdditionally, you can obtain all classification metrics simultaneously using `classification_report`:\\n```python\\nfrom sklearn.metrics import classification_report\\nreport = classification_report(y_val, y_pred)\\nprint(report)\\n```\\n\\nScikit-learn also provides other methods to compute precision, recall, and F1 score:\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\\n\\nThese functions help in evaluating different aspects of your model's performance, providing a comprehensive overview.\",\n",
       "  'answer_orig': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'document': 'cc53ae94',\n",
       "  'question': 'What are ways to evaluate a model using scikit learn metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To compute accuracy using the scikit-learn library, you can use the `accuracy_score` function. Here's an example:\\n\\n```python\\nfrom sklearn.metrics import accuracy_score\\n\\n# Assuming y_val contains the true labels and y_pred contains the predicted labels\\naccuracy = accuracy_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\n```\\n\\nThis code snippet will calculate the accuracy of your model's predictions and print it out. The `accuracy_score` function takes the true labels (`y_val`) and the predicted labels (`y_pred`) as inputs, and returns the accuracy of the model.\",\n",
       "  'answer_orig': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'document': 'cc53ae94',\n",
       "  'question': 'How can accuracy be computed using scikit learn library?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The function used to compute precision in scikit-learn is `precision_score`.\\n\\nExample usage:\\n```python\\nfrom sklearn.metrics import precision_score\\n\\n# Example:\\nprecision = precision_score(y_true, y_pred, average='binary')\\n```\",\n",
       "  'answer_orig': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'document': 'cc53ae94',\n",
       "  'question': 'Which function is used to compute precision in scikit learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can use the `recall_score` metric from the scikit-learn library to calculate recall.\\n\\nHere is an example:\\n\\n```python\\nfrom sklearn.metrics import recall_score\\n\\nrecall = recall_score(y_true, y_pred, average='binary')\\nprint(f'Recall: {recall}')\\n```\\n\\nThis method provides you with an easy way to calculate recall without having to manually define or compute metrics.\",\n",
       "  'answer_orig': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'document': 'cc53ae94',\n",
       "  'question': 'What scikit learn metric can I use to calculate recall?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The ROC AUC score in scikit-learn is computed using the `roc_auc_score` function from the `sklearn.metrics` module. This function requires two parameters: `y_true` and `y_score`.\\n\\n- **`y_true`**: The true binary labels of your data.\\n- **`y_score`**: The probability estimates or confidence values of the positive class.\\n\\nFor each numerical value in your dataset, it is passed as `y_score` to the `roc_auc_score` function, while the target variable is passed as `y_true`.\\n\\nHere's an example of how you might compute the ROC AUC score:\\n\\n```python\\nfrom sklearn.metrics import roc_auc_score\\n\\n# Example usage\\nroc_auc = roc_auc_score(y_true, y_score)\\nprint(f'ROC AUC: {roc_auc}')\\n```\\n\\nMake sure that `y_true` contains the actual target values (binary labels), and `y_score` contains the model's predicted probabilities or decision function scores.\",\n",
       "  'answer_orig': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'document': 'cc53ae94',\n",
       "  'question': 'How is ROC AUC score computed in scikit learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'An alternative way to compute Precision, Recall, and F1 score is by using `precision_recall_fscore_support` from the scikit-learn library. The implementation is as follows:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\n# Example usage\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\\n\\nThis method provides a way to compute all three metrics simultaneously in a straightforward manner.',\n",
       "  'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'document': '403bbdd8',\n",
       "  'question': 'What is an alternative way to compute Precision, Recall, and F1 score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, Scikit-learn provides several methods for calculating Precision, Recall, and F1 score. You can use the following methods:\\n\\n1. **precision_recall_fscore_support**:\\n   ```python\\n   from sklearn.metrics import precision_recall_fscore_support\\n   precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n   ```\\n\\n2. **Individual metric methods**:\\n   ```python\\n   from sklearn.metrics import precision_score, recall_score, f1_score\\n   precision = precision_score(y_true, y_pred, average='binary')\\n   recall = recall_score(y_true, y_pred, average='binary')\\n   f1 = f1_score(y_true, y_pred, average='binary')\\n   ```\\n\\nThese methods allow you to compute these metrics without having to manually define true positive, true negative, false positive, and false negative.\",\n",
       "  'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'document': '403bbdd8',\n",
       "  'question': 'Is there a Scikit-learn method for calculating Precision, Recall, and F1 score?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can use the `precision_recall_fscore_support` function from the scikit-learn library to compute the precision, recall, F1 score, and support for classification problems. Here's how it can be done:\\n\\nExample:\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\n# Assuming y_val are the true labels and y_val_pred are the predicted labels\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\\n\\nThis function allows you to calculate these metrics without manually defining true positives, true negatives, false positives, and false negatives. It provides a convenient and precise way to evaluate classification models.\",\n",
       "  'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'document': '403bbdd8',\n",
       "  'question': 'How can precision_recall_fscore_support be used in classification?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Sure, here's an example using `precision_recall_fscore_support` from Scikit-learn:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\n# Assuming you have your true labels and predicted labels\\ny_true = [0, 1, 1, 0, 1, 1, 0]\\ny_pred = [0, 1, 0, 0, 1, 0, 1]\\n\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred, zero_division=0)\\n\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1 Score: {fscore}')\\nprint(f'Support: {support}')\\n```\\n\\nIn this example, `y_true` represents the actual labels, and `y_pred` represents the predicted labels from your model. The function `precision_recall_fscore_support` computes the precision, recall, F1 score, and support for each class. The `zero_division=0` parameter handles the case where there might be a division by zero in the calculations by setting the corresponding values to zero.\",\n",
       "  'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'document': '403bbdd8',\n",
       "  'question': 'Can you provide an example using precision_recall_fscore_support from Scikit-learn?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The syntax for using `precision_recall_fscore_support` in Python is as follows:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\n\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```\\n\\nHere, `y_val` represents the true labels, and `y_val_pred` represents the predicted labels. The `zero_division` parameter handles divide-by-zero errors.',\n",
       "  'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       "  'document': '403bbdd8',\n",
       "  'question': 'What is the syntax for using precision_recall_fscore_support in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'ROC (Receiver Operating Characteristic) curves are appropriate for evaluating a model when the observations are balanced between each class. This is because ROC curves present an optimistic picture of the model on datasets with a class imbalance due to the use of true negatives in the False Positive Rate calculation. Hence, when there is a balanced class distribution, ROC curves will give a more accurate representation of model performance.',\n",
       "  'answer_orig': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'document': '7c68ace0',\n",
       "  'question': 'When are ROC curves appropriate for evaluating a model?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Precision-recall curves work better with imbalanced datasets because they do not include true negatives in their calculations. This characteristic helps to avoid the optimistic bias that ROC curves might present in the presence of class imbalance. Unlike ROC curves, which rely on metrics like the true positive rate and false positive rate—components that can reflect an overly optimistic performance with imbalanced classes due to the predominance of true negatives—precision-recall curves focus explicitly on the performance concerning the positive class. This makes them more meaningful and relevant when dealing with datasets where one class is significantly outnumbered by the other.',\n",
       "  'answer_orig': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'document': '7c68ace0',\n",
       "  'question': 'Why do precision-recall curves work better with imbalanced datasets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The issue with ROC curves on imbalanced datasets is that they tend to present an overly optimistic picture of the model's performance. This happens because ROC curves include true negatives in their calculation of the False Positive Rate (FPR). On imbalanced datasets, the large number of true negatives can overshadow the actual performance on the minority class. Therefore, ROC curves may not accurately reflect the classifier's ability to distinguish between the classes when there is a class imbalance.\",\n",
       "  'answer_orig': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'document': '7c68ace0',\n",
       "  'question': 'What is the issue with ROC curves on imbalanced datasets?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Class distributions can significantly impact metrics such as accuracy and precision. Here are the key points based on the context provided:\\n\\n1. **Accuracy and Precision Sensitivity to Class Distribution**:\\n   - **Accuracy**: This metric can be misleading in the presence of class imbalance. Accuracy measures the proportion of true results (both true positives and true negatives) in the population. When one class vastly outnumbers the other, the accuracy might appear artificially high even if the model is only predicting the majority class.\\n   - **Precision**: This metric, which calculates the proportion of true positive predictions among all positive predictions made by the model, is also affected by class imbalance. Precision can drop significantly if the number of false positives increases, which can be more common in imbalanced datasets.\\n\\n2. **Impact on Other Metrics Integrating Confusion Matrix Values**:\\n   - Metrics derived from the confusion matrix, such as precision, recall, F1 score, and lift, are affected by class distribution changes. These measures will change as the proportion of positive to negative instances changes, even if the classifier's fundamental performance remains the same.\\n\\n3. **Using Appropriate Curves**:\\n   - **ROC Curves**: Suitable for balanced datasets as they incorporate both true positive rate and false positive rate. They may present an overly optimistic view of model performance on imbalanced datasets because they involve true negatives, which are abundant in imbalanced datasets.\\n   - **Precision-Recall Curves**: Recommended for imbalanced datasets because they focus on the prediction of the minority class and do not consider true negatives, thus providing a more reliable performance evaluation in such scenarios.\\n\\nWhen evaluating models, especially on imbalanced datasets, it is crucial to consider how class distribution affects these metrics to avoid misleading conclusions about model performance.\",\n",
       "  'answer_orig': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'document': '7c68ace0',\n",
       "  'question': 'How do class distributions affect metrics like accuracy and precision?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'ROC curves do not change with different positive to negative ratios in a test set because the ROC curves are based on the True Positive Rate (TPR) and False Positive Rate (FPR). These rates do not depend on the absolute number of positive or negative instances but rather on their proportions within the respective groups. Specifically, TPR is defined as the ratio of true positives to the total number of actual positives, and FPR is the ratio of false positives to the total number of actual negatives. Therefore, changing the ratio of positive to negative instances does not affect these specific rates, thus keeping the ROC curve invariant under such changes.\\n',\n",
       "  'answer_orig': '- ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\\n- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\\n-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\\n- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\\n(Anudeep Vanjavakam)',\n",
       "  'document': '7c68ace0',\n",
       "  'question': \"Why don't ROC curves change with different positive to negative ratios in a test set?\",\n",
       "  'course': 'machine-learning-zoomcamp'}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2bd319f-a78d-4d22-bc6a-2bba8e746f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2af96a0d-627a-4f32-9965-c90ba542d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gpt4o = df_gpt4o.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8bfb5a3-06da-4459-bbc4-eed5ed1a84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = results_gpt4o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75d1702e-253a-4e5a-bb4f-686b0a829894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "453b5a1d-a0cc-4023-aa74-de23a65ce71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9b70aad3-71f4-46a4-81df-9c34ecfa6283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b99f368037c46eeba0bfa8e52a2aaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39e10c57-34a0-41bf-9867-10a50fc6d9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    732.000000\n",
       "mean       0.687674\n",
       "std        0.200547\n",
       "min       -0.139294\n",
       "25%        0.607614\n",
       "50%        0.732200\n",
       "75%        0.835239\n",
       "max        0.988077\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt4o['cosine'] = similarity\n",
    "df_gpt4o['cosine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80fe32cd-ad8d-40be-9bbb-4f623bc94d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/hhammad/.local/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hhammad/.local/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hhammad/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hhammad/.local/lib/python3.10/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hhammad/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hhammad/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install  matplotlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "96b69a9a-ecd6-43fa-85da-e4ca00bdc61b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86347/2392715083.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import seaborn objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/seaborn/rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Control plot style and scaling using the matplotlib rcParams interface.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;31m# triggering resolution of _auto_backend_sentinel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m rcParamsDefault = _rc_params_in_file(\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matplotlibrc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m     \u001b[0;31m# Strip leading comment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_get_data_path\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mspecify\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbase\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3f497b01-c8c6-4809-8ac8-2be8db0ca578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696d1c115310441aadc8eebf7aa69760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_gpt35 = df_gpt35.to_dict(orient='records')\n",
    "\n",
    "similarity_35 = []\n",
    "\n",
    "for record in tqdm(results_gpt35):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity_35.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "524528a3-1651-4200-8240-badcaeee67ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1830.000000\n",
       "mean        0.657599\n",
       "std         0.226062\n",
       "min        -0.168921\n",
       "25%         0.546504\n",
       "50%         0.714783\n",
       "75%         0.817262\n",
       "max         1.000000\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt35['cosine'] = similarity_35\n",
    "df_gpt35['cosine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "41e8a2ab-af71-48ea-8128-724160150592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "607005fb-d83f-44be-a51a-6bed9f6ebdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Answer: {answer_orig}\n",
    "Generated Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b067fc58-bba7-4cb9-bba3-6d7f1387ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt1_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fe131dc5-256b-4198-96f7-4c6209ca9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_gpt4o_mini.sample(n=150, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "97a7d4f8-a342-46a3-86f3-eef31b2ae8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 6),\n",
       "                                             answer_llm  \\\n",
       " 726  The syntax for using `precision_recall_fscore_...   \n",
       " \n",
       "                                            answer_orig  document  \\\n",
       " 726  Scikit-learn offers another way: precision_rec...  403bbdd8   \n",
       " \n",
       "                                               question  \\\n",
       " 726  What is the syntax for using precision_recall_...   \n",
       " \n",
       "                         course    cosine  \n",
       " 726  machine-learning-zoomcamp  0.901076  )"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape, df_sample.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af177e-2111-4cbe-9a6f-e33d6728a6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fd5b25b5-4244-4322-9cba-40b2dde6b7de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e61a635b-db6d-472f-89a1-a4d50f622124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'The syntax for using `precision_recall_fscore_support` in Python is as follows:\\n\\n```python\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n```',\n",
       " 'answer_orig': 'Scikit-learn offers another way: precision_recall_fscore_support\\nExample:\\nfrom sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\\n(Gopakumar Gopinathan)',\n",
       " 'document': '403bbdd8',\n",
       " 'question': 'What is the syntax for using precision_recall_fscore_support in Python?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'cosine': 0.9010756015777588}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = samples[0]\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e514f94b-8564-40ab-a367-a5a2dda0c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
      "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
      "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Original Answer: Scikit-learn offers another way: precision_recall_fscore_support\n",
      "Example:\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "(Gopakumar Gopinathan)\n",
      "Generated Question: What is the syntax for using precision_recall_fscore_support in Python?\n",
      "Generated Answer: The syntax for using `precision_recall_fscore_support` in Python is as follows:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "```\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the original\n",
      "answer and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt1_template.format(**record)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "57e6694f-bf49-467d-82ae-ca5bf06e94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(prompt, model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "05e6b9bd-4996-4b73-9a68-9210c7634074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Relevance\": \"RELEVANT\",\\n  \"Explanation\": \"The generated answer directly addresses the question about the syntax for using `precision_recall_fscore_support` in Python by providing the exact code snippet from the original answer. It maintains the context and aligns well with the provided information.\"\\n}'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "edbe0ddb-75ab-4663-abbd-9886bc23a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92b557cc-13a7-4832-b1c9-382cc51d08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6c510a856e469088f3f63390e69a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(samples):\n",
    "    prompt = prompt1_template.format(**record)\n",
    "    evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "    evaluations.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f3afb95a-53b9-474d-87fc-3d4461a7f27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_evaluations = []\n",
    "\n",
    "for i, str_eval in enumerate(evaluations):\n",
    "    json_eval = json.loads(str_eval)\n",
    "    json_evaluations.append(json_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3d6bb779-d349-4d0e-9654-edfdec80e119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a6419c24-1a39-4d3b-9468-b99208355035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations = pd.DataFrame(json_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a1d6aa1e-8c91-4a90-82e4-e2046a0e5be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relevance\n",
       "RELEVANT           124\n",
       "PARTLY_RELEVANT     16\n",
       "NON_RELEVANT        10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations.Relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3664805d-2500-4b65-9980-de162ccf5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer discusses a pip version e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer addresses a different err...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer incorrectly states that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not relate to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer incorrectly states that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer addresses a different que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not relate to the or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer addresses a different pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer discusses the requirement...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Relevance                                        Explanation\n",
       "4    NON_RELEVANT  The generated answer discusses a pip version e...\n",
       "11   NON_RELEVANT  The generated answer addresses a different err...\n",
       "27   NON_RELEVANT  The generated answer incorrectly states that t...\n",
       "41   NON_RELEVANT  The generated answer does not relate to the or...\n",
       "45   NON_RELEVANT  The generated answer incorrectly states that t...\n",
       "90   NON_RELEVANT  The generated answer addresses a different que...\n",
       "93   NON_RELEVANT  The generated answer does not address the cont...\n",
       "116  NON_RELEVANT  The generated answer does not relate to the or...\n",
       "138  NON_RELEVANT  The generated answer addresses a different pro...\n",
       "139  NON_RELEVANT  The generated answer discusses the requirement..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations[df_evaluations.Relevance == 'NON_RELEVANT'] #.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "550a962b-c807-4dcc-91c4-bb69f47f1c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': \"The cause of the pip version error in this week's serverless deep learning section could be a version conflict in Scikit-Learn. Specifically, if you are using a different version than what was used during the model training, it can lead to warnings and potential breaking code or invalid results. To resolve this, make sure to use the same version of Scikit-Learn that was used for training the model. For instance, if you trained with version 1.1.1, you should use that same version in your virtual environment.\",\n",
       " 'answer_orig': 'When running docker build -t dino-dragon-model it returns the above error\\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\nPastor Soto',\n",
       " 'document': '42c09143',\n",
       " 'question': \"What might be the cause of the pip version error in this week's serverless deep learning section?\",\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'cosine': 0.3094043433666229}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "443080f1-b2a5-4994-b958-65a5205c8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_evaluations[df_evaluations.Relevance == 'NON_RELEVANT'] #.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6cdb1246-4769-406c-a9a3-a602ce5d3801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: What is the syntax for using precision_recall_fscore_support in Python?\n",
      "Generated Answer: The syntax for using `precision_recall_fscore_support` in Python is as follows:\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "```\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(**record)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "650e8ecb-80e2-42f1-8039-8d497e3eeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Relevance\": \"RELEVANT\",\n",
      "  \"Explanation\": \"The generated answer directly addresses the question by providing the correct syntax for using the `precision_recall_fscore_support` function in Python, along with a relevant code example.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "03afffa3-9f7e-47eb-9c17-46ae39e38424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068269c6dd13458d88a85862f5e152c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations_2 = []\n",
    "\n",
    "for record in tqdm(samples):\n",
    "    prompt = prompt2_template.format(**record)\n",
    "    evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "    evaluations_2.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c00c22f5-fe31-4782-99ac-846249fc59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_evaluations_2 = []\n",
    "\n",
    "for i, str_eval in enumerate(evaluations_2):\n",
    "    json_eval = json.loads(str_eval)\n",
    "    json_evaluations_2.append(json_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "05c6d861-9d0c-41bb-a5db-454283c4b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_2 = pd.DataFrame(json_evaluations_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "09064866-d08c-4e0d-a095-44a9e8625bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the core...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Relevance                                        Explanation\n",
       "45   NON_RELEVANT  The generated answer explicitly states that it...\n",
       "49   NON_RELEVANT  The generated answer does not address the ques...\n",
       "102  NON_RELEVANT  The generated answer does not address the core..."
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_2[df_evaluations_2.Relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "95af0004-e25c-4328-8d22-d48443470da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': 'The provided context does not include specific commands to start the Docker daemon on Linux. Therefore, I cannot provide an answer based solely on the facts from the context.',\n",
       " 'answer_orig': 'Working on getting Docker installed - when I try running hello-world I am getting the error.\\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\\nSolution description\\nIf you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\\nOn Linux, start the docker daemon with either of these commands:\\nsudo dockerd\\nsudo service docker start\\nAdded by Ugochukwu Onyebuchi',\n",
       " 'document': '4b2a3181',\n",
       " 'question': 'What commands should I use to start the docker daemon on Linux?',\n",
       " 'course': 'machine-learning-zoomcamp',\n",
       " 'cosine': 0.5113022327423096}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2346971-07ef-482a-8d98-ff813a7bcc02",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183517bc-f027-43e4-9a4a-86a46f3dcfca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82f9c39-4d87-4b21-8789-c178649376a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main'\n",
    "relative_url = '04-monitoring/data/results-gpt4o-mini.csv'\n",
    "generated_gpt4_mini = f'{base_url}/{relative_url}?raw=1'\n",
    "\n",
    "df = pd.read_csv(generated_gpt4_mini)\n",
    "#df_ground_truth = df_ground_truth[df_ground_truth.course == 'machine-learning-zoomcamp']\n",
    "#ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca2e555-c2b7-470a-b4a5-a58d081df2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bd9d49-31b0-4d55-a823-ec467a9b0417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90875d1d-fbaf-4f49-a6a4-a36ab6caacd9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q1. Getting the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9aafbe-6e14-4bf5-9977-3f9303c6d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5018d5de-aff7-4700-b809-ab284b94b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824339da-6641-4fbb-a679-67a84c1b71af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['answer_llm', 'answer_orig', 'document', 'question', 'course'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e332933-fac7-4ef0-9796-69f04897c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dc0a0d-74be-4e2f-9cae-94ea02b7549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be8b937-2868-4378-bd65-afa1eb94c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_llm = embedding_model.encode(answer_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14664a2-8d13-44b9-91a4-b58dcacbc689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.22446787e-01, -2.24855945e-01, -3.24058473e-01, -2.84758747e-01,\n",
       "        7.25685246e-03,  1.01186700e-01,  1.03716828e-01, -1.89983383e-01,\n",
       "       -2.80596819e-02,  2.71588862e-01, -1.15337133e-01,  1.14666067e-01,\n",
       "       -8.49587619e-02,  3.32365334e-01,  5.52725159e-02, -2.22195953e-01,\n",
       "       -1.42540991e-01,  1.02519318e-01, -1.52333513e-01, -2.02912480e-01,\n",
       "        1.98424123e-02,  8.38148072e-02, -5.68632185e-01,  2.32843738e-02,\n",
       "       -1.67292967e-01, -2.39256978e-01, -8.05461779e-02,  2.57081706e-02,\n",
       "       -8.15464854e-02, -7.39289895e-02, -2.61550128e-01,  1.92576721e-02,\n",
       "        3.22909206e-01,  1.90357178e-01, -9.34726413e-05, -2.13165879e-01,\n",
       "        2.88942400e-02, -1.79529544e-02, -5.92760742e-02,  1.99918360e-01,\n",
       "       -4.75170761e-02,  1.71634018e-01, -2.45914217e-02, -9.38058272e-02,\n",
       "       -3.57002944e-01,  1.33263648e-01,  1.94046125e-01, -1.18530720e-01,\n",
       "        4.56915259e-01,  1.47728026e-01,  3.35945129e-01, -1.86959922e-01,\n",
       "        2.45954573e-01, -1.07774340e-01, -1.80860028e-01,  1.18171811e-01,\n",
       "        7.27293491e-02,  2.78034240e-01,  4.16389078e-01, -1.66038379e-01,\n",
       "       -1.28587544e-01,  3.40979755e-01, -5.31822741e-02,  2.86051054e-02,\n",
       "        2.70212889e-01,  1.59812327e-02,  5.30229270e-01, -4.24391508e-01,\n",
       "        4.41499911e-02,  1.12921625e-01,  5.93987703e-01,  1.31700784e-01,\n",
       "        7.96379242e-03, -9.32307169e-03,  2.07376942e-01,  9.57206115e-02,\n",
       "       -1.83128089e-01,  1.22415833e-01, -2.01750502e-01,  3.12428623e-01,\n",
       "       -5.54993525e-02, -2.87764519e-01, -1.73390672e-01,  1.55470833e-01,\n",
       "        1.19513851e-02,  3.49036843e-01, -8.38189647e-02, -1.50645673e-01,\n",
       "        5.33231497e-02, -1.06787771e-01,  1.13244697e-01,  3.14916521e-01,\n",
       "       -2.95591471e-03,  4.66762662e-01, -1.56012818e-01, -3.61533940e-01,\n",
       "        3.31575684e-02,  2.06269294e-01, -1.06713705e-01, -5.01035377e-02,\n",
       "        1.83970094e-01, -1.39952311e-02,  2.16501430e-01,  1.41000226e-01,\n",
       "        1.90431923e-01, -3.77752930e-01,  8.65839720e-02, -9.80310962e-02,\n",
       "        2.32716188e-01,  2.01485455e-02, -9.70340669e-02,  1.01333454e-01,\n",
       "       -1.72074765e-01, -1.49902523e-01, -2.79156834e-01,  4.38269109e-01,\n",
       "       -1.79244265e-01, -5.75874746e-02, -7.84665346e-02, -3.23518276e-01,\n",
       "       -2.88021326e-01,  1.18698701e-01,  8.74241889e-02,  2.05323339e-01,\n",
       "       -4.80113328e-02, -5.71198687e-02, -2.83935368e-02, -3.82448174e-02,\n",
       "       -2.26608112e-01, -6.20818585e-02, -8.28165710e-02,  3.57217342e-01,\n",
       "       -1.73984215e-01, -2.00290859e-01,  2.40433529e-01,  2.16891631e-01,\n",
       "       -6.78277835e-02,  1.77686363e-01, -7.99813494e-03,  2.68137783e-01,\n",
       "        1.61456153e-01,  3.94612193e-01,  8.85783583e-02,  2.40580857e-01,\n",
       "       -2.59242184e-03, -3.16258460e-01, -2.44215056e-01,  3.14837635e-01,\n",
       "       -1.19753405e-01,  2.73649432e-02, -1.63026467e-01, -1.62029341e-02,\n",
       "       -3.58390175e-02,  3.33115458e-01,  5.83249509e-01, -2.79058278e-01,\n",
       "        4.58128482e-01,  4.30684894e-01, -9.04698372e-02, -9.46645811e-02,\n",
       "       -4.69529629e-02,  1.82061538e-01,  1.93517879e-01, -5.74657433e-02,\n",
       "        5.64569905e-02,  6.01405799e-02, -2.71578550e-01,  3.31836641e-02,\n",
       "       -2.93490320e-01,  8.48192647e-02,  1.16654970e-01, -3.32128964e-02,\n",
       "       -9.28956419e-02,  8.89663324e-02,  4.50681895e-03, -1.12345457e-01,\n",
       "        2.86367029e-01, -4.58183020e-01,  2.24826097e-01, -1.59531206e-01,\n",
       "       -3.91896069e-02, -4.82417732e-01,  2.00283840e-01,  5.05793765e-02,\n",
       "       -7.37685412e-02, -2.89325342e-02,  5.19142114e-02,  2.53143936e-01,\n",
       "       -1.18403316e-01, -3.78581790e-05, -1.42470875e-03,  8.79565403e-02,\n",
       "       -1.54602751e-01,  1.39947578e-01, -1.63175523e-01, -2.27565065e-01,\n",
       "        2.43593212e-02,  1.83513477e-01,  2.09395960e-01,  6.44850498e-03,\n",
       "       -1.67488828e-01,  2.13646874e-01, -9.41734239e-02, -1.58160273e-02,\n",
       "       -8.20813030e-02, -1.83308542e-01,  2.60268122e-01, -5.81487060e-01,\n",
       "       -1.88392997e-01,  4.93148804e-01,  1.71536863e-01, -1.94943994e-01,\n",
       "       -2.98341885e-02, -1.10752098e-01,  1.75626159e-01, -2.15448767e-01,\n",
       "        3.90823662e-01, -4.05653584e-04,  1.63337633e-01, -1.30663574e-01,\n",
       "       -1.60531923e-01, -1.78250387e-01,  1.44646183e-01, -5.53424470e-02,\n",
       "       -2.84335196e-01, -2.52671540e-01, -2.86735028e-01,  7.28117898e-02,\n",
       "        1.01622291e-01,  2.40730360e-01,  2.70242900e-01, -2.02046037e-01,\n",
       "        3.18083584e-01,  2.44049832e-01, -1.42584518e-01,  8.29213336e-02,\n",
       "        4.85366620e-02,  2.96781361e-01,  4.12520528e-01, -3.14378828e-01,\n",
       "       -1.16970047e-01,  6.29203022e-02,  7.08070546e-02,  2.58253574e-01,\n",
       "       -1.14308326e-02,  4.45277423e-01,  1.21435836e-01, -1.32654235e-01,\n",
       "       -7.34771490e-02,  1.45819008e-01,  8.71957168e-02,  3.29194218e-02,\n",
       "       -1.06550090e-01,  1.32414714e-01,  5.37193939e-02,  1.55813500e-01,\n",
       "       -3.02492045e-02, -1.43763840e-01, -2.87022501e-01,  2.91071653e-01,\n",
       "        1.64530054e-01, -1.11769669e-01,  2.36827210e-01, -3.21419209e-01,\n",
       "       -1.66114420e-01, -1.28119681e-02,  3.42823088e-01, -1.29733726e-01,\n",
       "       -8.64384547e-02,  8.01282004e-02,  2.09743306e-01, -1.31370753e-01,\n",
       "       -4.56719965e-01, -2.07562461e-01,  2.11608663e-01, -1.06123202e-01,\n",
       "       -7.91288912e-02, -1.07585587e-01, -3.71915519e-01, -7.27089718e-02,\n",
       "       -1.18880950e-01, -8.83411691e-02, -1.14617441e-02, -1.38135478e-01,\n",
       "       -2.01837182e-01, -4.67054769e-02, -9.99320671e-02, -2.49767497e-01,\n",
       "        2.34003976e-01, -3.08610618e-01,  5.94544411e-02, -1.53505117e-01,\n",
       "        2.65017509e-01,  3.01973661e-03, -1.05429024e-01,  1.93159610e-01,\n",
       "        3.11239898e-01,  3.94330204e-01, -1.93251409e-02,  4.99086767e-01,\n",
       "        1.55236805e-02, -1.99357539e-01,  5.28639473e-04,  4.31942075e-01,\n",
       "        7.14836037e-03, -1.23075821e-01,  8.09212588e-03, -1.80045828e-01,\n",
       "        6.30232133e-03, -3.00479792e-02, -3.72296929e-01,  1.83589846e-01,\n",
       "        2.57175654e-01, -1.55412167e-01,  1.24235503e-01,  1.37525395e-01,\n",
       "       -1.35490865e-01,  3.29676084e-02, -9.26109552e-02, -5.65365136e-01,\n",
       "        2.80105211e-02, -2.07514241e-02, -1.69507459e-01, -2.20879570e-01,\n",
       "       -2.19279274e-01, -2.83492416e-01, -4.83235836e-01, -3.42599422e-01,\n",
       "       -9.17720199e-02,  2.96830446e-01, -5.98628353e-03,  6.14563860e-02,\n",
       "       -1.05392344e-01,  1.58028498e-01,  4.33736555e-02,  2.40767330e-01,\n",
       "       -3.02058876e-01, -2.03180507e-01,  3.31530154e-01, -3.77642214e-01,\n",
       "       -2.81263500e-01,  1.91836253e-01, -1.27187893e-01,  1.78842068e-01,\n",
       "       -2.64506459e-01,  1.23102181e-02, -1.61026716e-01,  1.34970218e-01,\n",
       "       -8.89988691e-02, -4.67516296e-02,  4.90023911e-01,  7.38458410e-02,\n",
       "       -1.30986854e-01, -3.05826992e-01, -1.67545572e-01,  1.94467992e-01,\n",
       "        1.63925245e-01,  1.95639223e-01,  2.50460088e-01, -4.28530991e-01,\n",
       "        3.45157646e-03, -2.58735716e-01,  5.29080987e-01, -3.74270082e-01,\n",
       "        1.52395874e-01, -1.04267634e-01,  5.28327897e-02,  6.97042271e-02,\n",
       "        3.10416430e-01,  1.34908870e-01, -1.46409914e-01,  8.79694968e-02,\n",
       "       -2.16057554e-01, -5.98206883e-03,  1.46351457e-01,  1.00754961e-01,\n",
       "       -3.56721640e-01,  2.37128824e-01, -2.08308369e-01, -1.37767941e-01,\n",
       "        1.87475309e-01, -1.37022287e-01,  1.79654717e-01,  9.50281098e-02,\n",
       "        3.05513024e-01, -2.35129194e-03, -5.99395745e-02,  3.43031436e-01,\n",
       "        2.22358525e-01,  3.86910826e-01, -4.61394712e-02, -5.23233116e-01,\n",
       "       -1.88939229e-01, -3.29891369e-02,  1.58315763e-01, -6.72142655e-02,\n",
       "        1.70431465e-01, -1.60779700e-01, -1.32888451e-01,  1.94981813e-01,\n",
       "       -7.49423355e-02,  1.33112490e-01, -3.46184261e-02, -3.98086198e-02,\n",
       "        1.55842796e-01,  7.61212111e-02,  6.12084121e-02,  1.30054295e-01,\n",
       "       -4.20598775e-01, -8.50358531e-02,  7.93767571e-02,  1.17576607e-01,\n",
       "        1.19344229e-02, -8.69084708e-03,  1.18011601e-01, -3.85569453e-01,\n",
       "       -1.99203864e-01,  3.17576230e-01,  6.12187423e-02, -1.49612010e-01,\n",
       "       -8.59778225e-02, -3.21717486e-02,  1.85915425e-01, -1.35080591e-01,\n",
       "        3.26779962e-01,  1.11499630e-01,  2.00124979e-01, -1.81169987e-01,\n",
       "        9.46175028e-03,  2.59241760e-01, -8.62892270e-02,  1.24217950e-01,\n",
       "       -2.26192176e-01,  1.87308654e-01, -6.76105320e-02,  1.09886616e-01,\n",
       "        7.81963542e-02,  1.59898788e-01,  4.34805155e-02,  3.77582192e-01,\n",
       "        2.00295318e-02,  3.12313259e-01,  3.17892283e-01,  3.18764038e-02,\n",
       "        1.84716970e-01,  1.02976814e-01, -1.29831247e-02, -5.33965945e-01,\n",
       "       -4.15449170e-03, -9.74933580e-02,  2.92788357e-01, -1.68568827e-02,\n",
       "       -3.65061492e-01,  2.62162954e-01, -1.99976221e-01, -1.79652423e-01,\n",
       "       -6.74350783e-02, -7.36466050e-02,  7.99735077e-03, -8.52292180e-02,\n",
       "        3.02016109e-01,  7.65381873e-01,  1.86538070e-01,  9.91610214e-02,\n",
       "       -3.15703489e-02, -2.39601493e-01,  3.49765569e-01, -3.79434586e-01,\n",
       "        4.37371463e-01, -3.03044915e-01,  3.50981392e-02, -1.36658363e-02,\n",
       "       -3.92119922e-02,  1.74251080e-01,  2.19215870e-01,  3.65047865e-02,\n",
       "        1.70673192e-01,  1.96939513e-01, -1.31071225e-01, -4.28889804e-02,\n",
       "        1.75453350e-01, -4.72445823e-02,  6.98638707e-02,  3.11694294e-02,\n",
       "        3.22514057e-01,  4.49009240e-01,  1.86776459e-01, -1.16198212e-02,\n",
       "       -1.47021309e-01, -2.00406462e-01,  7.67491460e-02, -1.27667755e-01,\n",
       "       -6.29129484e-02, -4.80404019e-01,  1.54105470e-01, -9.61345807e-02,\n",
       "       -2.31545091e-01,  3.21784727e-02, -2.23328829e-01, -9.49444342e-03,\n",
       "        1.23602450e-01, -1.67446628e-01,  2.84746259e-01,  1.42774368e-02,\n",
       "       -2.71190912e-01, -3.30283642e-01,  3.32763910e-01,  1.96248457e-01,\n",
       "       -2.55561262e-01,  1.17187388e-01,  4.77881014e-01, -4.56433184e-02,\n",
       "       -1.86917484e-01, -2.67586440e-01,  1.55057460e-01,  4.42823023e-01,\n",
       "        3.23591769e-01,  2.25058839e-01,  4.15324569e-02, -2.00824827e-01,\n",
       "        1.77915275e-01,  3.05669248e-01,  2.31600389e-01, -3.19246769e-01,\n",
       "       -1.18139260e-01,  1.34351581e-01,  2.80499488e-01, -2.21151471e-01,\n",
       "        5.30993640e-02,  1.29903704e-01,  6.53694794e-02, -1.18017374e-02,\n",
       "        6.73177615e-02,  8.00406039e-02, -5.13182759e-01, -5.82337454e-02,\n",
       "        6.54618070e-02,  2.76209205e-01, -1.83693431e-02,  2.94542134e-01,\n",
       "       -3.07798862e-01,  1.83834136e-01,  1.59030899e-01,  4.07142013e-01,\n",
       "       -1.66562796e-01, -4.79783326e-01, -2.10012287e-01, -4.13606048e-01,\n",
       "        5.92363775e-02,  1.54404178e-01,  6.58277946e-04,  2.10360914e-01,\n",
       "        2.83089727e-01,  3.63211900e-01,  1.09330736e-01, -5.22614777e-01,\n",
       "       -1.22007132e-01,  1.48911718e-02,  8.16331431e-02,  9.79121178e-02,\n",
       "       -1.71466529e-01, -1.19786099e-01,  3.07113945e-01,  2.41553888e-01,\n",
       "        1.07853383e-01, -1.99582413e-01, -3.44078958e-01, -6.04019500e-02,\n",
       "        5.21589555e-02,  3.28598432e-02, -3.27344029e-03,  1.36293814e-01,\n",
       "       -3.81061547e-02, -2.55616337e-01,  1.35476336e-01, -2.36675933e-01,\n",
       "       -4.05685216e-01,  2.38533825e-01,  4.29498106e-02,  1.72241256e-02,\n",
       "       -2.48770982e-01, -5.33274673e-02, -3.07872538e-02,  5.81806749e-02,\n",
       "       -1.23233400e-01,  2.18564849e-02,  2.29185998e-01,  9.34011936e-02,\n",
       "       -2.54486740e-01, -1.76821023e-01,  1.78003624e-01, -1.88909963e-01,\n",
       "       -2.32188895e-01,  1.38992369e-01,  8.63837004e-02,  1.66123450e-01,\n",
       "       -1.26168042e-01,  4.47001457e-02, -2.57241167e-03,  9.59316716e-02,\n",
       "        1.70473695e-01,  4.11358997e-02,  3.73511076e-01, -2.45751202e-01,\n",
       "        8.69029164e-02, -3.60793531e-01, -1.35020092e-01,  2.28721783e-01,\n",
       "        5.50211556e-02,  2.56113350e-01, -2.00423226e-02,  1.00339249e-01,\n",
       "        2.35081837e-02, -5.19269891e-02,  1.96859062e-01, -3.61584331e-04,\n",
       "        3.07785809e-01, -1.46380305e-01,  9.31805372e-02,  1.40383616e-01,\n",
       "        1.81082040e-02, -1.38114646e-01,  4.64576036e-02,  5.93978502e-02,\n",
       "        4.88917083e-02,  2.51312286e-01, -8.47730488e-02,  7.51707852e-02,\n",
       "        1.41398963e-02,  3.21199179e-01,  1.87376663e-01,  3.38847637e-01,\n",
       "        1.51104882e-01,  3.21048468e-01,  1.62717327e-01, -4.25226182e-01,\n",
       "       -3.60833198e-01,  4.96278405e-02,  1.51703060e-01, -2.27194399e-01,\n",
       "       -1.19753852e-01, -2.01471411e-02, -2.40546048e-01,  5.62720783e-02,\n",
       "        3.93153280e-02,  1.62902594e-01,  4.60218899e-02,  1.69598982e-01,\n",
       "       -1.60548344e-01, -2.60229339e-03, -9.23943892e-02, -4.32310075e-01,\n",
       "        9.29703489e-02,  3.15057337e-01,  1.51553424e-02,  4.36365977e-02,\n",
       "       -2.50602573e-01, -8.28461647e-02,  1.22858964e-01,  3.40400226e-02,\n",
       "        1.75263077e-01, -1.60563752e-01, -3.39264154e-01,  8.57212469e-02,\n",
       "        7.73875937e-02, -1.93213969e-01,  1.34666383e-01,  3.19171953e-03,\n",
       "        2.99942225e-01, -2.12176647e-02,  1.68446288e-01,  3.32506180e-01,\n",
       "       -2.90306926e-01, -2.65653003e-02,  1.57835707e-01, -1.45968214e-01,\n",
       "        1.02217034e-01, -3.66670936e-01, -1.05406441e-01,  8.61350894e-02,\n",
       "       -2.02301562e-01,  3.80774029e-02, -1.82997420e-01, -1.71337619e-01,\n",
       "       -1.67107940e-01, -4.81708080e-01, -3.76031473e-02, -2.37805232e-01,\n",
       "        1.88101411e-01, -5.88411167e-02,  5.97653806e-01,  2.54528642e-01,\n",
       "        3.43908668e-02,  3.61726992e-02, -3.01713318e-01, -3.13887030e-01,\n",
       "       -2.44418848e-02,  3.54958177e-02, -1.56710401e-01, -3.16900879e-01,\n",
       "        2.10860781e-02,  2.57124811e-01,  4.08265144e-01,  1.50323123e-01,\n",
       "       -6.00392697e-03, -1.62305713e-01,  3.02282065e-01, -2.29149401e-01,\n",
       "        1.85533553e-01,  9.38914269e-02,  4.12929766e-02, -1.50900915e-01,\n",
       "       -1.33867145e-01,  2.03005984e-01, -2.04596579e-01,  3.01504314e-01,\n",
       "        2.38587230e-01,  6.47759959e-02,  1.75633088e-01,  2.81314284e-01,\n",
       "        1.58511728e-01,  2.10842025e-03,  3.89198929e-01, -2.64083624e-01,\n",
       "       -2.38809317e-01,  2.06713200e-01,  1.16575725e-01, -1.77012801e-01,\n",
       "       -2.77638156e-02,  1.83659017e-01,  2.25405917e-01, -8.78798813e-02,\n",
       "       -1.43584803e-01,  2.10636675e-01,  1.69541121e-01, -5.85013244e-04,\n",
       "       -3.66569944e-02, -3.57495815e-01,  2.06115067e-01, -1.50764853e-01,\n",
       "       -8.18620622e-02,  3.86621356e-02,  1.66414917e-01,  1.40702883e-02,\n",
       "       -2.76515484e-01, -1.28965557e-01, -6.98803812e-02,  2.13923350e-01,\n",
       "       -2.81380713e-01, -3.66245098e-02, -1.43628970e-01,  6.90277517e-02,\n",
       "       -7.71772638e-02,  1.96926981e-01, -2.78328627e-01, -4.52038832e-02,\n",
       "       -1.19244650e-01, -3.84129494e-01,  8.88783112e-02,  4.29855824e-01,\n",
       "        2.08253011e-01, -3.37734938e-01, -2.85533667e-01,  9.70599577e-02,\n",
       "        2.04244569e-01, -6.10274412e-02, -1.66054487e-01, -2.67020494e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61759b3-1a59-48c5-8860-4bc9e98f2f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.515997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "v_orig = embedding_model.encode(df.iloc[0].answer_orig)\n",
    "\n",
    "v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a656daf-e432-4b3c-b54a-a9c82cfd89b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q2. Computing the dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ddc5a0e-9ba0-4753-8bbb-f7eadee974e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_llm': 'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Where can I sign up for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can sign up using the link provided in the course GitHub repository: [https://airtable.com/shryxwLd0COOEaqXo](https://airtable.com/shryxwLd0COOEaqXo).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Can you provide a link to sign up?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an FAQ for the Machine Learning course. It addresses frequently asked technical questions and is available in the course GitHub repository. You can find the link to the FAQ [here](https://airtable.com/shryxwLd0COOEaqXo).',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Is there an FAQ for this Machine Learning course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The context does not provide any specific information about a GitHub repository for the sign-up link for the course. Therefore, I cannot confirm whether or not such a repository exists.',\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'Does this course have a GitHub repository for the sign-up link?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To structure your questions and answers for the course effectively, consider the following steps:\\n\\n1. **Identify Your Topic**: Start by clearly defining the topic your question addresses. This helps in organizing related questions together.\\n\\n2. **Be Specific**: Formulate your question to be specific and concise. This makes it easier for others to understand what information you're seeking.\\n\\n3. **Provide Context**: If your question arises from a particular scenario or issue, briefly explain that context so that it can be better understood.\\n\\n4. **Refer to Course Materials**: When formulating answers, reference course materials, such as course pages or specific modules, to provide a clear and credible response.\\n\\n5. **Consider Audience**: Keep in mind who will be reading your questions and answers. Make sure the language and complexity match their understanding level.\\n\\n6. **Format for Clarity**: Use bullet points or numbered lists for answers to enhance clarity and readability.\\n\\n7. **Incorporate Feedback Loop**: Encourage readers to ask follow-up questions or provide feedback on the answers if clarity is lacking.\\n\\nBy structuring your questions and answers this way, you can ensure they are clear, relevant, and beneficial to others in the course.\",\n",
       "  'answer_orig': 'Machine Learning Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\nIn the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\\nwork',\n",
       "  'document': '0227b872',\n",
       "  'question': 'How can I structure my questions and answers for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course videos are pre-recorded, and you can start watching them right now.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Are the course videos live or pre-recorded?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can start watching the course videos right now, as they are pre-recorded. If you haven’t taken part in the previous iteration, you are encouraged to begin watching them.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'When can I start watching the course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the live office hours sessions are recorded.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Are the live office hours sessions recorded?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the office hours sessions in the course playlist on YouTube, where both the live office hours and the pre-recorded course videos are available.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Where can I find the office hours sessions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can access the pre-recorded course videos on the course page at http://mlzoomcamp.com/. You can also find them in the course playlist on YouTube by visiting the DTC YouTube channel and searching for the relevant course playlist.',\n",
       "  'answer_orig': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.',\n",
       "  'document': '39fda9f0',\n",
       "  'question': 'Where can I access the pre-recorded course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, all sessions are recorded, so if you miss one, you won't miss anything. You can catch up on the content later. Additionally, you can submit your questions in advance for office hours, and those sessions are also recorded.\",\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Are sessions recorded if I miss one?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can ask your questions in advance if you can't attend a live stream. You can submit your questions for office hours ahead of time, and they will be covered during the live session. Additionally, you can always ask questions in Slack.\",\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': \"Can I ask questions in advance if I can't attend a live stream?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you miss a session, don't worry! Everything is recorded, so you won’t miss anything. You can also ask your questions for office hours in advance, and they will be covered during the live stream. Additionally, you can always ask questions in Slack.\",\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'How will my questions be addressed if I miss a session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a way to catch up on a missed session. Everything is recorded, so you won’t miss anything. You can also ask your questions for office hours in advance, and they will be covered during the live stream. Additionally, you can always ask questions in Slack.',\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Is there a way to catch up on a missed session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can still interact with instructors after missing a session. Everything is recorded, so you won’t miss any content. You can ask your questions in advance for office hours, and those questions will be covered during the live stream. Additionally, you can always ask questions in Slack.',\n",
       "  'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       "  'document': '5170565b',\n",
       "  'question': 'Can I still interact with instructors after missing a session?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course will cover the bare minimum of theory, focusing more on practical application. The theory will be addressed on an intuitive level, meaning specific derivations, such as the gradient update rule for logistic regression, will not be covered in depth. Instead, the course will teach how to use logistic regression and interpret the results.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'How much theoretical content is there in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the course will not cover theoretical derivations like the gradient update rule for logistic regression. The focus is more on practical applications, and the theory will only be covered at an intuitive level.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Will the course cover theoretical derivations like the gradient update rule for logistic regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The focus of the course is more on practice rather than theory. The course will cover only the bare minimum of theory, primarily on an intuitive level, while emphasizing practical applications and skills.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Is the focus of the course more on practice or theory?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find more about the theoretical topics not covered in this course, consider looking for other great courses that delve deeper into those areas. The course itself focuses primarily on practical applications and covers theoretical concepts only at an intuitive level.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Where can I find more about the theoretical topics not covered in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course will help you understand how to use logistic regression and interpret its results, although it will cover the theory only at an intuitive level. The course focuses more on practical applications rather than in-depth theoretical derivations.',\n",
       "  'answer_orig': \"The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\",\n",
       "  'document': 'ecca790c',\n",
       "  'question': 'Will the course help me understand how to use logistic regression and interpret its results?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No prior knowledge in math is necessary for this course. While some linear algebra will be covered, the course will primarily focus on code with very few formulas.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Is prior knowledge in math necessary for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course will cover some linear algebra, but there will be very few formulas involved, focusing mostly on code instead.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'What type of mathematical content will be covered?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there are resources available to help with linear algebra. You can watch the \"ML Zoomcamp 1.8 - Linear Algebra Refresher\" from Alexey Grigorev and the \"Essence of Linear Algebra\" playlist from 3Blue1Brown. Additionally, you can ask the community for help if you have any questions.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Are there resources available to help with linear algebra?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can seek help from the community for math-related questions.',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Can I seek help from the community for math-related questions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there are recommended videos for learning linear algebra. You can watch \"ML Zoomcamp 1.8 - Linear Algebra Refresher\" by Alexey Grigorev, or you can check out the excellent playlist from 3Blue1Brown titled \"Vectors | Chapter 1, Essence of Linear Algebra.\"',\n",
       "  'answer_orig': \"Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'c25b3de4',\n",
       "  'question': 'Are there any recommended videos for learning linear algebra?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you haven't received a confirmation email after filling out the form, it is normal to experience a delay as the process is automated. You should eventually receive the email. In the meantime, check your promotions tab in Gmail as well as your spam folder. If you unsubscribed from the newsletter, you won't receive course-related updates. To ensure you don’t miss any important information, consider joining the #course-ml-zoomcamp channel in Slack and the telegram channel with announcements.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': \"What should I do if I haven't received a confirmation email after filling out the form?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, it is possible that your confirmation email went to your spam folder. If you haven't received it, it's recommended to check your promotions tab in Gmail as well as your spam folder.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Is it possible my confirmation email went to my spam folder?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, unsubscribing from the newsletter will affect your receipt of course-related updates. If you unsubscribe, you won't receive those updates. However, you can join the #course-ml-zoomcamp channel in Slack and the telegram channel with announcements to stay informed about the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Can unsubscribing from the newsletter affect receiving course updates?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you don't receive the confirmation email, check your promotions tab in Gmail as well as your spam folder. If you unsubscribed from the newsletter, you won't receive course-related updates. To ensure you don’t miss any updates, join the #course-ml-zoomcamp channel in Slack and the telegram channel with announcements. This will keep you informed about the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': 'Where can I get course updates if I don’t receive the confirmation email?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you're having email issues, you can stay informed about the course by joining the #course-ml-zoomcamp channel in Slack and the Telegram channel with announcements. This will ensure you don’t miss any important updates regarding the course.\",\n",
       "  'answer_orig': \"The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\\nIf you unsubscribed from our newsletter, you won't get course related updates too.\\nBut don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\",\n",
       "  'document': '6ba259b1',\n",
       "  'question': \"How can I stay informed about the course if I'm having email issues?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The duration of the course is approximately 4 months, but it may take longer if you choose to engage in extra activities such as an additional project or article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'What is the duration of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course takes approximately 4 months, but it may take more time if you choose to participate in extra activities such as an extra project or an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'How many months does the course take?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course can take more than 4 months if you choose to engage in extra activities such as an additional project or article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'Can the course take more than 4 months?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Based on the context, the course duration is approximately 4 months, but it may take longer if you choose to engage in extra activities, such as additional projects or articles. There is no specific mention of a formal process for extending the course duration itself, but the self-paced mode allows you to go through the materials at your own speed.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'Is it possible to extend the course duration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Extra activities that might extend the course time include undertaking an extra project or writing an article.',\n",
       "  'answer_orig': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)',\n",
       "  'document': '67e2fd13',\n",
       "  'question': 'What extra activities might extend the course time?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should allocate around ~10 hours per week for this course.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'How many hours per week should I allocate for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The time needed for different modules was analyzed by Timur Kamaliev.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Who analyzed the time needed for different modules?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Students of the previous cohort spent around ~10 hours per week on the course.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'How much time did students of the previous cohort spend weekly?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The estimated weekly time commitment for the course is around 10 hours per week.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Can you give an estimated weekly time commitment for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the detailed analysis of study time requirements conducted by Timur Kamaliev in a full article that outlines how much time students of the previous cohort needed to spend on different modules and projects.',\n",
       "  'answer_orig': 'Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article',\n",
       "  'document': 'a6897e8c',\n",
       "  'question': 'Where can I find the detailed analysis of study time requirements?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To earn a certificate in this course, you need to submit at least 2 out of 3 course projects and review 3 peers’ projects by the deadline. If you successfully meet these requirements, you will receive a certificate.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'How can I earn a certificate in this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To receive a certificate, you need to submit at least 2 out of the 3 course projects and peer-review at least 3 projects from your course-mates for each submission by the deadline.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'What are the requirements to receive a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you do not need to complete all projects to get a certificate. You only need to submit at least 2 out of 3 projects and review 3 peers’ projects by the deadline.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'Do I need to complete all projects to get a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The deadline to qualify for the certificate is based on when you need to submit 2 out of 3 course projects and review 3 peers’ projects. Specific deadline dates can be found on the 2023 cohort page.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'What is the deadline to qualify for the certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is an option for a certificate without a robot image.',\n",
       "  'answer_orig': 'Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.',\n",
       "  'document': '2eba08e3',\n",
       "  'question': 'Is there an option for a certificate without a robot image?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you could still receive a certificate even if you don't complete the midterm project, as long as you finish at least 2 out of 3 projects and review 3 peers' projects by the deadline.\",\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': \"Will I receive a certificate if I don't complete the midterm project?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to graduate without submitting the midterm project, as you can still receive a certificate by completing at least 2 out of 3 projects and reviewing 3 peers’ projects by the deadline.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Can I still graduate without submitting the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you may still be eligible for certification if you missed the midterm project. As long as you complete at least 2 out of the 3 projects and review 3 peers’ projects by the deadline, you will receive a certificate.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Am I eligible for certification if I missed the midterm project?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not mandatory to complete the midterm project for the certificate. You can receive a certificate by submitting at least 2 out of the 3 course projects and reviewing 3 peers’ projects by the deadline.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Is it mandatory to complete the midterm project for the certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, missing the midterm project does not affect your certification eligibility as long as you submit at least two out of three projects and complete the peer reviews required. According to the course coordinator, only two projects are needed to obtain the course certificate.',\n",
       "  'answer_orig': \"Yes, it's possible. See the previous answer.\",\n",
       "  'document': '1d644223',\n",
       "  'question': 'Does missing the midterm project affect my certification eligibility?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To participate in this course, you should have a basic understanding of Python, specifically the following skills:\\n\\n1. How to run a Jupyter notebook.\\n2. How to import libraries and an understanding of what libraries are.\\n3. How to declare a variable and what variables are.\\n4. Important operations related to data analysis.\\n\\nIf you are familiar with these topics, you know enough Python for the course. If not, it is recommended that you review these basics before enrolling. You can also take the free course \"Learn Python Basics for Data Analysis\" on the OpenClassrooms e-learning platform to help you prepare.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'What Python basics are required for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can learn the necessary Python knowledge for this course by reading an article that outlines the required Python skills. If you know everything in that article, you are prepared for the course. If not, it\\'s recommended that you read the article and also consider joining the \"Introduction to Python\" course. Additionally, you can follow a free English course on the OpenClassrooms e-learning platform titled \"Learn Python Basics for Data Analysis,\" which covers the Python basics essential for data analysis, such as running a Jupyter notebook, importing libraries, declaring variables, and performing important data analysis operations.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'Where can I learn the necessary Python knowledge for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'It is important to know how to run a Jupyter notebook for this course because notebooks provide a user-friendly interface for developing and testing machine learning models. They allow for interactive coding, visualization of data, and step-by-step execution of code, which can simplify the training process compared to running scripts. Additionally, understanding how to navigate and execute commands in Jupyter notebooks is essential when working on projects that may require peer review, as seen with the need for a train.py file that complements the notebook for seamless collaboration.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'Why is it important to know how to run a Jupyter notebook for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For this course, you should know some basic operations regarding data analysis, including how to run a Jupyter notebook, how to import libraries and understand what libraries are, how to declare a variable and understand what variables are, and engage in important operations related to data analysis.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'What basic operations regarding data analysis should I know for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To learn how to declare variables in Python for this course, it\\'s recommended that you read the article mentioned in the FAQs. This article will provide the necessary information about basic Python knowledge, including how to declare a variable and what variables are. Additionally, you can follow the free English course, \"Learn Python Basics for Data Analysis,\" offered by OpenClassrooms, which covers Python basics that are essential for data analysis, including variable declaration.',\n",
       "  'answer_orig': 'Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\\nIntroduction to Python – Machine Learning Bookcamp\\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\\n(Mélanie Fouesnard)',\n",
       "  'document': '14890cd2',\n",
       "  'question': 'How can I learn to declare variables in Python for this course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For the course, all you need is a working laptop with an internet connection for the Machine Learning part. The Deep Learning part is more resource-intensive, but you can use a cloud service (Saturn Cloud is recommended, but others can be used as well).',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Are there any specific hardware requirements for the course, or is everything mainly based on cloud services?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, for the Machine Learning section, you just need a working laptop with an internet connection.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Do I just need a laptop and internet for the Machine Learning section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the Deep Learning part of the course is more resource-intensive than the Machine Learning part. While a working laptop with an internet connection is sufficient for the Machine Learning part, the Deep Learning part requires more resources, for which you can use cloud services (the course uses Saturn cloud but other options are available).',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Is the Deep Learning part of the course more resource-intensive than the Machine Learning part?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, Saturn Cloud can be used for the Deep Learning part of the course. While the Deep Learning section is more resource-intensive, you can utilize Saturn Cloud or any other cloud service for these tasks.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Can Saturn Cloud be used for the Deep Learning part of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to use a cloud service other than Saturn Cloud for the Deep Learning section. While Saturn Cloud is used in the course, any other cloud service can be utilized as well.',\n",
       "  'answer_orig': 'For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\\n(Rileen Sinha; based on response by Alexey on Slack)',\n",
       "  'document': 'a4fad482',\n",
       "  'question': 'Is it possible to use a cloud service other than Saturn Cloud for the Deep Learning section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find instructions to install TensorFlow GPU on Ubuntu by referring to this article: [https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/). Additionally, make sure to follow steps to install CUDA and cuDNN as they are necessary for GPU support.',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Where can I find instructions to install TensorFlow GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a guide to set up TensorFlow with GPU support on Ubuntu. You can find an article that worked for someone here: [Setting up TensorFlow with GPU on Ubuntu](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/). Additionally, you will need to install CUDA and cuDNN, and the context provides a simplified guide for that setup. Make sure to follow the installation instructions carefully to ensure proper functionality.',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Is there a guide to set up TensorFlow with GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To configure GPU support for TensorFlow on your Ubuntu machine, you'll need to install CUDA and cuDNN. A simplified guide for installation can be helpful. Additionally, you may find an article that outlines the process useful: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/. Make sure to follow the steps for installing the appropriate NVIDIA drivers and setting up the TensorFlow environment tailored for GPU usage.\",\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'How do I configure GPU support for TensorFlow on my Ubuntu machine?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can refer to the article linked here for guidance on setting up TensorFlow with GPU support on Ubuntu: [Setup TensorFlow with GPU support](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/). This resource includes helpful steps for the installation process.',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'Can you direct me to a resource for installing TensorFlow with GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set up TensorFlow with GPU support on Ubuntu, follow these general steps:\\n\\n1. **Install Nvidia Drivers**: You can download the necessary Nvidia drivers from the official Nvidia website: [Nvidia Drivers](https://www.nvidia.com/download/index.aspx).\\n\\n2. **Install CUDA Toolkit**: Download and install the CUDA toolkit (version 11.x.x) from the CUDA Toolkit Archive: [CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive).\\n\\n3. **Install cuDNN**: Download and install cuDNN from the following link: [cuDNN Download](https://developer.nvidia.com/rdp/cudnn-download).\\n\\n4. **Install TensorFlow**: You have two options:\\n   - Install TensorFlow with GPU support through Anaconda by running: `conda install tensorflow-gpu`.\\n   - Alternatively, install TensorFlow using pip by following the instructions at the TensorFlow installation guide for Linux: [TensorFlow Installation](https://www.tensorflow.org/install/pip#linux_1).\\n\\n5. **Set Environment Variables**: Make sure to follow any specific instructions for setting environment variables as necessary.\\n\\nFor a detailed guide tailored to your needs, you can refer to this article: [Setting up TensorFlow GPU on Ubuntu](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/).',\n",
       "  'answer_orig': 'Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/',\n",
       "  'document': '34b7fd35',\n",
       "  'question': 'What is the procedure to set up TensorFlow GPU on Ubuntu?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To join a channel in Slack, follow these steps:\\n\\n1. Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\n2. Browse the list of public channels in your workspace or use the search bar to search by channel name or description.\\n3. Select a channel from the list to view it.\\n4. Click Join Channel.\\n\\nFor further details, you can visit the Slack help page: [Join a channel](https://slack.com/help/articles/205239967-Join-a-channel).\",\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'How can I join a channel in Slack?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To find course channels on Slack, follow these steps:\\n\\n1. Go to the Slack workspace.\\n2. Click “All channels” at the top of your left sidebar. If you don’t see this option, click “More” to find it.\\n3. Browse the list of public channels or use the search bar to search by channel name or description.\\n4. Select the channel you want from the list to view it.\\n5. Click “Join Channel” to become a member. \\n\\nFor more detailed guidance, you can visit this link: https://slack.com/help/articles/205239967-Join-a-channel.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'What steps should I follow to find course channels on Slack?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you need to provide the URL to your GitHub repository. The repository must be public so that whoever corrects the homework can access it.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Is there a need to give the URL to our GitHub repo?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You need to provide the GitHub URL for your homework by including the URL to your repository. This URL is required in order to receive a grade for your homework.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Where do I provide the GitHub URL for our homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To join a Slack channel, follow these steps:\\n\\n1. Click \"All channels\" at the top of your left sidebar. If you don\\'t see this option, click “More” to find it.\\n2. Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\n3. Select a channel from the list to view it.\\n4. Click \"Join Channel.\"\\n\\nFor more detailed instructions, you can visit this link: https://slack.com/help/articles/205239967-Join-a-channel.',\n",
       "  'answer_orig': \"Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\\nClick “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\\nSelect a channel from the list to view it.\\nClick Join Channel.\\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\\nYes. You are required to provide the URL to your repo in order to receive a grade\",\n",
       "  'document': '4930aa19',\n",
       "  'question': 'Can you show me how to join a Slack channel?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can still participate in the course if you join late. However, you won’t be able to submit some of the homeworks. To receive a certificate, you need to submit 2 out of 3 course projects and review 3 peers' projects by the deadline.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'If I join the course late, can I still participate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can still obtain a certificate if you join the course after it has started. As long as you submit 2 out of 3 course projects and review 3 peers’ projects by the deadline, you will be eligible for a certificate.',\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'Will I be able to obtain a certificate if I join the course after it has started?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To be eligible for a certificate, you must submit 2 out of 3 course projects and review 3 peers’ projects by the deadline.',\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'How many course projects must I submit to be eligible for a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To receive a certificate, you are required to finish at least 2 out of 3 projects and review 3 peers' projects by the deadline.\",\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'What is the requirement for peer reviews to get a certificate?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can join the course after it has started. However, you won’t be able to submit some of the homeworks. Homework is not mandatory for finishing the course, but to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline.',\n",
       "  'answer_orig': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.',\n",
       "  'document': 'ee58a693',\n",
       "  'question': 'Can I submit homework if I join the course after it has started?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the course is available in a self-paced mode, allowing you to go through the materials at any time.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Is the course available in a self-paced mode?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can start the course anytime. The course is available in self-paced mode, allowing you to go through the materials at any time. If you prefer to do it with other students, the next cohort iterations will begin in September 2023 and September 2024.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Can I start the course anytime?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The next cohort will start in September 2023. Subsequent iterations are also expected to occur in September 2024 and potentially other Septembers as well.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'When is the next cohort starting?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The course materials are available in self-paced mode, so you can go through them at any time. There are no set times for the course materials if you choose this option. If you prefer to progress with a cohort, the next iterations will occur in September 2023 and September 2024.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Are there set times for the course materials?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there will be iterations in future Septembers, specifically in September 2024 and potentially in other Septembers as well.',\n",
       "  'answer_orig': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).',\n",
       "  'document': '636f55d5',\n",
       "  'question': 'Will there be iterations in future Septembers?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you cannot submit your homework after the due date. The form closes after the due date, and late submissions will not be evaluated. However, homework is not mandatory for finishing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Can I send in my homework late?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not feasible to submit homework after the deadline. The form is closed after the due date, and homework is not mandatory for finishing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Is it feasible to submit homework post deadline?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you miss the homework submission deadline, you will not be able to submit the homework as the form is closed after the due date. However, it's important to note that homework is not mandatory for finishing the course.\",\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'What happens if I miss the homework submission deadline?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it’s not possible to submit homework after the due date. The form is closed after the due date. However, homework is not mandatory for finishing the course.',\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'After the due date, can homework still be submitted?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"No, late homework submissions are not accepted. Once the due date has passed, the submission form is closed, and you cannot submit your homework. However, it's worth noting that homework is not mandatory for completing the course.\",\n",
       "  'answer_orig': 'No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course.',\n",
       "  'document': 'c839b764',\n",
       "  'question': 'Are late homework submissions accepted?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The initial step after joining the course is to go to the course page (http://mlzoomcamp.com/), scroll down, and start going through the course materials. Then, read everything in the cohort folder for your cohort’s year, and begin watching the videos and office hours from previous cohorts.',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'What is the initial step after joining the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To view the content of the course, go to the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/), scroll down, and start going through the course materials. Be sure to read everything in the cohort folder for your cohort’s year, click on the links, and start watching the videos. You can also access office hours from previous cohorts and check the DTC YouTube channel for playlists related to the course. Alternatively, you can use this link: [http://mlzoomcamp.com/#syllabus](http://mlzoomcamp.com/#syllabus).',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'How can I view the content of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can find the materials related to your cohort by going to the course page at http://mlzoomcamp.com/, scrolling down, and accessing the cohort folder for your specific cohort's year. Make sure to read everything in that folder and click on the links to start watching the videos. Additionally, you can watch office hours from previous cohorts on the DTC YouTube channel by searching for your course year in the Playlists. You can also use this direct link to access the syllabus: http://mlzoomcamp.com/#syllabus.\",\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'Where can I find the materials related to my cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To access the videos from previous office hours, go to the DTC YouTube channel, click on Playlists, and search for the relevant course year.',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'How do I access the videos from previous office hours?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'ML Zoomcamp was first launched in 2021.',\n",
       "  'answer_orig': 'Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\\nOr you can just use this link: http://mlzoomcamp.com/#syllabus',\n",
       "  'document': '0a278fb2',\n",
       "  'question': 'When was ML Zoomcamp first introduced?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the deadlines for the 2023 cohort by visiting the 2023 cohort page, where the deadlines are listed.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where can I find the deadlines for the 2023 cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The deadline information for the 2023 cohort is available by visiting the designated page (it’s taken from the 2023 cohort page).',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where is the deadline information available for the 2023 cohort?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the deadlines for the 2023 course, you can see them on the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'How do I check the deadlines for the 2023 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can see the deadlines for the 2023 cohort by referring to the 2023 cohort page.',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Can you show me where to see the 2023 cohort deadlines?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The deadlines for the 2023 cohort are listed on the 2023 cohort page. You can see them [here](it’s taken from the 2023 cohort page).',\n",
       "  'answer_orig': 'For the 2023 cohort, you can see the deadlines here (it’s taken from the 2023 cohort page)',\n",
       "  'document': '8de4fefd',\n",
       "  'question': 'Where are the 2023 cohort deadlines listed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The main changes between the 2022 and 2023 versions of the course are that there was one special module (BentoML) included in the previous iteration, while the rest of the modules remain the same. Additionally, the homework for the 2023 version is different from that of 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'What has changed between the 2022 and 2023 versions of the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a module missing in the 2023 course compared to 2022. The special module (BentoML) that was part of the previous iteration of the course in 2022 is not included in the 2023 course. However, the rest of the modules are the same as in 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Is there any module missing in the 2023 course compared to 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the homework assignments in the 2023 course are different from those in 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Are the homework assignments in the 2023 course the same as in 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, BentoML was included in the 2022 course but was a special module that is not part of the 2023 course iteration.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Was BentoML included in the 2022 course but not in the 2023 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the majority of modules are unchanged from the 2022 course. There was one special module (BentoML) in the previous iteration, but the rest of the modules remain the same as in 2022.',\n",
       "  'answer_orig': 'There’s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.',\n",
       "  'document': '94e86808',\n",
       "  'question': 'Are the majority of modules unchanged from the 2022 course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We won’t be releasing new course videos; we will be using the videos from 2021. The focus of the course and the skills we want to teach have remained the same, and the videos are still up-to-date. If you haven't taken part in the previous iteration, you can start watching the videos, and it will still be useful for your learning. However, it's recommended to use Python 3.10 instead of Python 3.8.\",\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Will you be releasing new course videos or using the ones from 2021?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the course videos are not being re-recorded for this iteration. The videos from the previous iteration are still up-to-date and will be used.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Are the course videos being re-recorded for this iteration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'There is no difference in the skills taught in this iteration compared to 2021. The focus of the course and the skills intended to be taught have remained the same.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Is there any difference in the skills taught in this iteration compared to 2021?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you should watch the videos even if you didn't take the course in 2021. The videos are still up-to-date, and you will find them useful for learning new things. However, it's recommended to use Python 3.10 instead of Python 3.8.\",\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': \"Should I watch the videos if I didn't take the course in 2021?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The recommended Python version for this course iteration is Python 3.10.',\n",
       "  'answer_orig': 'We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\\nIf you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.',\n",
       "  'document': 'e7ba6b8a',\n",
       "  'question': 'Which Python version is recommended for this course iteration?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When posting about what you learned from the course on your social media pages, use the tag **#mlzoomcamp**.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'What tag should I use when posting about my course learning on social media?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To submit your learning in public links when turning in homework, you should include the URLs of your social media posts in the designated section of the homework submission form. You should use the tag #mlzoomcamp when posting about what you learned. If you have multiple links, separate them using any whitespace character such as line breaks, spaces, or tabs. Keep in mind that you can earn a maximum of 7 points for this section, regardless of the number of links you submit; submitting more than 7 links will still cap your score at 7 points.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'How should I submit my learning in public links when turning in homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, it is not possible to earn more than 7 points for posting learning links in weekly homework. Regardless of the number of links posted, you can only receive a maximum of 7 points.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'Is it possible to earn more than 7 points for posting learning links in weekly homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can post the same content on multiple social sites to earn points for your homework. You can earn up to 7 points if you add 7 different URLs for your posts, even if the content is the same.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'Can I post the same content on multiple social sites to earn points for my homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can earn up to 14 points for posting public learning links during midterms and capstones. This is because the points awarded for learning in public links are doubled during this period, allowing you to submit 14 URLs for the projects.',\n",
       "  'answer_orig': 'When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.',\n",
       "  'document': 'f7bc2f65',\n",
       "  'question': 'How many points can I earn for posting public learning links during midterms and capstones?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Based on the provided context, it does not explicitly mention whether you can share your course projects on a public platform. However, it does state that you can create your own GitHub repository for the course, where you can include your notes, homework, and projects. This implies that sharing your projects in your own repository is permitted, but the context does not provide clear guidance on sharing them publicly beyond that.',\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Can I share my course projects on a public platform?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To add your notes to the Community Notes section, you can create your own GitHub repository for the course that includes your notes, homework, and projects. Then, fork the original course repository and add a link to your own repository under the 'Community Notes' section. After that, create a pull request to sync your fork with the original course repository.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'How do I add my notes to the Community Notes section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To sync your changes with the original course repo, first, create your own GitHub repository for the course with your notes, homework, projects, etc. Then, fork the original course repo and add a link under the 'Community Notes' section to the notes in your own repo. Once that is completed, create a pull request to sync your fork with the original course repo.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'What is the process to sync my changes with the original course repo?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You should host your course-related notes and projects by creating your own GitHub repository for the course. Include your notes, homework, and projects in that repository. Then, fork the original course repository and add a link under the 'Community Notes' section to your own repository. After doing that, create a pull request to sync your fork with the original course repository.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Where should I host my course-related notes and projects?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can add a link to your own repo in the course repository under the 'Community Notes' section. First, create your own GitHub repository for the course and fork the original course repo. After that, you can add the link to your notes in your own repo and then create a pull request to sync your fork with the original course repo.\",\n",
       "  'answer_orig': \"You can create your own github repository for the course with your notes, homework, projects, etc.\\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\\nAfter that's done, create a pull request to sync your fork with the original course repo.\\n(By Wesley Barreto)\",\n",
       "  'document': 'ae52a907',\n",
       "  'question': 'Is there a specific location to add a link to my own repo in the course repository?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the leaderboard links for 2023 and 2022 at the following URLs:\\n\\n- 2023: [Leaderboard Link](https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml)\\n- 2022: [Leaderboard Link](https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml)',\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'Where can I find the leaderboard links for 2023 and 2022?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To compute the hash for your email using Python, you can use the following code:\\n\\n```python\\nfrom hashlib import sha1\\n\\ndef compute_hash(email):\\n    return sha1(email.lower().encode('utf-8')).hexdigest()\\n\\n# Example usage - replace 'YOUR_EMAIL_HERE' with your actual email address\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\n```\\n\\nMake sure to replace 'YOUR_EMAIL_HERE' with your actual email address, and remember that the quotes are required to denote that your email is a string.\",\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'How do I compute the hash for my email using Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you do not need to format your email in any special way before computing the hash, other than ensuring it is in lowercase. The Python code provided will convert the email to lowercase automatically before computing the hash.',\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'Do I need to format my email in any special way before computing the hash?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you can use an online tool to compute your email's hash. You can enter your email on the website http://www.sha1-online.com to get the hashed value. After obtaining the hash, you can copy and paste it as needed.\",\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': \"Is there an online tool I can use to compute my email's hash?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To use the hashed email to check your scores on the leaderboard, you need to follow these steps:\\n\\n1. Compute the hash of your email using the provided Python code:\\n   ```python\\n   from hashlib import sha1\\n   def compute_hash(email):\\n       return sha1(email.lower().encode(\\'utf-8\\')).hexdigest()\\n   \\n   # Example usage\\n   email = \"YOUR_EMAIL_HERE\"\\n   hashed_email = compute_hash(email)\\n   print(\"Hashed Email (SHA-1):\", hashed_email)\\n   ```\\n   Make sure to replace `\"YOUR_EMAIL_HERE\"` with your actual email address.\\n\\n2. Alternatively, you can use the website http://www.sha1-online.com to compute your hashed email.\\n\\n3. After obtaining your hashed email, go to the leaderboard link provided for the corresponding year (for example, 2023:\\n   [Leaderboard 2023](https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml)).\\n\\n4. Finally, copy and paste your hashed email into the “research” bar of the leaderboard to find your scores.',\n",
       "  'answer_orig': \"Leaderboard Links:\\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\nPython Code:\\nfrom hashlib import sha1\\ndef compute_hash(email):\\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\\nYou need to call the function as follows:\\nprint(compute_hash('YOUR_EMAIL_HERE'))\\nThe quotes are required to denote that your email is a string.\\n(By Wesley Barreto)\\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\\n(Mélanie Fouesnard)\",\n",
       "  'document': 'dab5a24a',\n",
       "  'question': 'How do I use the hashed email to check my scores on the leaderboard?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To resolve the error 'wget is not recognized as an internal or external command' on a Windows system, you need to install wget. The easiest way to do this is by using Chocolatey; you can run the command `choco install wget`. Alternatively, you can download a binary from a source and place it in any location that is part of your PATH, such as `C:/tools/`.\",\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': \"How can I resolve 'wget is not recognized as an internal or external command' on a Windows system?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To install wget on a Mac, follow these steps:\\n\\n1. Open your terminal.\\n2. Use Homebrew to install wget by running the command:\\n   ```\\n   brew install wget\\n   ```\\n\\nAlternatively, you can use a Python wget library by first installing it with pip:\\n1. Install the library by running:\\n   ```\\n   pip install wget\\n   ```\\n2. In your Python code, use it by writing:\\n   ```python\\n   import wget\\n   wget.download(\"URL\")\\n   ```\\n\\nThis will download the file from the specified URL.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'What are the steps to install wget on a Mac?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can use Python to perform the same task as wget. You can utilize the `wget` library in Python by first installing it with pip:\\n\\n```bash\\npip install wget\\n```\\n\\nThen, in your Python code, you can download a file using:\\n\\n```python\\nimport wget\\nwget.download(\"URL\")\\n```\\n\\nThis will download the file located at the specified URL to the same directory as your code.\\n\\nAlternatively, you can use the built-in `urllib` library as follows:\\n\\n```python\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n```\\n\\nBoth methods will allow you to download files from the internet in a way similar to wget.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'Can I use Python to perform the same task as wget? If so, how?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can use the function `urllib.request.urlretrieve()` to download files from URLs in Python. This function allows you to retrieve files from the internet and save them locally. Here\\'s an example of how to use it:\\n\\n```python\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n``` \\n\\nThis will download the file from the specified URL and save it as \"housing.csv\" in your local directory.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': \"What function in Python's urllib.request can I use to download files from URLs?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can read a CSV file directly from a URL using pandas with the following code:\\n\\n```python\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\n```\\n\\nMake sure that the URL is valid; supported URL schemes include http, ftp, s3, gs, and file. If needed, you might have to bypass https checks using:\\n\\n```python\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\n```\\n\\nAlternatively, you can use the built-in Python functionality with `urllib.request`:\\n\\n```python\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\n```\\n\\nThis will allow you to download files from URLs and save them locally.',\n",
       "  'answer_orig': 'If you get “wget is not recognized as an internal or external command”, you need to install it.\\nOn Ubuntu, run\\nsudo apt-get install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\nchoco install wget\\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\\nOn Mac, the easiest way to install wget is to use brew.\\nBrew install wget\\nAlternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAnd then in your python code, for example in your jupyter notebook, use:\\nimport wget\\nwget.download(\"URL\")\\nThis should download whatever is at the URL in the same directory as your code.\\n(Memoona Tahira)\\nAlternatively, you can read a CSV file from a URL directly with pandas:\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\ndf = pd.read_csv(url)\\nValid URL schemes include http, ftp, s3, gs, and file.\\nIn some cases you might need to bypass https checks:\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\nOr you can use the built-in Python functionality for downloading the files:\\nimport urllib.request\\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\nurllib.request.urlretrieve(url, \"housing.csv\")\\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\\n(Mohammad Emad Sharifi)',\n",
       "  'document': '49f9bda9',\n",
       "  'question': 'How can I read a CSV file directly from a URL using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To download a CSV file inside a Jupyter notebook, you can use the following methods:\\n\\n1. **Using wget**:\\n   ```python\\n   !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n   ```\\n   (The exclamation mark `!` allows you to run shell commands in your notebook.)\\n\\n2. **Using the pandas library**:\\n   You can read a CSV file directly from a URL with pandas:\\n   ```python\\n   import pandas as pd\\n   url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n   df = pd.read_csv(url)\\n   ```\\n\\n3. **Using urllib**:\\n   You can also download the file using the built-in Python `urllib` module:\\n   ```python\\n   import urllib.request\\n   url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n   urllib.request.urlretrieve(url, \"housing.csv\")\\n   ```\\n\\nAll these methods will allow you to download and access the CSV file within your Jupyter notebook environment.',\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How can I download a CSV file inside a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In a Jupyter notebook, the exclamation mark (!) is used to run shell commands directly from the notebook. For example, if you use `!ls -lh`, it will execute the `ls -lh` command in the shell and display the output in the notebook. However, if you are using Windows and encounter an error with the `ls` command, you can use `!dir` to achieve similar functionality.',\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'What does the exclamation mark (!) do in a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To execute shell commands like `ls`, `cp`, and `mkdir` in a Jupyter notebook, you can use the `!` operator followed by the command you want to run. \\n\\nFor example:\\n- To list files in a directory, use: `!ls` (on Unix systems) or `!dir` (on Windows systems).\\n- To copy files, use: `!cp <source> <destination>` (on Unix systems) or the equivalent command for Windows.\\n- To create a directory, use: `!mkdir <directory_name>`.\\n\\nRemember that the availability of specific commands may vary based on your operating system. If you're using Windows and encounter errors like `'ls' is not recognized as an internal or external command`, switch to using `!dir` instead.\",\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How can I execute shell commands like ls, cp, and mkdir in a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To create a directory named 'data', use the command:\\n\\n```\\n!mkdir -p ../data/\\n```\",\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': \"What command do I use to create a directory named 'data'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To move a file into a different directory within a Jupyter notebook, you can use the `!mv` command. For example, if you want to move a file named `housing.csv` into a directory called `data`, you can execute the following commands in your notebook:\\n\\n1. First, create the directory if it doesn't exist:\\n   ```\\n   !mkdir -p ../data/\\n   ```\\n\\n2. Then, move the file to that directory:\\n   ```\\n   !mv housing.csv ../data/\\n   ```\\n\\nThis will relocate `housing.csv` into the `data` directory.\",\n",
       "  'answer_orig': 'You can use\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\\n!mkdir -p ../data/\\n!mv housing.csv ../data/',\n",
       "  'document': 'd44de7d1',\n",
       "  'question': 'How do I move a file into a different directory within a Jupyter notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set up a WSL development environment on a Windows 11 device, follow these steps:\\n\\n1. Use the Microsoft Learn link to set up a WSL development environment: [Set up a WSL development environment | Microsoft Learn](https://learn.microsoft.com/en-us/windows/wsl/install).\\n2. Ensure you have the “Virtual Machine Platform” feature activated in your Windows features. You can check this by searching for \"features\" in the research bar.\\n3. Install a Linux distribution such as ‘Ubuntu’ or ‘Debian’ from the Microsoft Store.\\n4. After installation, open the app and create a username and a secure password.\\n5. Once inside the Linux system, you can execute commands like `pwd` to verify you are in Linux.\\n6. To connect to VS Code, download the Microsoft verified VS Code extension ‘WSL’ which allows remote connections to your WSL Ubuntu instance similar to a virtual machine.\\n\\nMake sure your system supports virtualization in the BIOS settings as well.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'How can I set up a WSL development environment on a Windows 11 device?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To connect your WSL Ubuntu instance to VS Code on Windows 11, follow these steps:\\n\\n1. Make sure you have WSL set up on your Windows 11 device. You can refer to the Microsoft Learn link for guidance: Set up a WSL development environment | Microsoft Learn.\\n\\n2. Download and install the Microsoft verified VS Code extension called 'WSL'. This extension allows you to remotely connect to your WSL Ubuntu instance as if it were a virtual machine. \\n\\nFollowing these steps will enable you to use VS Code seamlessly with your WSL Ubuntu environment.\",\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'How do I connect my WSL Ubuntu instance to VS Code on Windows 11?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To connect VS Code to WSL, you should download the Microsoft verified VS Code extension ‘WSL’. This will allow you to remotely connect to your WSL Ubuntu instance.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Which extension should I download to connect VS Code to WSL?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can use VS Code to remotely connect to your WSL Ubuntu instance. To do this, you need to download the Microsoft verified VS Code extension ‘WSL’. This will allow you to connect to your WSL Ubuntu instance just as if it were a virtual machine. For more details, you can refer to the Microsoft Learn link on setting up a WSL development environment.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Can I use VS Code to remotely connect to my WSL Ubuntu instance?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a guide on Microsoft Learn for setting up WSL on Windows 11. You can find it at the link: Set up a WSL development environment | Microsoft Learn.',\n",
       "  'answer_orig': '(Tyler Simpson)',\n",
       "  'document': '314ebe32',\n",
       "  'question': 'Is there a guide on Microsoft Learn for setting up WSL on Windows 11?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix the error message \\'src refspec master does not match any\\' when using git push, you can run the following commands:\\n\\n1. First, create an initial commit with the command:\\n   ```\\n   git commit -m \"initial commit\"\\n   ```\\n2. Then, push your changes to the main branch using:\\n   ```\\n   git push origin main\\n   ```\\n\\nIf this is your first time using GitHub, you might also find a helpful tutorial at this link: https://dennisivy.com/github-quickstart.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"How can I fix the error message 'src refspec master does not match any' when using git push?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a great and straightforward tutorial on using GitHub for the first time at this link: [https://dennisivy.com/github-quickstart](https://dennisivy.com/github-quickstart).',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"Where can I find a tutorial on using GitHub if it's my first time?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To push an initial commit to GitHub, follow these steps:\\n\\n1. Make sure you have made your changes and are ready to commit them.\\n2. Run the command to commit your changes:\\n   ```\\n   git commit -m \"initial commit\"\\n   ```\\n3. Push your changes to GitHub using:\\n   ```\\n   git push origin main\\n   ```\\n\\nIf you encounter the error message \"src refspec master does not match any,\" ensure you\\'re pushing to the correct branch name, which is typically \"main\" for new repositories. You can also refer to this [GitHub quickstart tutorial](https://dennisivy.com/github-quickstart) for further guidance.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': 'What are the steps to push an initial commit to GitHub?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can use the “upload file” functionality from GitHub to submit your homework.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': \"Can I use the 'upload file' functionality on GitHub to submit my homework?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to directly share code from Google Colab to GitHub. You can do this by using the built-in functionality within Google Colab to save your notebooks directly to your GitHub repository.',\n",
       "  'answer_orig': 'This is my first time using Github to upload a code. I was getting the below error message when I type\\ngit push -u origin master:\\nerror: src refspec master does not match any\\nerror: failed to push some refs to \\'https://github.com/XXXXXX/1st-Homework.git\\'\\nSolution:\\nThe error message got fixed by running below commands:\\ngit commit -m \"initial commit\"\\ngit push origin main\\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\\n(Asia Saeed)\\nYou can also use the “upload file” functionality from GitHub for that\\nIf you write your code on Google colab you can also directly share it on your Github.\\n(By Pranab Sarma)',\n",
       "  'document': '98cff602',\n",
       "  'question': 'Is it possible to directly share code from Google Colab to GitHub?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A singular matrix error occurs during matrix inversion because not every matrix is invertible. In the homework, this error often arises when there is an issue with the order of matrix multiplication. Since multiplication is not commutative, the order of the matrices involved in the operation (using the method .dot) must be respected; otherwise, it can lead to incorrect results and a singular matrix error.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'Why does a singular matrix error occur during matrix inversion in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To avoid getting a singular matrix error while using the .dot method in your homework, be sure to pay close attention to the order of your matrix multiplication. Remember that multiplication is not commutative, meaning that \\\\( X.dot(Y) \\\\) is not necessarily equal to \\\\( Y.dot(X) \\\\). Therefore, you need to respect the order in which you perform the multiplication to avoid errors. Additionally, ensure that the matrices you're multiplying are suitable for inversion, as not every matrix can be inverted.\",\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'How can I avoid getting a singular matrix error while using .dot method in the homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The primary cause of a singular matrix error in matrix multiplication tasks is that not every matrix can be inverted. This error often arises because the order of multiplication matters; specifically, \\\\( X.dot(Y) \\\\) is not necessarily equal to \\\\( Y.dot(X) \\\\). Failing to respect this order can lead to incorrect results and result in a singular matrix error.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'What is the primary cause of a singular matrix error in matrix multiplication tasks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'In the homework, X.dot(Y) is not necessarily equal to Y.dot(X) because matrix multiplication is not commutative. The order of multiplying matrices matters; for a valid multiplication, the number of columns in the first matrix must match the number of rows in the second matrix. If this condition is not satisfied, or if the matrices are not conformable for multiplication, you may encounter a ValueError due to dimension mismatch.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'In the homework, why is X.dot(Y) not necessarily equal to Y.dot(X)?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To prevent singular matrix errors in the machine learning course, pay close attention to the following:\\n\\n1. **Matrix Multiplication Order**: Use the method `.dot()` for matrix multiplication and remember that multiplication is not commutative. This means that \\\\( X.dot(Y) \\\\) is not necessarily equal to \\\\( Y.dot(X) \\\\). Always respect the order of multiplication to avoid incorrect matrices.\\n\\n2. **Matrix Inversion**: Be cautious about inverting matrices in your code. If you calculate the inverse of a matrix \\\\( X \\\\) once, avoid doing it a second time without ensuring that the matrix has not changed in a way that makes it singular.\\n\\nBy focusing on these aspects, you can minimize the likelihood of encountering singular matrix errors.',\n",
       "  'answer_orig': \"I'm trying to invert the matrix but I got error that the matrix is singular matrix\\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\",\n",
       "  'document': '54ec0de4',\n",
       "  'question': 'What should I pay close attention to in order to prevent singular matrix errors in the machine learning course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If conda is not recognized as an internal command, you should check if you have Anaconda or Miniconda installed on your system. If not, you need to install one of them first. If you are on Windows and have just installed Anaconda, try using Anaconda’s own terminal called “Anaconda Prompt”.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': 'What should I do if conda is not recognized as an internal command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, the command 'conda create -n ml-zoomcamp python=3.9' will work with Python 3.8. The context states that any of Python versions 3.8, 3.9, or 3.10 should be fine.\",\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Will the command 'conda create -n ml-zoomcamp python=3.9' work with Python 3.8?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"It is not strictly necessary to use Anaconda's own terminal on Windows, but it is recommended. You can use Anaconda Prompt, which is provided as part of the Anaconda installation, to run commands like `conda create` and `conda activate`.\",\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Is it necessary to use Anaconda's own terminal on Windows?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, you need to install Anaconda or Miniconda if you don't already have them.\",\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': \"Do I need to install Anaconda or Miniconda if I don't already have them?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, the specified Python version for the course does not need to be exactly 3.9. While the recommended version for the 2023 cohort is Python 3.10, using Python 3.8 is also mentioned, and the course materials are still relevant regardless of the slight version differences.',\n",
       "  'answer_orig': 'I have a problem with my terminal. Command\\nconda create -n ml-zoomcamp python=3.9\\ndoesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\\nIf you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\\nIf you don’t have Anaconda or Miniconda, you should install it first\\n(Tatyana Mardvilko)',\n",
       "  'document': 'f81f4ecb',\n",
       "  'question': 'Does the specified Python version for the course need to be exactly 3.9?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To read a dataset with Pandas in Windows, you may encounter issues if you use backslashes in the file path. In Python, the backslash (`\\\\`) is treated as an escape character, which can cause conflicts. To avoid this, you should add an \"r\" before the file path so that Python treats it as a literal string instead. \\n\\nHere’s how you should load the file:\\n\\n```python\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n```\\n\\nBy using `r` in front of the path, you can successfully read the dataset without running into errors related to escape sequences.',\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"How do I read a dataset with Pandas in Windows when my code doesn't work?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The code `df = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')` does not work in Windows because the backslash (`\\\\`) used in the file path is interpreted by Python as an escape character, which can lead to conflicts. To avoid this issue, you should prepend the file path with an \"r\" to treat it as a raw string, like this: `df = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')`. This way, Python will correctly interpret the backslashes in the file path.',\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"Why does my code df = pd.read_csv('C:\\\\Users\\\\username\\\\Downloads\\\\data.csv') not work in Windows?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The reason behind the backslash causing an issue when reading files in Windows using Pandas is that Windows uses the backslash (\\\\) to navigate files, which conflicts with Python\\'s use of the backslash as an escape sequence. Escape sequences allow for the inclusion of special characters in strings, such as \"\\\\n\" for new lines or \"\\\\t\" for tabs. To avoid this issue, it is recommended to prefix the file path with \"r\" to treat it as a literal string, preventing Python from interpreting the backslashes as escape sequences.',\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': 'What is the reason behind backslash causing an issue when reading files in Windows using Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Adding 'r' before the file path in Windows with Pandas indicates that the string is a raw string literal. This means that Python will treat the backslashes in the file path as literal characters rather than escape sequences. This is necessary because in Python, the backslash has a special meaning as an escape character, which can cause conflicts when specifying file paths. By using the raw string format, you can correctly read the file without encountering issues related to escape sequences.\",\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': \"What does adding 'r' before the file path do when reading a file in Windows with Pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To read a file with Pandas in Windows, you should use the raw string format to avoid issues with the backslash in file paths. Here\\'s an example:\\n\\n```python\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n```\\n\\nBy adding \"r\" before the file path, Python treats it as a literal string, preventing any conflicts with escape sequences.',\n",
       "  'answer_orig': 'How do I read the dataset with Pandas in Windows?\\nI used the code below but not working\\ndf = pd.read_csv(\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\nUnlike Linux/Mac OS, Windows uses the backslash (\\\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the \\'\\\\\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\\\n\" to add a new line or \"\\\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\\nHere’s how we should be loading the file instead:\\ndf = pd.read_csv(r\\'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv\\')\\n(Muhammad Awon)',\n",
       "  'document': 'be760b92',\n",
       "  'question': 'Can you show an example of reading a file with Pandas in Windows?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix a \\'403 Forbidden\\' error when pushing to GitHub, you should run the following command:\\n\\n```bash\\ngit config -l | grep url\\n```\\n\\nEnsure that the output looks like this:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```\\n\\nIf it doesn\\'t, change it to the following format:\\n\\n```bash\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': \"What command should I run to fix a '403 Forbidden' error when pushing to GitHub?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the current URL configuration for your Git repository, you can use the following command:\\n\\n```bash\\ngit config -l | grep url\\n```\\n\\nThe output will show you the URL, formatted like this:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'How can I check the current URL configuration for my Git repository?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Before making changes to fix a GitHub push error, the `remote.origin.url` should look like this:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```\\n\\nYou should then change it to reflect the following format:\\n\\n```\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'What should the remote.origin.url look like before making changes to fix a GitHub push error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To change the remote URL and fix a \\'403 Forbidden\\' error when pushing to GitHub, you can follow these steps:\\n\\n1. First, check the current remote URL by typing the following command in your terminal:\\n   ```\\n   git config -l | grep url\\n   ```\\n   The output should look like this:\\n   ```\\n   remote.origin.url=https://github.com/github-username/github-repository-name.git\\n   ```\\n\\n2. Change the remote URL to the following format using the command below:\\n   ```\\n   git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n   ```\\n\\nEnsure that the changes are reflected by running the command in step 1 again.',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': \"How do I change the remote URL to fix a '403 Forbidden' error when pushing to GitHub?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To ensure the changes to the Git URL are correctly applied after updating, you should verify the configuration by using the following command:\\n\\n```\\ngit config -l | grep url\\n```\\n\\nThe output should show the updated URL in the format:\\n\\n```\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\n```\\n\\nMake sure that this reflects the changes you made with the command:\\n\\n```\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n```',\n",
       "  'answer_orig': 'Type the following command:\\ngit config -l | grep url\\nThe output should look like this:\\nremote.origin.url=https://github.com/github-username/github-repository-name.git\\nChange this to the following format and make sure the change is reflected using command in step 1:\\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\\n(Added by Dheeraj Karra)',\n",
       "  'document': 'a2cfa1c9',\n",
       "  'question': 'After updating the Git URL, how can I ensure the changes are correctly applied?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You are encountering the 'fatal: Authentication failed for https://github.com/username' error because support for password authentication was removed on August 13, 2021. To resolve this issue, you need to create a personal access token from your GitHub account and use it when making a push of your changes. For more information on currently recommended modes of authentication, you can refer to the GitHub documentation at: https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls.\",\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': \"Why am I encountering 'fatal: Authentication failed for https://github.com/username' error when pushing code?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If password authentication for GitHub is no longer supported, you should create a personal access token from your GitHub account and use it when you push your code. For detailed information about currently recommended modes of authentication, you can visit this link: [GitHub Docs](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls).',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'What should I do if password authentication for GitHub is no longer supported?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find information on currently recommended modes of authentication for GitHub after password support was removed by visiting the following link: [GitHub Authentication Documentation](https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls).',\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'Where can I find information on recommended authentications for GitHub after password support was removed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To configure your GitHub authentication if you can't use a password, you need to create a personal access token from your GitHub account and use that token when pushing changes. Additionally, you can set up SSH keys for authentication by following the instructions provided in the GitHub documentation on generating a new SSH key and adding it to the SSH agent. This way, you can authenticate with GitHub without needing to use a password.\",\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': \"How can I configure my GitHub authentication if I can't use a password?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To create a personal access token for GitHub authentication, you can follow the guide provided by GitHub. Specifically, you'll need to create a personal access token from your GitHub account and use it when pushing your changes. For detailed instructions, refer to the GitHub documentation at [this link](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).\",\n",
       "  'answer_orig': \"I had a problem when I tried to push my code from Git Bash:\\nremote: Support for password authentication was removed on August 13, 2021.\\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\\nfatal: Authentication failed for 'https://github.com/username\\nSolution:\\nCreate a personal access token from your github account and use it when you make a push of your last changes.\\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\nBruno Bedón\",\n",
       "  'document': '7b907071',\n",
       "  'question': 'Where do I create a personal access token for GitHub authentication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To resolve the \"wget: unable to resolve host address\" error in Kaggle, you should turn on the Internet for your session in the Kaggle notebook settings. You can find this option on the settings panel, located on the right-hand side of the Kaggle screen. Additionally, you may need to verify your phone number to confirm that you are not a bot.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'How can I resolve the wget: unable to resolve host address error in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When wget fails to resolve a host address in Kaggle, you should turn on the Internet for your session in your Kaggle notebook settings. This option is found on the settings panel on the right-hand side of the Kaggle screen. You may be required to verify your phone number to confirm that you are not a bot.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'What should I do when wget fails to resolve a host address in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To import data with `wget` in Kaggle without encountering errors, make sure to enable the Internet for your session. You can do this by accessing the settings panel on the right-hand side of the Kaggle screen. After enabling the Internet, you can use the command:\\n\\n```bash\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIf you still face issues with resolving the host address, double-check that the Internet is indeed enabled for your session.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'How to import data with wget in Kaggle without encountering errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fix wget errors in Kaggle, you need to adjust the settings in your Kaggle notebook by turning on the Internet for your session. This option is available on the settings panel on the right-hand side of the Kaggle screen. Please note that you will be asked to verify your phone number to confirm you are not a bot.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'What settings need to be adjusted in Kaggle to fix wget errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You need to verify your phone number to use wget in Kaggle because turning on the Internet for your session requires this step. Verifying your phone number helps Kaggle ensure that you are not a bot.',\n",
       "  'answer_orig': \"In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\\nwget: unable to resolve host address 'raw.githubusercontent.com'\\nSolution:\\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\",\n",
       "  'document': 'fc2e0a61',\n",
       "  'question': 'Why do I need to verify my phone number to use wget in Kaggle?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should watch the video titled \"Creating Virtual Environment for Python from VS Code\" to set up a virtual environment for Python in VS Code.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'What video should I watch to set up a virtual environment for Python in VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, it is possible to use Jupyter Notebooks in VS Code without a web browser. VS Code has native support for Jupyter Notebooks, allowing you to code in a Notebook directly within the VS Code environment.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'Is it possible to use Jupyter Notebooks in VS Code without a web browser?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To execute remote Jupyter Notebook files from your local machine using VS Code, you need to ensure that you have port forwarding enabled and run the `jupyter notebook` command from the remote machine. Additionally, you must have a remote connection configured in your `.ssh/config` file. Once set up, you can use VS Code's native Jupyter Notebooks support to code in the Notebook without needing to open a web browser. More details can be found in the VS Code Jupyter documentation: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\",\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'How can I execute remote Jupyter Notebooks files from my local machine using VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more information about working with GitHub from VS Code at the following link: https://code.visualstudio.com/docs/sourcecontrol/overview.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'Where can I find more information about working with GitHub from VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To run Jupyter Notebooks remotely using VS Code, you need to ensure the following setup:\\n\\n1. **Port Forwarding**: Make sure that port forwarding is enabled.\\n2. **Jupyter Notebook Command**: Run the command `jupyter notebook` from a remote machine.\\n3. **Remote Connection Configuration**: Configure a remote connection in your `.ssh/config` file.\\n\\nFollowing these steps will allow VS Code to execute remote Jupyter Notebook files from your local machine. For further assistance, you can refer to the official VS Code documentation on Jupyter Notebooks: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.',\n",
       "  'answer_orig': 'I found this video quite helpful: Creating Virtual Environment for Python from VS Code\\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\\n(Added by Ivan Brigida)',\n",
       "  'document': 'd43e5742',\n",
       "  'question': 'What setup is necessary to run Jupyter Notebooks remotely using VS Code?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'No, you do not need to run \"conda create -n ....\" every time you start VS Code to work on the project. This command is only necessary the first time to create the environment. After that, you only need to run \"conda activate ml-zoomcamp\" to activate the environment whenever you want to use it.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': \"Is it necessary to run 'conda create -n ...' every time I start VS Code to work on the project?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'After creating the environment, you should use the command \"conda activate ml-zoomcamp\" to activate it in VS Code.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'What command should I use to activate the environment in VS Code after creating it?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To save your current Conda environment to a file, you can use the command:\\n\\n```\\nconda env export > environment.yml\\n```\\n\\nThis will create a YAML file named `environment.yml` that contains all the dependencies of your existing environment. You can then recreate the environment using the following command:\\n\\n```\\nconda env create -f environment.yml\\n```',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'How can I save my current Conda environment to a file?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The command that allows you to recreate an environment from a saved YAML file is `conda env create -f environment.yml`.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'What command allows me to recreate an environment from a saved YAML file?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'After the Conda environment is created, you need to execute the command `conda activate ml-zoomcamp` to use it.',\n",
       "  'answer_orig': 'With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\\nAnswer:\\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\\n(Added by Wesley Barreto)\\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml',\n",
       "  'document': '32bc0538',\n",
       "  'question': 'After the Conda environment is created, which command is executed to use it?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When you invert a matrix and multiply it by the original matrix, you would typically expect to obtain an identity matrix. However, due to floating point precision issues in computer arithmetic, you may not get an exact identity matrix. Instead, you might see results like small numerical errors, for instance:\\n\\n```\\n[[ 1.00000000e+00 -1.38777878e-16]\\n [ 3.16968674e-13  1.00000000e+00]]\\n```\\n\\nThis discrepancy occurs because floating point math can introduce small errors when performing calculations, leading to results that are very close to but not exactly equal to the expected identity matrix.',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': \"If I invert a matrix and multiply by the original, why don't I get an exact identity matrix?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The matrices do not return an exact identity after multiplication due to floating point precision issues in computer calculations. Floating point math can lead to small errors, which means that when the inverse of a matrix is multiplied by the original matrix, the result may be very close to the identity matrix but not exactly equal to it. This can result in values like -1.38777878e-16 or 3.16968674e-13 appearing instead of perfect zeros.',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Why do the matrices not return an exact identity after multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The imprecise results after matrix inversion can be attributed to the limitations of floating point arithmetic in computers. When you invert a matrix and multiply it by the original matrix, ideally, the result should be an identity matrix. However, due to floating point precision issues, you may encounter very small values close to zero that affect the outcome, resulting in a matrix that resembles an identity matrix but includes tiny discrepancies, such as:\\n\\n```\\n[[ 1.00000000e+00 -1.38777878e-16]\\n [ 3.16968674e-13  1.00000000e+00]]\\n```\\n\\nThese discrepancies arise because floating point math may not produce exact results as expected, which is a known issue in computational mathematics.',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Can you explain the imprecise results after matrix inversion?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The slight errors observed when multiplying an inverted matrix by the original matrix are primarily due to floating point precision issues in computer arithmetic. When performing calculations with floating point numbers, the results may not be exact, leading to very small discrepancies, as illustrated in the example given in the FAQ. This is a common occurrence when dealing with matrix operations in programming and can be attributed to the inherent limitations of floating point representation.',\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'What causes the slight errors when multiplying an inverted matrix by the original?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Floating-point math affects the accuracy of matrix operations because it doesn't work well on computers, leading to small numerical errors. For example, when calculating the inverse of a matrix and multiplying it by the original matrix, the expected result should ideally be an identity matrix. However, due to the limitations of floating-point arithmetic, the result may deviate slightly, as shown in the example where the product yielded values very close to 1 but not exactly, indicating precision issues inherent in floating-point computations.\",\n",
       "  'answer_orig': \"I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\\nInverse * Original:\\n[[ 1.00000000e+00 -1.38777878e-16]\\n[ 3.16968674e-13  1.00000000e+00]]\\nSolution:\\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n(Added by Wesley Barreto)\",\n",
       "  'document': 'b6730228',\n",
       "  'question': 'Why does floating-point math affect the accuracy of matrix operations?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The command that helps print dataset information in pandas is `df.info()`. This function prints information about the dataset, such as the index datatype, number of entries, column information with not-null count and datatype, and memory usage by the dataset.',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'What command helps print dataset information in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the column info and memory usage of a pandas DataFrame, you can use the `info()` method. It prints details about the dataset, including the index datatype, number of entries, column information with not-null count and datatype, and memory usage by the dataset. You can use it as follows:\\n\\n```python\\ndf.info()\\n```',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'How to check column info and memory usage of a pandas DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To view the index datatype and the number of entries in a dataset, you should use the method `pandas.DataFrame.info()`. You can do this by calling it as follows:\\n\\n```python\\ndf.info()\\n``` \\n\\nThis will print the information about the dataset, including the index datatype and the number of entries.',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'What method should I use to view index datatype and the number of entries?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The pandas function that prints column data type and not-null count is `pandas.DataFrame.info()`.',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'Which pandas function prints column data type and not-null count?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To obtain general dataset info in pandas, you can use the `pandas.DataFrame.info()` method. This method prints information about the dataset, including the index datatype, number of entries, column information with not-null count and datatype, and memory usage by the dataset. You can call this method on your dataframe like this:\\n\\n```python\\ndf.info()\\n```',\n",
       "  'answer_orig': 'Answer:\\nIt prints the information about the dataset like:\\nIndex datatype\\nNo. of entries\\nColumn information with not-null count and datatype\\nMemory usage by dataset\\nWe use it as:\\ndf.info()\\n(Added by Aadarsha Shrestha & Emoghena Itakpe)',\n",
       "  'document': '3ce9bbb8',\n",
       "  'question': 'How can I obtain general dataset info in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you get a NameError for 'np' in your code, it likely means that the numpy library has not been imported. To resolve this, you should ensure that you include the following import statement at the beginning of your code:\\n\\n```python\\nimport numpy as np\\n```\\n\\nIt's also advisable to import the pandas library if you are using it, with the following line as well:\\n\\n```python\\nimport pandas as pd\\n``` \\n\\nMake sure these import statements are included before any other code that uses 'np' or 'pd'.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"What should I do if I get a NameError for 'np' in my code?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix a NameError for 'pd' when using pandas, make sure you have imported the pandas library at the beginning of your script. You should include the following line of code before any usage of 'pd':\\n\\n```python\\nimport pandas as pd\\n```\\nIf you are also using numpy, you should import it as well:\\n\\n```python\\nimport numpy as np\\n``` \\n\\nThis ensures that both libraries are properly imported and available for use in your code.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"How can I fix a NameError for 'pd' when using pandas?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If you receive a NameError for 'np', it means that the numpy library has not been imported.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"What libraries might not be imported if I receive a NameError for 'np'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You are seeing a NameError for 'pd' in your machine learning script because the pandas library has not been imported. To resolve this issue, make sure to include the following line at the beginning of your script:\\n\\n```python\\nimport pandas as pd\\n```\\n\\nThis will properly define 'pd' so that you can use it in your code.\",\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': \"Why am I seeing a NameError for 'pd' in my machine learning script?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution to the NameError issue in the FAQ section was added by Manuel Alejandro Aponte.',\n",
       "  'answer_orig': \"Pandas and numpy libraries are not being imported\\nNameError: name 'np' is not defined\\nNameError: name 'pd' is not defined\\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\\nimport pandas as pd\\nimport numpy as np\\nAdded by Manuel Alejandro Aponte\",\n",
       "  'document': '4e584d06',\n",
       "  'question': 'Who added the solution to the NameError issue in the FAQ section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To select columns by data type in a pandas dataframe, you can use the `select_dtypes` method. For example, to get all columns with numeric data types, you can use:\\n\\n```python\\ndf.select_dtypes(include=np.number).columns.tolist()\\n```\\n\\nTo select columns with object data types, you can use:\\n\\n```python\\ndf.select_dtypes(include='object').columns.tolist()\\n```\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'How can I select columns by data type in pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'A concise method to get numeric columns in a dataframe is to use the following code:\\n\\n```python\\ndf.select_dtypes(include=np.number).columns.tolist()\\n```',\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'What is a concise method to get numeric columns in a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To list columns with object data type in a dataframe, you can use the following code:\\n\\n```python\\ndf.select_dtypes(include='object').columns.tolist()\\n```\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'How do I list columns with object data type in a dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The pandas function to filter columns by dtype is `df.select_dtypes(include=np.number).columns.tolist()` for numeric data and `df.select_dtypes(include='object').columns.tolist()` for object data.\",\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': \"What's the pandas function to filter columns by dtype?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution for selecting columns by dtype was contributed by Gregory Morris.',\n",
       "  'answer_orig': \"What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\\ndf.select_dtypes(include=np.number).columns.tolist()\\ndf.select_dtypes(include='object').columns.tolist()\\nAdded by Gregory Morris\",\n",
       "  'document': 'ff4da2b6',\n",
       "  'question': 'Who contributed the solution for selecting columns by dtype?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To determine the structure of a dataset in Pandas, you can use the `.shape` attribute. This attribute provides information about the number of rows and columns in the dataset. For example, you can identify the number of rows with `df.shape[0]` and the number of columns with `df.shape[1]`. Additionally, you can use `df.info()` to get a detailed summary of the dataset, which includes index datatype, number of entries, column information with not-null count and datatype, and memory usage.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'How can I determine the structure of a dataset in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The attribute in Pandas that helps identify the number of rows in a dataset is the `.shape` attribute. You can use `df.shape[0]` to get the number of rows.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'What attribute in Pandas helps identify the number of rows in a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You would use the `.shape` attribute to identify the number of columns in a Pandas DataFrame. Specifically, you can access it with `df.shape[1]` to get the number of columns.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'Which attribute would you use to identify the number of columns in a Pandas DataFrame?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about identifying the shape of a dataset was contributed by Radikal Lukafiardi.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'Who contributed the information about identifying the shape of a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'One method to find the shape of a dataset in Pandas is by using the `.shape` attribute. For example, you can use `df.shape` to get the overall shape, `df.shape[0]` to identify the number of rows, and `df.shape[1]` to identify the number of columns.',\n",
       "  'answer_orig': 'There are many ways to identify the shape of dataset, one of them is using .shape attribute!\\ndf.shape\\ndf.shape[0] # for identify the number of rows\\ndf.shape[1] # for identify the number of columns\\nAdded by Radikal Lukafiardi',\n",
       "  'document': '58c1c168',\n",
       "  'question': 'What is one method to find the shape of a dataset in Pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To avoid value errors in your homework when performing matrix multiplication, you should use `np.dot()` or the `@` operator (or `np.matmul()`). It's essential to ensure that the number of columns in the first matrix matches the number of rows in the second matrix for the multiplication to be valid. Remember that the order of multiplication matters and can affect the result.\",\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'What function should I use for matrix multiplication to avoid value errors in our homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The order of matrices is important in matrix multiplication because changing the order changes the result, as they can yield matrices of different sizes and usually different values. Specifically, if you have two matrices A (m x n) and B (n x p), the product A * B results in a matrix C (m x p), while the product B * A results in a matrix D (n x n). Since C and D can be of different dimensions and values, the order of multiplication must be respected to achieve the correct result.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'Why is the order of matrices important in matrix multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check if the dimensions of two matrices are compatible for multiplication, you need to ensure that the number of columns in the first matrix matches the number of rows in the second matrix. If you encounter a dimension mismatch, consider rearranging the order of the matrices to satisfy this condition.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'How can I check if the dimensions of two matrices are compatible for multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The number of columns in the first matrix should match the number of rows in the second matrix when doing matrix multiplication.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'What should the number of columns in the first matrix match when doing matrix multiplication?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information on avoiding Value errors with array shapes was provided by Leah Gotladera.',\n",
       "  'answer_orig': 'First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\\nDimension Mismatch\\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\\nAdded by Leah Gotladera',\n",
       "  'document': '96076a1a',\n",
       "  'question': 'Who provided the information on avoiding Value errors with array shapes?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To handle NaN values in a column, you can replace them with the average of that column. First, calculate the average and save it to a variable. Then, replace the NaN values with this average variable. This process is known as imputing, and it allows you to retain rows with valuable information despite the presence of NaN values.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'How do I handle NaN values in a column?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'We should keep rows with NaN values because they may contain valuable information that contributes to other columns in the dataset. Instead of discarding these rows, we can impute the NaN values (for example, by replacing them with the average of the column) to retain that useful information while still making the dataset usable for analysis.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'Why should we keep rows with NaN values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"In machine learning, 'imputing' refers to the method of replacing NaN (null) values in a dataset with a specific value, often the average of the column. This is done instead of removing rows with NaN values to retain valuable information that may contribute to other columns.\",\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': \"What is meant by 'imputing' in machine learning?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Before replacing NaN values, you need to first calculate the average of the column where the NaN values are present and save it to a variable. Then, you can use that average variable to replace the NaN values. This process is known as imputing.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'What needs to be done before replacing NaN values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Replacing NaN values with the column average is done through a method called imputing. This is important because it allows you to retain rows that contain valuable information for other columns, rather than discarding those rows just because they have NaN values. By replacing the NaN values with the average of the column, you maintain the integrity of your dataset while addressing the missing data issue.',\n",
       "  'answer_orig': 'You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\\nAdded by Anneysha Sarkar',\n",
       "  'document': '3218389a',\n",
       "  'question': 'Why replace NaN values with the column average?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The mathematical formula for linear regression mentioned in Question 7 can be summarized as follows: a matrix \\\\( X \\\\) is multiplied by some unknown weights \\\\( w \\\\) resulting in the target \\\\( y \\\\).',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'What is the mathematical formula for linear regression mentioned in Question 7?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To solve the initial problem involving a Matrix X in linear regression, you need to multiply the Matrix X by some unknown weights w to obtain the target y. This process is fundamental to setting up the linear regression model. For further insights, you may want to explore additional readings such as \"Ordinary least squares,\" \"Multiple Linear Regression in Matrix Form,\" and \"Pseudoinverse Solution to OLS.\"',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'How can I solve the initial problem involving a Matrix X in linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The additional reading recommended for understanding ordinary least squares includes:\\n\\n- Ordinary least squares\\n- Multiple Linear Regression in Matrix Form\\n- Pseudoinverse Solution to OLS\\n\\nThese materials were added by Sylvia Schmitt and received comments from Dmytro Durach.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'What additional reading is recommended for understanding ordinary least squares?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more resources about multiple linear regression in matrix form in the additional reading and videos section of the course material. Here are some specific resources mentioned: \\n\\n- Ordinary least squares\\n- Multiple Linear Regression in Matrix Form\\n- Pseudoinverse Solution to OLS\\n\\nThese resources will provide you with valuable information on the topic.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'Where can I find more resources about multiple linear regression in matrix form?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about the mathematical formula for linear regression was added by Sylvia Schmitt, with comments from Dmytro Durach.',\n",
       "  'answer_orig': 'In Question 7 we are asked to calculate\\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\\nAdditional reading and videos:\\nOrdinary least squares\\nMultiple Linear Regression in Matrix Form\\nPseudoinverse Solution to OLS\\nAdded by Sylvia Schmitt\\nwith commends from Dmytro Durach',\n",
       "  'document': '183a1c90',\n",
       "  'question': 'Who added the information about the mathematical formula for linear regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The final multiplication does not have 5 columns in section 1 because it is likely that the first step of the multiplication was interchanged, which would lead to an incorrect shape of the resulting matrix.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Why does the final multiplication not have 5 columns in section 1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The issue with the final multiplication missing columns is most likely caused by interchanging the first step of the multiplication. This can lead to incorrect dimensions in the resulting matrix.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'What likely caused the issue with the final multiplication missing columns?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, interchanging the first step of multiplication can cause errors. In matrix multiplication, the order is important, as changing the order results in matrices of different sizes and usually different values. This can lead to errors if the dimensions do not align correctly for the intended calculation.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Can interchanging the first step of multiplication cause errors?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The note about the multiplication step error was added by Emmanuel Ikpesu.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'Who added the note about the multiplication step error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If your final multiplication in the first section is incorrect, you should check if you interchanged the first step of the multiplication. This may cause issues such as not having the correct number of columns. Additionally, remember that the order of multiplication is crucial when performing matrix operations, as it affects the result. Make sure the number of columns in the first matrix matches the number of rows in the second matrix for proper matrix multiplication.',\n",
       "  'answer_orig': 'This is most likely that you interchanged the first step of the multiplication\\nYou used  instead of\\nAdded by Emmanuel Ikpesu',\n",
       "  'document': 'f0bc1c19',\n",
       "  'question': 'What should I check if my final multiplication is incorrect in the first section?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'When performing matrix-matrix multiplication in Python, the preferred operators are `@` or `np.matmul()`.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'When performing matrix-matrix multiplication in Python, which operator is preferred?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Matrix-vector multiplication can be written as the * operator in some sources, but it is performed as the @ operator or np.matmul() via numpy. It is important to note that the * operator performs element-wise multiplication (Hadamard product).',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'How can matrix-vector multiplication be written in some sources?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The * symbol in numpy performs element-wise multiplication (Hadamard product).',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'What operator does the * symbol perform in numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'For scalar multiplication in numpy, you can use `numpy.multiply()` or the `*` operator.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'What functions can be used for scalar multiplication in numpy?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The information about multiplication operators was provided by Andrii Larkin.',\n",
       "  'answer_orig': 'Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\\nIf multiplying by a scalar numpy.multiply() or * is preferred.\\nAdded by Andrii Larkin',\n",
       "  'document': '735e6c78',\n",
       "  'question': 'Who provided the information about multiplication operators?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you encounter an ImportError related to Jinja2 when launching a new Jupyter notebook in a new environment, you should switch to the main environment and run the following command: \\n\\n```\\npip install nbconvert --upgrade\\n```',\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': 'What should I do if I encounter an ImportError when launching a new Jupyter notebook in a new environment?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To resolve the error related to 'contextfilter' from 'jinja2' when starting a Jupyter notebook, switch to the main environment and run the command `pip install nbconvert --upgrade`.\",\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"How can I resolve an error related to 'contextfilter' from 'jinja2' when starting a Jupyter notebook?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To fix the ImportError involving 'contextfilter' from 'jinja2', you should switch to the main environment and run the command:\\n\\n```\\npip install nbconvert --upgrade\\n```\",\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"What command should I run to fix the ImportError involving 'contextfilter' from 'jinja2'?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The solution for ImportError issues when launching Jupyter notebooks was added by George Chizhmak.',\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': 'Who added the solution for ImportError issues when launching Jupyter notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To solve the \\'contextfilter\\' ImportError in Jupyter notebook, you need to switch to the main environment and run the command: \"pip install nbconvert --upgrade\".',\n",
       "  'answer_orig': 'If you face an error kind of ImportError: cannot import name \\'contextfilter\\' from \\'jinja2\\' (anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) when launching a new notebook for a brand new environment.\\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\\nAdded by George Chizhmak',\n",
       "  'document': 'b8ca1cd3',\n",
       "  'question': \"Which package do I need to upgrade to solve the 'contextfilter' ImportError in Jupyter notebook?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you encounter wget hanging on MacOS Ventura M1 and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. After making this change, try running wget again.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'How do I fix wget hanging on MacOS Ventura M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If `wget` shows IPv6 addresses and hangs, you should go to your System Settings > Network > your network connection > Details > Configure IPv6, set it to Manually, then click OK. After doing this, try using `wget` again.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'What should I do if wget shows IPv6 addresses and hangs?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To configure IPv6 settings on MacOS Ventura M1, go to your System Settings, then select Network. Choose your network connection, click on Details, and under Configure IPv6, set it to Manually. Finally, click OK and try your action again.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'How do I configure IPv6 settings on MacOS Ventura M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To set IPv6 to Manually on MacOS Ventura, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK.',\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'Where can I find the option to set IPv6 to Manually on MacOS Ventura?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Wget does not come pre-installed with macOS, including MacOS M1. If you are facing issues with wget, you can use alternatives such as curl to download files. The recommended command to use with curl is:\\n\\n```bash\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIf you specifically want to use wget, you can install it using Homebrew by running:\\n\\n```bash\\nbrew install wget\\n```\\n\\nThis should help resolve any issues you're experiencing with wget on your MacOS M1 device.\",\n",
       "  'answer_orig': 'If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again',\n",
       "  'document': 'efdb235f',\n",
       "  'question': 'Is there a network setting adjustment for wget issues on MacOS M1?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If you are using macOS and having trouble with Wget, you can use `curl` as an alternative. Here is an example of how to use it:\\n\\n```bash\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\n`curl` is a utility for retrieving information from the internet, and this command will store the downloaded file with the name you provide.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'What alternative can I use to Wget if I am using macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To download a file using curl on macOS, you can use the following command:\\n\\n```bash\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIn this command:\\n- `curl` is a utility for retrieving information from the internet.\\n- `-o` tells curl to store the result as a file.\\n- `./housing.csv` is the name you choose for the downloaded file.\\n- The URL provided is the location of the file you want to download. \\n\\nThis command will extract data from the specified URL and save it as `housing.csv` in your current directory.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'Can you provide an example of using curl to download a file on macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The -o option in the curl command is used to tell curl to store the result as a file. You can choose the filename for the saved file, and curl will extract data from the specified web address (URL) and save it under the name you provide.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'What does the -o option do in the curl command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To specify the name of the file when using curl on macOS, you can use the `-o` option followed by the desired filename. For example, the command would look like this: \\n\\n```bash\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nIn this command, `-o` tells curl to store the result as a file, and `housing.csv` is the name you choose for the file.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'How do you specify the name of the file when using curl on macOS?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more information about using curl in the Curl Documentation, as mentioned in the context provided.',\n",
       "  'answer_orig': \"Wget doesn't ship with macOS, so there are other alternatives to use.\\nNo worries, we got curl:\\nexample:\\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nExplanations:\\ncurl: a utility for retrieving information from the internet.\\n-o: Tell it to store the result as a file.\\nfilename: You choose the file's name.\\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\\nMore about it at:\\nCurl Documentation\\nAdded by David Espejo\",\n",
       "  'document': '355348f0',\n",
       "  'question': 'Where can I find more information about using curl?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You can use the `round()` function or f-strings to limit the number of decimal places. For example, you can round a number to 4 decimal places using `round(number, 4)`. Alternatively, you can use f-strings like `print(f'Average mark for the Homework is {avg:.3f}')` to format a number to 3 decimal places. Additionally, if you want to round values in a whole Series, you can use `pandas.Series.round()`. Please refer to the documentation for more details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round.\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'What function can I use to limit a number of decimal places?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To round a number to 4 decimal places in Python, you can use the `round()` function like this:\\n\\n```python\\nrounded_number = round(number, 4)\\n```\\n\\nAdditionally, if you want to format a number while printing, you can use f-strings:\\n\\n```python\\nprint(f'Your number is {number:.4f}')\\n```\\n\\nIf you're working with a pandas Series and want to round all values in the Series, you can utilize the `round()` method available in pandas. For more details, check the pandas documentation at [pandas.Series.round](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'How can I round a number to 4 decimal places in Python?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To format a number to 3 decimal places using f-strings, you can use the following syntax:\\n\\n```python\\nprint(f'Average mark for the Homework is {avg:.3f}')\\n```\\n\\nThis will display the value of `avg` rounded to 3 decimal places.\",\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Can you demonstrate using f-strings to format a number to 3 decimal places?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a function to round an entire Series in pandas. You can use the `pandas.Series.round()` method to round the values in the whole Series. For more details, you can check the documentation [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round).',\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Is there a function to round an entire Series in pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find more information about rounding values in pandas Series by checking the documentation for the `pandas.Series.round` function at the following link: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round. This function allows you to round the values in the entire Series.',\n",
       "  'answer_orig': \"You can use round() function or f-strings\\nround(number, 4)  - this will round number up to 4 decimal places\\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\\nAlso there is pandas.Series. round idf you need to round values in the whole Series\\nPlease check the documentation\\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\nAdded by Olga Rudakova\",\n",
       "  'document': '67afabf5',\n",
       "  'question': 'Where can I find more information about rounding values in pandas Series?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can ask questions for the Live Sessions for Week 2 at the following link: [Ask questions for Live Sessions](https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I ask questions for the Live Sessions for Week 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The start date for Week 2 of the Machine Learning for Regression course is September 18, 2023.',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'What is the start date for Week 2 of the Machine Learning for Regression course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find the Week 2 homework for the course at the following link: [Week 2 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I find the Week 2 homework for the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can submit the Week 2 homework using the following link: [Submit HW Week 2](https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform). This link is also available at the bottom of the Week 2 homework page: [Week 2 HW](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'How can I submit the Week 2 homework?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can check the calendar for the weekly meetings at this link: [Calendar for weekly meetings](https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1).',\n",
       "  'answer_orig': 'Here are the crucial links for this Week 2 that starts September 18, 2023\\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n~~Nukta Bhatia~~',\n",
       "  'document': '50d737e7',\n",
       "  'question': 'Where can I check the calendar for the weekly meetings?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To visualize the distribution of the `median_house_value` in the housing dataset, you can use a histogram. Here\\'s an example of how to do this using Python:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\n```\\n\\nAlternatively, you can check the skewness of the `median_house_value` and use the `describe` method:\\n\\n```python\\n# Check skewness and describe\\nprint(df[\\'median_house_value\\'].describe())\\n\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n```',\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'How can we visualize the distribution of the median_house_value in the housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To check the skewness of the `median_house_value` variable from the dataset, you can use the following method:\\n\\n1. Calculate the skewness of the variable using the `skew()` function in pandas:\\n   ```python\\n   skewness = df[\\'median_house_value\\'].skew()\\n   ```\\n\\n2. Print the skewness value:\\n   ```python\\n   print(\"Skewness of \\'median_house_value\\':\", skewness)\\n   ```\\n\\nAdditionally, you can visualize the distribution of the variable using a histogram with seaborn:\\n```python\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\n```\\n\\nThese methods will help you check the skewness of the `median_house_value` variable in the dataset.',\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'What is a method to check the skewness of the median_house_value variable from the dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To generate a histogram for the `median_house_value`, the following Python libraries are needed:\\n\\n1. `pandas` - for data manipulation and analysis.\\n2. `matplotlib.pyplot` - for plotting graphs.\\n3. `seaborn` - for advanced data visualization.\\n\\nYou can use them as shown in the example:\\n\\n```python\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n```',\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'Which Python libraries are needed to generate a histogram for the median_house_value?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The command `sns.histplot(df['median_house_value'], kde=False)` creates a histogram of the 'median_house_value' column from the DataFrame `df`. The parameter `kde=False` indicates that no Kernel Density Estimate (KDE) curve will be added to the plot. This visual representation helps to understand the distribution and potential long tail of the 'median_house_value' data.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': \"What does the command sns.histplot(df['median_house_value'], kde=False) do?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Checking the distribution or skewness of the target variable is important in machine learning for regression because it helps to understand the characteristics of the data set. If the target variable is highly skewed, transforming it using logarithm distribution can be beneficial. This transformation can help to stabilize variance and make the model's assumptions more valid, thus potentially improving the performance of the regression model. Plotting the distribution of the target variable is the easiest way to evaluate its skewness.\",\n",
       "  'answer_orig': 'We can use histogram:\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Load the data\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\ndf = pd.read_csv(url)\\n# EDA\\nsns.histplot(df[\\'median_house_value\\'], kde=False)\\nplt.show()\\nOR ceck skewness and describe:\\nprint(df[\\'median_house_value\\'].describe())\\n# Calculate the skewness of the \\'median_house_value\\' variable\\nskewness = df[\\'median_house_value\\'].skew()\\n# Print the skewness value\\nprint(\"Skewness of \\'median_house_value\\':\", skewness)\\n(Mohammad Emad Sharifi)',\n",
       "  'document': 'bbc0fca3',\n",
       "  'question': 'Why is checking the distribution or skewness important in machine learning for regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"You might encounter a LinAlgError: Singular matrix error while following course videos due to a few reasons. Firstly, it can happen because not every matrix can be inverted, which is one of the main causes of a singular matrix error. Specifically, in the course, it often occurs when dealing with matrix multiplication, as the order in which matrices are multiplied is crucial. Make sure to respect the order of multiplication, as X.dot(Y) is not necessarily equal to Y.dot(X). \\n\\nAdditionally, if you have performed the inverse of X once in your code and attempt to do it again, that could also trigger the error. It's a normal part of the learning process, and the reason for this error will be further explained in the Regularization video.\",\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Why might I encounter a LinAlgError: Singular matrix error while following course videos?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, the Regularization video will explain why the LinAlgError: Singular matrix issue happens.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Will the Regularization video explain the LinAlgError: Singular matrix issue?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, it is normal to encounter a Singular Matrix error when following the course. It may happen during the lessons, and you will receive an explanation for it in the Regularization video. Don't worry, it's a common occurrence in the course.\",\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Is it normal to get a Singular Matrix error when following the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, performing the inverse of X twice in your code can cause a LinAlgError, specifically a Singular Matrix error. This can occur because not every matrix can be inverted, and if you attempt to invert it again after already performing the inverse, it may lead to such an error.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Can performing the inverse of X twice in my code cause a LinAlgError?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The additional information regarding the Singular Matrix error was provided by Cécile Guillot.',\n",
       "  'answer_orig': 'It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\\nYou can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\\n(Added by Cécile Guillot)',\n",
       "  'document': '6f3bdd20',\n",
       "  'question': 'Who provided the additional information regarding the Singular Matrix error?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The context provided does not specify a particular dataset used for regression in the course. However, it mentions several general sources for datasets, including Kaggle, UCI Machine Learning Repository, and others. You may need to choose an appropriate dataset from those sources for your regression tasks.',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'What dataset is used for regression in the course?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can find a detailed description of the California housing dataset [here](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Where can I find information about the California housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, details on the dataset used in Section 2, specifically the California housing dataset, can be found in the following link: [California housing dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Do you provide details on the datasets used in Section 2?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, you can learn more about the California housing dataset by visiting the following link: [California housing dataset description](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html).',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'Is there a link to learn more about the California housing dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To access the description for the dataset in Machine Learning for Regression, you will need to read the dataset directly from GitHub. You can do this by using the following code to load it into a pandas dataframe:\\n\\n```python\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```',\n",
       "  'answer_orig': 'You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\nKS',\n",
       "  'document': '27c2d90a',\n",
       "  'question': 'How can I access the description for the dataset in Machine Learning for Regression?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You got NaNs after applying .mean() when using for loops to calculate RMSE because there were NaN values present in your data. This issue occurred during the mean step of your RMSE function, specifically after squaring the errors. The root of the problem was that you only applied fillna(0) to the training data, not to the validation and test datasets. After applying fillna(0) to all datasets (train, val, test), the issue was resolved, and your for loops to calculate RMSE worked correctly.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'Why did I get NaNs after applying .mean() when using for loops to calculate RMSE?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'I identified the cause of NaNs in my RMSE calculation by tracing the problem back to where I first started to split the data. Initially, I had used `fillna(0)` only on the train data and not on the validation and test data. This resulted in NaNs appearing in the array after squaring the errors in the RMSE function. Once I applied `fillna(0)` to all the datasets (train, validation, and test), the issue was resolved and my for loops to calculate RMSE from all the seed values worked correctly.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'How did you identify the cause of NaNs in your RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The issue leading to NaNs in the RMSE function was due to NaN values in the data array. This occurred because the user had only applied `fillna(0)` to the training data and not to the validation and test data. Once `fillna(0)` was applied to all datasets (train, validation, and test), the NaN issue was resolved, and the RMSE calculations worked correctly.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'What was the issue with the data that led to NaNs in the RMSE function?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The problem of getting NaNs in the RMSE calculation was resolved by applying `fillna(0)` to all the datasets (train, validation, and test), instead of just the train data.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'What step resolved the problem of getting NaNs in the RMSE calculation?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You needed to apply `fillna(0)` on all datasets, including train, validation, and test, because if you only applied it to the training data, any NaNs left in the validation and test datasets would result in errors or NaN values during subsequent calculations, such as when computing metrics like RMSE. Ensuring that all datasets are filled eliminates the risk of encountering NaN values during analysis, which can disrupt the processing and evaluation of your machine learning models.',\n",
       "  'answer_orig': 'I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\\nAdded by Sasmito Yudha Husada',\n",
       "  'document': '88e9600a',\n",
       "  'question': 'Why did you need to apply fillna(0) on all datasets, including train, val, and test?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"We should transform the target variable to logarithm distribution when the target variable is highly skewed. This transformation helps to manage long tail distributions, such as prices, by stabilizing variance and making the data more normally distributed. The easiest way to evaluate this is by plotting the distribution of the target variable to assess skewness. However, it's important to note that if the target variable contains negative values, the logarithmic transformation cannot be applied.\",\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Why should we transform the target variable to logarithm distribution?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'We do not transform the target variable to logarithm distribution for all machine learning projects. This transformation is only applied if the target variable is highly skewed. The easiest way to evaluate skewness is by plotting the distribution of the target variable. If it has a long tail distribution, like in prices, you can transform it using the np.log1p() method, but this method cannot be used if the target variable has negative values.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Do we transform the target variable to logarithm distribution for all machine learning projects?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You should transform the target variable to a logarithm distribution when the target variable has a long tail distribution, such as prices, and exhibits a wide range. This can be done using the np.log1p() method. However, be cautious if your target variable has negative values, as this method will not be effective in that case. Additionally, transformation is beneficial if the target is highly skewed, which can be assessed by plotting its distribution.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'When should we transform the target variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To evaluate if the target variable is highly skewed, you can plot the distribution of the target variable using a histogram. This visual representation will help you understand the skewness of the data. Additionally, you can calculate the skewness value using the `.skew()` method in Python. A significantly skewed value indicates that the target variable may need transformation.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'How can we evaluate if the target variable is highly skewed?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Understanding the skewness of the target variable is important because it helps determine whether transformation, such as applying a logarithmic distribution, is needed. This is particularly relevant when the target variable has a long tail distribution, as in the case of prices. Evaluating the skewness by plotting the distribution of the target variable can guide decisions on how to transform the data, which can subsequently enhance the performance of machine learning models.',\n",
       "  'answer_orig': 'Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\\nhttps://en.wikipedia.org/wiki/Skewness\\nPastor Soto',\n",
       "  'document': 'd59d8df7',\n",
       "  'question': 'Why is it important to understand the skewness of the target variable?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can read a dataset directly from GitHub into a pandas dataframe using the following code:\\n\\n```python\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'How can I read a dataset directly from GitHub into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can load data from a GitHub link into pandas by using the following method:\\n\\n```python\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis reads the dataset directly into a pandas DataFrame.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'What method allows me to load data from a GitHub link into pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To read data from a GitHub URL using pandas, you can use the following code:\\n\\n```python\\ndf = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nThis will directly load the dataset into a pandas dataframe from the provided GitHub link.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'Could you explain how to read data from a GitHub URL using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To import a dataset from a GitHub link into a pandas dataframe, you can use the following code:\\n\\n```python\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'What is the code to import a dataset from a GitHub link into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To fetch data from a GitHub repository into a pandas dataframe, you can use the following method:\\n\\n```python\\ndfh = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n```\\n\\nAlternatively, you can also use the `requests` library to download the dataset first and then read it into a pandas dataframe:\\n\\n```python\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\n        \\ndf = pd.read_csv(\\'housing.csv\\')\\n```\\n\\nBoth methods will allow you to load the data from the GitHub repository into a pandas dataframe.',\n",
       "  'answer_orig': 'The dataset can be read directly to pandas dataframe from the github link using the technique shown below\\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\nKrishna Anand',\n",
       "  'document': '0b3eaf92',\n",
       "  'question': 'How do I fetch data from a GitHub repository into a pandas dataframe?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To load the dataset directly into Kaggle Notebooks, you can use the following command:\\n\\n```bash\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nMake sure to include the `!` before `wget`, as it is essential. Once the dataset is loaded to the Kaggle notebook server, you can read it using the following pandas command:\\n\\n```python\\ndf = pd.read_csv('housing.csv')\\n```\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'How can I load the dataset directly into Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'To download the dataset to Kaggle Notebooks, use the following command:\\n\\n```\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n```\\n\\nMake sure to include the `!` before the `wget` command.',\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'What command do I need to use to download the dataset to Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'Yes, there is a specific symbol you need to use before the wget command when using it in Google Colab. You should use the exclamation mark `!` to indicate that you are running a shell command. For example, the command would look like this: `!wget -P /content/drive/My\\\\ Drive/Downloads/ URL`.',\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'Is there a specific symbol I need to use before the wget command?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"To read the dataset in a Kaggle Notebook after downloading, you can use the following commands:\\n\\n1. Load the dataset using the `wget` command:\\n   ```python\\n   !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n   ```\\n\\n2. Once the dataset is loaded, read it using the pandas library:\\n   ```python\\n   df = pd.read_csv('housing.csv')\\n   ``` \\n\\nMake sure to include the `!` before `wget` when using the command in the notebook.\",\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'After downloading, how do I read the dataset in a Kaggle Notebook?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The instructions for loading the dataset in Kaggle Notebooks were provided by Harish Balasundaram.',\n",
       "  'answer_orig': \"For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\\ndf = pd.read_csv('housing.csv')\\nHarish Balasundaram\",\n",
       "  'document': '8fe56032',\n",
       "  'question': 'Who provided the instructions for loading the dataset in Kaggle Notebooks?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can filter a dataset by specific values in a column using the following methods:\\n\\n1. Using logical operators:\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\nHere, `|` is used for \\'OR\\' conditions.\\n\\n2. Using the `isin` method:\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nBoth approaches allow you to filter the dataset based on specific values in the \\'ocean_proximity\\' column.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'How can I filter a dataset by specific values in a column?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"The symbols used for 'OR' and 'AND' operations when filtering a dataset in Pandas are | for 'OR' and & for 'AND'.\",\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': \"What are the symbols used for 'OR' and 'AND' operations when filtering a dataset?\",\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"Yes, there is an alternative method to filter a dataset by specific column values. You can use the `isin` method as follows:\\n\\n```python\\ndf = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\\n```\\n\\nThis allows you to filter the dataset for multiple values in the specified column, instead of using `|` for 'OR' conditions.\",\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'Is there an alternative method to filter a dataset by specific column values?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can filter a dataset using multiple conditions in Pandas by employing logical operators. For example, to filter a dataset for rows where the \"ocean_proximity\" is either \"<1H OCEAN\" or \"INLAND\", you can use the following syntax:\\n\\n```python\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\n```\\n\\nAlternatively, you can use the `isin` method for a more concise approach:\\n\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nTo combine conditions using logical AND, you would use the `&` operator.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'Can you provide an example of filtering a dataset using multiple conditions?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'The `isin()` method in the context of filtering a dataset is used to filter rows based on whether the values in a specified column are within a given list of values. For example, you can use it as follows:\\n\\n```python\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\n```\\n\\nThis will return a subset of the dataset where the values in the `\"ocean_proximity\"` column are either `\"<1H OCEAN\"` or `\"INLAND\"`.',\n",
       "  'answer_orig': 'We can filter a dataset by using its values as below.\\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\\nYou can use | for ‘OR’, and & for ‘AND’\\nAlternative:\\ndf = df[df[\\'ocean_proximity\\'].isin([\\'<1H OCEAN\\', \\'INLAND\\'])]\\nRadikal Lukafiardi',\n",
       "  'document': 'af833e0a',\n",
       "  'question': 'What does the isin() method do in the context of filtering a dataset?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'An alternative way to load the data using the requests library is as follows:\\n\\n```python\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```\\n\\nThis approach retrieves the dataset from the specified URL and saves it to a file named `housing.csv`.',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What is an alternative way to load the data using the requests library?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can directly download the dataset from GitHub without using pandas by using the `requests` library in Python. Here’s how you can do it:\\n\\n```python\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```\\n\\nThis code sends a GET request to the dataset\\'s URL and saves the content to a file named \\'housing.csv\\' if the download is successful.',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'How can I directly download the dataset from GitHub without using pandas?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'You can fetch data for homework using the `requests` library with the following method:\\n\\n```python\\nimport requests\\n\\n# Get data for homework\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    with open(\\'housing.csv\\', \\'wb\\') as file:\\n        file.write(response.content)\\nelse:\\n    print(\"Download failed.\")\\n```\\n\\nThis code retrieves the dataset from GitHub and saves it as \\'housing.csv\\' on your local machine.',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'Could you share a method to fetch data for homework using the requests library?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': \"If the status code is 200 when downloading data with requests, you should open a file in write-binary mode and write the content of the response to that file. Here's how you can do it:\\n\\n```python\\nwith open('housing.csv', 'wb') as file:\\n    file.write(response.content)\\n```\\n\\nThis indicates that the download was successful, and you can proceed to save the downloaded data into a file.\",\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What should I do if the status code is 200 when downloading data with requests?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'answer_llm': 'If the file download fails when using the requests library, the code will print \"Download failed.\" This occurs when the response status code is not 200.',\n",
       "  'answer_orig': 'Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\\n# Get data for homework\\nimport requests\\nurl = \\'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\'\\nresponse = requests.get(url)\\nif response.status_code == 200:\\nwith open(\\'housing.csv\\', \\'wb\\') as file:\\nfile.write(response.content)\\nelse:\\nprint(\"Download failed.\")\\nTyler Simpson',\n",
       "  'document': '8d209d6d',\n",
       "  'question': 'What does the code using the requests library do if the file download fails?',\n",
       "  'course': 'machine-learning-zoomcamp'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gpt4omini_list = df.to_dict(orient=\"records\")\n",
    "results_gpt4omini_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62c56c3-6176-4fd2-8f5e-b4b8a1196b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gpt4omini_list[0]['answer_llm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05e2081-72fe-4ba4-94f5-38dda2ab88fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['answer_llm', 'answer_orig', 'document', 'question', 'course'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86171b4f-289e-4a75-9778-2103934df7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1380bfba-b7e7-4900-b611-324b0b7c3d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3604bb0e0e24dc788db2ef38411f22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(results_gpt4omini_list):\n",
    "    v_llm = embedding_model.encode(record['answer_llm'])\n",
    "    v_orig = embedding_model.encode(record['answer_orig'])\n",
    "    dp = v_llm.dot(v_orig)\n",
    "    evaluations.append(dp) #dotproduct = compute_similarity(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d022bdbf-474f-4006-9cfc-e1cd589a761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations= pd.DataFrame()\n",
    "df_evaluations['score'] = evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d1e86e-a50e-43e9-8795-46a8f84676b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.495996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.384743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.547926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.307844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.336861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.674304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.476013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "count  300.000000\n",
       "mean    27.495996\n",
       "std      6.384743\n",
       "min      4.547926\n",
       "25%     24.307844\n",
       "50%     28.336861\n",
       "75%     31.674304\n",
       "max     39.476013"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c615ff-0a96-43df-9b3e-f1a696da592b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d099fa-ca35-4087-bd6b-c151e5fab962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd16902-7fb7-4512-a9a8-9d2c5d53cb9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q3. Computing the cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae73a230-a8a6-4360-bb2b-f0d9dab71f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0a2183b-5727-413b-9217-b5ebaac46dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "733c2346-94c5-43e9-b0e3-1dda5f97b070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.11306959e-02, -1.53223593e-02, -3.43620703e-02,  1.98937692e-02,\n",
       "        2.64813774e-04, -2.77096275e-02, -1.63180511e-02,  3.27728651e-02,\n",
       "        1.02764834e-02,  4.32049185e-02,  2.97470968e-02, -1.64741985e-02,\n",
       "        4.28259596e-02,  8.87097139e-03,  2.40928703e-03, -4.06071218e-03,\n",
       "       -2.75038555e-02,  2.57525593e-02, -6.17438108e-02,  1.62241049e-02,\n",
       "       -3.53289098e-02,  5.32247871e-02,  5.25180344e-03,  2.28471104e-02,\n",
       "        3.92429205e-03,  1.27627961e-02,  1.35364877e-02,  7.77296051e-02,\n",
       "       -3.22006419e-02, -6.01691008e-02, -1.34758176e-02,  1.85390767e-02,\n",
       "       -1.31161604e-03,  2.39917058e-02, -1.60573945e-05, -1.42468023e-03,\n",
       "        9.87560004e-02, -1.14630526e-02, -3.72214094e-02, -3.54782380e-02,\n",
       "       -5.72972447e-02, -4.96582240e-02, -2.75296974e-03, -4.27222028e-02,\n",
       "        5.11458032e-02,  1.45730404e-02,  4.68182117e-02,  3.17602456e-02,\n",
       "        5.42251505e-02,  4.59975377e-02,  5.68841100e-02,  1.83609016e-02,\n",
       "        2.98353862e-02, -3.24024968e-02,  5.41718192e-02, -1.63467806e-02,\n",
       "        6.29811198e-04,  3.11492123e-02,  4.70835045e-02,  2.31318921e-02,\n",
       "       -8.46489239e-03,  1.33872571e-04,  3.18432115e-02,  3.31482179e-02,\n",
       "       -8.26483686e-03, -1.56679712e-02,  1.79711487e-02, -3.54666822e-02,\n",
       "        4.18729633e-02,  1.50031550e-02,  8.01149905e-02,  2.53812689e-02,\n",
       "        2.27106474e-02,  1.22047998e-02, -3.67775932e-02, -2.30282452e-02,\n",
       "        8.49403217e-02,  9.00111161e-03, -7.14501040e-03, -8.55221413e-03,\n",
       "       -3.21762674e-02,  2.03220057e-03, -3.85318324e-02,  4.88246465e-03,\n",
       "       -7.25077419e-03, -2.98444577e-03, -2.10257657e-02,  2.02412717e-02,\n",
       "       -7.52859423e-03,  2.77286731e-02,  5.31572476e-03,  6.75280578e-03,\n",
       "       -2.60883160e-02, -8.73529539e-03, -9.09537729e-03,  8.18100292e-03,\n",
       "        1.79951712e-02,  9.99110658e-03,  4.45813648e-02,  1.99396312e-02,\n",
       "        5.27160801e-02,  1.12725509e-04, -1.32769125e-03,  2.42050644e-02,\n",
       "        3.80980261e-02, -1.10945152e-02,  3.31491679e-02, -5.30739091e-02,\n",
       "        6.26145750e-02,  4.31150943e-02, -1.65331562e-03, -2.81008203e-02,\n",
       "        3.10667586e-02, -9.78925172e-03, -2.02849489e-02, -4.50967206e-03,\n",
       "        4.19694819e-02, -3.91752757e-02, -1.76575799e-02,  3.07020377e-02,\n",
       "       -4.81954068e-02, -8.99644475e-03, -2.22911984e-02,  3.24964747e-02,\n",
       "       -3.11031025e-02, -2.05608886e-02,  4.83186953e-02, -1.43219726e-02,\n",
       "        7.88380764e-03, -4.39192308e-03, -1.65364854e-02, -4.43966389e-02,\n",
       "       -4.22270820e-02, -2.29851343e-02,  3.46679948e-02, -5.88380359e-03,\n",
       "        5.91432489e-02, -2.98480932e-02,  3.73175927e-02, -2.17127558e-02,\n",
       "        6.75091892e-02,  1.70230549e-02,  1.77479684e-02,  9.50950459e-02,\n",
       "        2.29800493e-02,  8.34188331e-03,  3.56504880e-03,  3.67179736e-02,\n",
       "        6.68452028e-03,  8.96488223e-03, -4.16700728e-02, -8.18571635e-03,\n",
       "        2.76095439e-02,  6.47153929e-02, -8.52214545e-03, -5.20284334e-03,\n",
       "        1.68968551e-02, -2.68035289e-02, -5.21521941e-02, -3.44283655e-02,\n",
       "        5.37538528e-02, -1.99586675e-02,  2.84620672e-02, -1.14446087e-02,\n",
       "       -2.36016568e-02,  6.33683130e-02, -4.53997403e-02, -6.83913939e-03,\n",
       "        1.52745936e-02, -9.63552371e-02,  2.14549806e-02, -2.09308765e-03,\n",
       "       -1.20930402e-02, -1.90969910e-02, -5.40111996e-02,  7.55033121e-02,\n",
       "        4.14545350e-02, -2.55553164e-02, -3.83373387e-02,  5.91196865e-02,\n",
       "       -1.68528389e-02, -3.79348136e-02, -7.59430113e-04,  2.23977566e-02,\n",
       "        2.63417549e-02, -3.06184813e-02, -5.20173926e-03,  1.44081993e-03,\n",
       "        1.54217994e-02, -1.76848262e-03, -3.71117555e-02, -1.54140173e-02,\n",
       "       -3.47809494e-02,  1.21466536e-03,  1.68848485e-02, -7.80687435e-04,\n",
       "        4.60632592e-02, -9.35098249e-03,  9.83869657e-03,  3.66058685e-02,\n",
       "       -3.09528317e-03, -2.57379562e-03, -1.73776317e-03,  1.78437512e-02,\n",
       "        4.07574102e-02,  3.28406505e-02,  2.87236739e-02,  3.73708047e-02,\n",
       "       -8.94216597e-02, -2.09742542e-02,  2.76562124e-02,  1.26270752e-03,\n",
       "       -5.87341860e-02, -2.21676920e-02, -1.63521636e-02,  3.51784229e-02,\n",
       "        5.91804832e-02, -6.93627633e-03,  1.71188414e-02,  2.06318051e-02,\n",
       "       -2.22572982e-02,  1.80194620e-02, -1.02618955e-01,  5.95752569e-03,\n",
       "        1.86442770e-02, -1.69780746e-03, -2.08500382e-02, -1.99235752e-02,\n",
       "        3.22898105e-03, -6.76854774e-02,  2.44104881e-02, -1.49447275e-02,\n",
       "       -2.80260257e-02,  7.46359527e-02, -3.09600383e-02, -1.74658257e-03,\n",
       "        4.04557064e-02,  1.77383572e-02,  2.43751463e-02,  1.11731105e-02,\n",
       "       -1.29780872e-02,  8.74722078e-02,  2.57369466e-02,  5.84426057e-03,\n",
       "        2.42244694e-02, -4.68549281e-02, -3.61080207e-02, -1.07354894e-02,\n",
       "       -6.65898202e-03, -1.97749082e-02,  4.60527949e-02, -8.07998795e-03,\n",
       "       -2.50613932e-02,  2.55216211e-02,  4.57697697e-02,  2.35263016e-02,\n",
       "        1.34155564e-02, -7.03908280e-02, -2.14082580e-02, -2.47225221e-02,\n",
       "        5.20548634e-02, -5.86605296e-02,  1.43546788e-02,  2.63849106e-02,\n",
       "        5.97594772e-03,  9.99047048e-03,  1.88063830e-02,  4.92711104e-02,\n",
       "        9.35189947e-02,  1.36883184e-02,  1.71099175e-02,  6.63761189e-03,\n",
       "       -2.77416757e-03, -9.17123407e-02,  1.38650266e-02, -9.15683992e-03,\n",
       "       -3.30321081e-02,  3.21374275e-02, -3.05073317e-02, -1.31863914e-02,\n",
       "       -7.83279613e-02, -3.84540223e-02, -3.13955476e-03,  3.44636501e-03,\n",
       "       -3.08231283e-02, -2.03628875e-02, -1.08468463e-03, -6.33690283e-02,\n",
       "        8.46508332e-03,  2.40144506e-02, -3.89749743e-02, -1.47891510e-02,\n",
       "        4.59508859e-02,  1.17808012e-02, -6.30203113e-02, -1.42653589e-03,\n",
       "       -1.88991409e-02,  2.86193267e-02,  2.24058740e-02,  1.30714029e-02,\n",
       "       -3.49942483e-02,  2.57567670e-02, -4.08812128e-02,  4.62082922e-02,\n",
       "        5.57374861e-03, -1.32450201e-02,  4.71071675e-02, -8.24062228e-02,\n",
       "        2.06338475e-03, -5.47280610e-02,  1.23899607e-02,  3.46348770e-02,\n",
       "        1.57304164e-02, -3.52931991e-02,  3.93259563e-02,  5.34946509e-02,\n",
       "        4.13837172e-02,  5.26788086e-02,  2.28934716e-02, -3.79876904e-02,\n",
       "       -3.08844652e-02, -8.07494670e-03,  4.94750254e-02, -2.84403819e-03,\n",
       "       -7.29378387e-02, -3.42237987e-02,  1.47015536e-02, -4.31775935e-02,\n",
       "        1.91658530e-02,  1.30131897e-02,  2.08538342e-02,  2.24831682e-02,\n",
       "        1.37112588e-02,  5.18520623e-02, -1.81678031e-02, -1.02190003e-02,\n",
       "       -4.91344854e-02, -7.51880333e-02,  1.40858823e-02, -1.67288259e-02,\n",
       "       -7.66339451e-02,  4.79095168e-02,  5.90087473e-02,  2.60586366e-02,\n",
       "        1.67333279e-02, -4.48898859e-02, -5.40948026e-02,  7.92140048e-03,\n",
       "        2.43107858e-03, -2.90038176e-02,  1.49202012e-02,  7.13638216e-02,\n",
       "       -3.19939964e-02, -6.63953647e-02, -4.35652919e-02,  1.42246224e-02,\n",
       "        3.66908647e-02,  2.24842187e-02,  6.24883957e-02, -1.50674898e-02,\n",
       "        3.46734305e-04, -2.75728796e-02,  4.68070917e-02, -1.56481545e-02,\n",
       "        9.43956617e-03,  8.93686861e-02,  3.86360660e-02,  6.52365386e-02,\n",
       "        1.67384204e-02,  5.64238243e-02,  1.61375050e-02,  5.12066903e-03,\n",
       "       -3.18973027e-02,  3.75502259e-02,  2.25698184e-02, -4.45748344e-02,\n",
       "       -2.41184961e-02, -5.33949435e-02, -6.49871379e-02, -2.02469854e-03,\n",
       "       -4.40740259e-03,  4.75952141e-02,  1.41608678e-02,  8.96411482e-03,\n",
       "        2.16509104e-02,  6.82501346e-02,  3.03213019e-02,  2.96162236e-02,\n",
       "        5.44841476e-02,  5.09786755e-02, -2.07217373e-02,  1.60161052e-02,\n",
       "       -4.33484539e-02, -2.72889361e-02,  4.54508625e-02,  3.43527808e-03,\n",
       "       -1.25996303e-02, -6.21351600e-02,  5.97827695e-03,  6.42211689e-03,\n",
       "       -1.31513188e-02,  5.64741604e-02, -1.70647316e-02, -5.59062790e-03,\n",
       "       -1.21478019e-02,  2.82570142e-02,  4.69437763e-02, -4.93100546e-02,\n",
       "       -4.53400351e-02,  2.84811947e-02,  4.76193614e-02, -1.53266983e-02,\n",
       "       -5.61300330e-02,  1.31885335e-02, -3.91004942e-02, -1.90747366e-03,\n",
       "       -4.03336668e-03, -5.17429747e-02, -5.38696684e-02, -6.92168623e-02,\n",
       "       -7.96502233e-02,  4.18203212e-02, -2.32170452e-03, -4.65691909e-02,\n",
       "       -4.02262583e-02, -6.32020012e-02,  4.97814901e-02, -1.12783331e-02,\n",
       "       -4.85018417e-02,  2.21897215e-02, -1.93072055e-02, -1.41031770e-02,\n",
       "       -2.99676247e-02, -1.39215752e-03,  8.60504527e-03,  2.96307895e-02,\n",
       "        2.49980167e-02,  4.60648425e-02,  4.09025364e-02,  4.76703793e-02,\n",
       "       -2.05445066e-02, -4.41348925e-02,  2.74266638e-02, -6.91467710e-03,\n",
       "       -1.29305106e-03,  4.22992110e-02,  2.33272724e-02,  2.96329614e-02,\n",
       "       -1.02244979e-02,  6.51022792e-02, -3.49236503e-02,  3.83795844e-03,\n",
       "       -1.37026776e-02,  6.12161793e-02, -3.96440998e-02,  4.45691636e-03,\n",
       "        2.34881360e-02,  1.95712447e-02, -5.24401758e-03, -4.90038022e-02,\n",
       "        1.49582373e-02,  1.12838939e-01, -1.92837196e-03, -3.57967429e-03,\n",
       "       -1.71943605e-02, -6.51225001e-02,  6.86428770e-02, -1.17598213e-02,\n",
       "       -6.96777308e-04, -7.55034527e-03, -1.36340978e-02, -3.02756466e-02,\n",
       "        3.91655089e-03,  4.59793583e-02,  7.05575524e-03, -8.33517388e-02,\n",
       "        1.98215097e-02, -9.82170366e-03,  5.45179937e-03,  1.17115872e-02,\n",
       "        2.05010325e-02, -8.30699652e-02,  1.39913643e-02, -5.82459196e-02,\n",
       "        5.92276081e-02,  4.22990276e-03,  3.77329551e-02, -4.75160815e-02,\n",
       "       -3.47453691e-02,  8.99394974e-03, -4.18277308e-02, -2.29576733e-02,\n",
       "        2.78687257e-05, -2.92809717e-02, -3.26813012e-02, -2.25580446e-02,\n",
       "        1.00581562e-02, -2.26077829e-02, -1.61315072e-02, -5.57542639e-03,\n",
       "        1.89781170e-02, -4.12024595e-02,  4.09707315e-02, -5.16870953e-02,\n",
       "       -2.32519861e-02, -1.65148254e-03, -1.00876708e-02, -9.47692059e-03,\n",
       "       -5.48657030e-02, -4.37970087e-02,  4.01654691e-02, -2.31270306e-02,\n",
       "       -2.31997129e-02,  1.46242678e-02,  4.52802107e-02, -3.39664742e-02,\n",
       "       -2.40242854e-02,  8.50960333e-03, -3.39425765e-02, -7.84971863e-02,\n",
       "        1.39802406e-02,  4.14604284e-02,  2.53345370e-02, -1.63873881e-02,\n",
       "        1.30748283e-02,  3.09007820e-02, -1.05585353e-02, -4.88268547e-02,\n",
       "        2.87440047e-02, -2.21910253e-02, -3.62559818e-02,  2.54858006e-02,\n",
       "       -2.84193512e-02,  2.91994633e-03, -3.19681875e-02, -2.95182932e-02,\n",
       "       -5.30059598e-02, -4.39237840e-02,  3.42158377e-02,  3.47388498e-02,\n",
       "        1.06290902e-03, -3.75689827e-02,  5.02476357e-02,  9.52891912e-03,\n",
       "        2.68511362e-02, -1.19266436e-02, -4.09233682e-02, -2.45425235e-02,\n",
       "       -1.85878705e-02,  8.83122906e-03,  3.30209471e-02,  5.70924394e-03,\n",
       "       -2.12908722e-02,  7.95485005e-02, -4.77071665e-02, -6.98971748e-02,\n",
       "        1.37128374e-02, -7.88955018e-02, -1.45206731e-02, -1.44107454e-02,\n",
       "        2.29496206e-03,  1.57590136e-02, -1.71215180e-02,  3.01661268e-02,\n",
       "        7.34565407e-02, -4.31336239e-02, -5.45877777e-02, -1.02260988e-02,\n",
       "        7.46937282e-03,  2.36920640e-02, -2.03218963e-02,  2.51537329e-03,\n",
       "       -3.24345082e-02, -7.12451525e-03,  5.20505272e-02,  2.38259276e-03,\n",
       "       -2.60895584e-02, -3.80725674e-02, -2.87707895e-02,  4.42598462e-02,\n",
       "        3.15623656e-02, -3.41619328e-02,  2.70485021e-02, -2.75516398e-02,\n",
       "        6.92788064e-02, -2.96621695e-02, -5.00889262e-03, -2.50462424e-02,\n",
       "       -1.75702590e-02,  5.48605062e-03,  1.98363736e-02,  2.29198784e-02,\n",
       "        1.09094232e-02,  3.85529883e-02, -2.20176429e-02, -1.36113521e-02,\n",
       "       -1.84807219e-02,  6.83538914e-02,  5.86498342e-02, -3.26897344e-03,\n",
       "       -7.73717463e-02,  1.62199605e-02,  6.19378686e-02, -5.66031504e-03,\n",
       "        2.51480397e-02, -5.62479906e-03, -9.94769577e-03,  5.81580307e-03,\n",
       "       -8.86386726e-03,  1.57683939e-02, -5.54259717e-02,  6.16815388e-02,\n",
       "        5.94400652e-02,  3.80097367e-02,  1.59576237e-02, -4.10188362e-02,\n",
       "       -1.81704704e-02, -4.32400815e-02,  6.37712628e-02,  6.78045955e-03,\n",
       "       -5.92683144e-02,  3.33313122e-02, -1.37779600e-04, -2.06050910e-02,\n",
       "        1.57843214e-02,  1.74496882e-02,  5.55459447e-02, -4.23908159e-02,\n",
       "       -2.08360255e-02,  1.12296073e-02, -1.12460286e-03, -2.64167674e-02,\n",
       "        4.94405553e-02,  4.82321046e-02,  2.96059567e-02, -5.78580834e-02,\n",
       "        2.87214220e-02, -5.05626053e-02,  7.98703730e-03, -9.94560681e-03,\n",
       "        4.71072160e-02, -5.35482764e-02, -4.20093946e-02, -1.59418397e-02,\n",
       "        1.77849308e-02,  1.44229196e-02,  4.14634980e-02,  2.95456257e-02,\n",
       "        1.53276194e-02, -1.42393000e-02, -2.49292403e-02, -3.65284644e-02,\n",
       "       -2.64467429e-02, -1.07968850e-02, -3.83402221e-02, -5.66987367e-03,\n",
       "        7.41722062e-02,  2.04548519e-03,  7.65292123e-02,  4.01378945e-02,\n",
       "        5.89364469e-02,  4.31923680e-02,  8.77965335e-03,  2.41685004e-04,\n",
       "        1.88476872e-02,  1.38472402e-02, -2.31850054e-02,  5.38901836e-02,\n",
       "        8.86067376e-02,  1.98084535e-03,  3.39444391e-02,  4.11213748e-02,\n",
       "       -5.99208549e-02,  2.41467338e-02, -6.25041798e-02, -2.16482002e-02,\n",
       "       -2.42391918e-02,  8.84196348e-03, -8.16205423e-03,  3.27250846e-02,\n",
       "       -6.51320741e-02, -1.45175932e-02, -1.16264097e-01,  1.32612698e-03,\n",
       "       -1.49899526e-02, -2.24451185e-03,  5.25494688e-04, -2.56792568e-02,\n",
       "        2.67145783e-02, -2.08824258e-02,  8.82358849e-02,  7.59239718e-02,\n",
       "        6.04949631e-02, -1.83617007e-02, -7.74816144e-03, -5.04010022e-02,\n",
       "        5.23919798e-02, -1.04295984e-02,  5.51448800e-02, -2.72309501e-02,\n",
       "        8.52695946e-03, -1.47415018e-02,  1.76266842e-02,  6.44240826e-02,\n",
       "        6.41189341e-04,  2.18756255e-02,  1.37881301e-02,  1.61015429e-02,\n",
       "       -4.32464946e-03,  7.84128997e-03, -1.21808108e-02, -2.47262698e-02,\n",
       "       -2.22270060e-02,  5.09146787e-02, -4.89335917e-02,  4.44964170e-02,\n",
       "        4.27893130e-03, -4.21271622e-02, -9.53078270e-03, -1.53086800e-02,\n",
       "        6.82983100e-02,  5.31823263e-02,  5.90701215e-02, -4.82736453e-02,\n",
       "       -4.11199443e-02,  1.79872904e-02,  2.30891146e-02,  4.78609372e-03,\n",
       "       -5.78164421e-02,  2.33832803e-02,  2.10929886e-02,  5.68993986e-02,\n",
       "        5.69851045e-03, -2.98431981e-02,  9.99758691e-02, -9.52224061e-03,\n",
       "        2.61733942e-02,  3.81460227e-03, -2.94302274e-02, -2.44853701e-02,\n",
       "       -1.98679511e-02, -1.32425195e-02,  1.71878245e-02,  1.00467280e-02,\n",
       "       -1.20644793e-02, -6.15471378e-02, -1.73938349e-02,  5.79589829e-02,\n",
       "       -2.13369709e-02, -1.18786758e-02, -1.57838892e-02,  2.11497433e-02,\n",
       "        7.26078916e-03,  1.94071084e-02, -2.36190967e-02,  1.93420667e-02,\n",
       "        2.44921092e-02, -9.03189648e-03, -3.51028554e-02,  3.34617645e-02,\n",
       "       -1.48809627e-02, -1.97614059e-02,  1.57857705e-02,  2.33233236e-02,\n",
       "        3.11737843e-02, -2.58034691e-02,  2.62350421e-02, -3.86959761e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizing(v_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cca8a372-8f82-4ed4-80f2-6570b01015d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.515997,\n",
       " 13.4184065,\n",
       " 25.31325,\n",
       " 12.147419,\n",
       " 18.747728,\n",
       " 33.970398,\n",
       " 30.251701,\n",
       " 29.521585,\n",
       " 35.2722,\n",
       " 27.751759,\n",
       " 32.34471,\n",
       " 31.441847,\n",
       " 36.38073,\n",
       " 33.340515,\n",
       " 30.606163,\n",
       " 32.503044,\n",
       " 29.674446,\n",
       " 24.353462,\n",
       " 20.13246,\n",
       " 23.995468,\n",
       " 30.880272,\n",
       " 32.692432,\n",
       " 30.049168,\n",
       " 16.078167,\n",
       " 31.796417,\n",
       " 37.98001,\n",
       " 20.839043,\n",
       " 32.612854,\n",
       " 38.894203,\n",
       " 34.051826,\n",
       " 28.26388,\n",
       " 27.124832,\n",
       " 23.975266,\n",
       " 26.34014,\n",
       " 18.658117,\n",
       " 25.016396,\n",
       " 21.101133,\n",
       " 33.726788,\n",
       " 29.340345,\n",
       " 28.65449,\n",
       " 29.608582,\n",
       " 30.810736,\n",
       " 33.3312,\n",
       " 26.220486,\n",
       " 26.550077,\n",
       " 13.148602,\n",
       " 12.962549,\n",
       " 12.275612,\n",
       " 9.974444,\n",
       " 10.883927,\n",
       " 29.84507,\n",
       " 32.36179,\n",
       " 22.187178,\n",
       " 30.268929,\n",
       " 25.091877,\n",
       " 32.742783,\n",
       " 28.22099,\n",
       " 27.274975,\n",
       " 24.208641,\n",
       " 22.568905,\n",
       " 19.767452,\n",
       " 18.679333,\n",
       " 20.422318,\n",
       " 22.051323,\n",
       " 18.188011,\n",
       " 28.455889,\n",
       " 25.919704,\n",
       " 23.332329,\n",
       " 22.205935,\n",
       " 28.296305,\n",
       " 39.23055,\n",
       " 36.758514,\n",
       " 31.913893,\n",
       " 31.202858,\n",
       " 36.91305,\n",
       " 30.514198,\n",
       " 36.261467,\n",
       " 27.397549,\n",
       " 37.7928,\n",
       " 23.297688,\n",
       " 34.25258,\n",
       " 34.550613,\n",
       " 30.316456,\n",
       " 35.70352,\n",
       " 31.012527,\n",
       " 35.459606,\n",
       " 35.075764,\n",
       " 35.429825,\n",
       " 29.881172,\n",
       " 30.037115,\n",
       " 31.247982,\n",
       " 29.89394,\n",
       " 28.52552,\n",
       " 31.754606,\n",
       " 32.59006,\n",
       " 39.476013,\n",
       " 34.973724,\n",
       " 28.725086,\n",
       " 30.6841,\n",
       " 37.26411,\n",
       " 35.626842,\n",
       " 33.202927,\n",
       " 25.320385,\n",
       " 32.188915,\n",
       " 22.51867,\n",
       " 30.810644,\n",
       " 37.474186,\n",
       " 27.127542,\n",
       " 27.558426,\n",
       " 33.077446,\n",
       " 25.819044,\n",
       " 35.047585,\n",
       " 34.910133,\n",
       " 35.862923,\n",
       " 35.66462,\n",
       " 23.890945,\n",
       " 26.650799,\n",
       " 19.03632,\n",
       " 23.732626,\n",
       " 36.686493,\n",
       " 23.632248,\n",
       " 26.224363,\n",
       " 28.331976,\n",
       " 28.205883,\n",
       " 29.783936,\n",
       " 27.14346,\n",
       " 14.999909,\n",
       " 18.169992,\n",
       " 18.22865,\n",
       " 20.363129,\n",
       " 5.448556,\n",
       " 5.932483,\n",
       " 7.870978,\n",
       " 4.547926,\n",
       " 5.385088,\n",
       " 27.713978,\n",
       " 20.70014,\n",
       " 27.329466,\n",
       " 16.532581,\n",
       " 19.901352,\n",
       " 31.073858,\n",
       " 29.63746,\n",
       " 28.450882,\n",
       " 24.889107,\n",
       " 26.59752,\n",
       " 26.664059,\n",
       " 33.775314,\n",
       " 28.7658,\n",
       " 19.707882,\n",
       " 17.306507,\n",
       " 34.357483,\n",
       " 30.779724,\n",
       " 30.170496,\n",
       " 27.354588,\n",
       " 32.209927,\n",
       " 26.880234,\n",
       " 28.167713,\n",
       " 29.976238,\n",
       " 28.072685,\n",
       " 31.957588,\n",
       " 30.330582,\n",
       " 29.305618,\n",
       " 27.504326,\n",
       " 27.409878,\n",
       " 26.012466,\n",
       " 31.341301,\n",
       " 29.24813,\n",
       " 34.054394,\n",
       " 29.529327,\n",
       " 27.144753,\n",
       " 26.034893,\n",
       " 31.496738,\n",
       " 32.259842,\n",
       " 21.932495,\n",
       " 30.88007,\n",
       " 39.092705,\n",
       " 32.14211,\n",
       " 25.34589,\n",
       " 23.977585,\n",
       " 27.314087,\n",
       " 30.87744,\n",
       " 28.470531,\n",
       " 28.86776,\n",
       " 28.17321,\n",
       " 27.834417,\n",
       " 33.211502,\n",
       " 27.782202,\n",
       " 28.150196,\n",
       " 27.548817,\n",
       " 29.624218,\n",
       " 28.466618,\n",
       " 27.704876,\n",
       " 27.51704,\n",
       " 26.017353,\n",
       " 16.815933,\n",
       " 29.18188,\n",
       " 30.130188,\n",
       " 27.391752,\n",
       " 28.57114,\n",
       " 21.442024,\n",
       " 29.0719,\n",
       " 27.174828,\n",
       " 26.663849,\n",
       " 26.153393,\n",
       " 29.611273,\n",
       " 27.712759,\n",
       " 17.301191,\n",
       " 25.073742,\n",
       " 26.105171,\n",
       " 15.056329,\n",
       " 32.224457,\n",
       " 26.998768,\n",
       " 24.001842,\n",
       " 30.299423,\n",
       " 31.251953,\n",
       " 24.931458,\n",
       " 27.136257,\n",
       " 20.048168,\n",
       " 22.204578,\n",
       " 18.39867,\n",
       " 23.471605,\n",
       " 23.65659,\n",
       " 20.096872,\n",
       " 27.78518,\n",
       " 23.721882,\n",
       " 29.47656,\n",
       " 31.923643,\n",
       " 27.786556,\n",
       " 25.28249,\n",
       " 21.090363,\n",
       " 34.241966,\n",
       " 34.5661,\n",
       " 35.93258,\n",
       " 22.799603,\n",
       " 33.241447,\n",
       " 19.981958,\n",
       " 22.636581,\n",
       " 24.131182,\n",
       " 23.120184,\n",
       " 11.905024,\n",
       " 32.218346,\n",
       " 29.58118,\n",
       " 22.276997,\n",
       " 26.254911,\n",
       " 18.278994,\n",
       " 33.623142,\n",
       " 29.776499,\n",
       " 30.866945,\n",
       " 26.539066,\n",
       " 26.438843,\n",
       " 23.820015,\n",
       " 27.360676,\n",
       " 29.637075,\n",
       " 31.316284,\n",
       " 20.207516,\n",
       " 34.520557,\n",
       " 33.354904,\n",
       " 28.474087,\n",
       " 27.6943,\n",
       " 21.486216,\n",
       " 29.00384,\n",
       " 26.53431,\n",
       " 28.94831,\n",
       " 27.005249,\n",
       " 24.340912,\n",
       " 21.608488,\n",
       " 33.209812,\n",
       " 31.647537,\n",
       " 30.631363,\n",
       " 25.52513,\n",
       " 31.80573,\n",
       " 34.976177,\n",
       " 29.750322,\n",
       " 28.022009,\n",
       " 24.982285,\n",
       " 34.44129,\n",
       " 33.405495,\n",
       " 30.952478,\n",
       " 25.19504,\n",
       " 29.613966,\n",
       " 31.487972,\n",
       " 29.713959,\n",
       " 28.341745,\n",
       " 28.797325,\n",
       " 28.471718,\n",
       " 38.855076,\n",
       " 35.335587,\n",
       " 13.9045925,\n",
       " 38.246567,\n",
       " 30.029453,\n",
       " 33.36308,\n",
       " 25.7122,\n",
       " 32.535797,\n",
       " 31.411255,\n",
       " 30.524258,\n",
       " 34.001778,\n",
       " 33.690865,\n",
       " 34.491535,\n",
       " 27.53835,\n",
       " 18.414099]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa21c635-e3cd-4700-938d-206353c8d8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04ef71b15c3493caafbe4b671dedb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations_q3 = []\n",
    "\n",
    "for record in tqdm(results_gpt4omini_list):\n",
    "    v_llm = embedding_model.encode(record['answer_llm'])\n",
    "    v_orig = embedding_model.encode(record['answer_orig'])\n",
    "    #dp = v_llm.dot(v_orig)\n",
    "    n_v_llm= normalizing(v_llm)\n",
    "    n_v_orig= normalizing(v_orig) \n",
    "    dp2 = n_v_llm.dot(n_v_orig)\n",
    "    evaluations_q3.append(dp2) #dotproduct = compute_similarity(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08623d3-92d1-4967-8b2b-72baaf20ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(evaluations_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eb6a16e-41ba-4947-a12e-192f68bd7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations_q3= pd.DataFrame()\n",
    "df_evaluations_q3['normalized_score'] = evaluations_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85175208-8612-4e99-ab18-52df5657c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.728392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.157755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.125357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.651273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.763761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.836235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.958796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       normalized_score\n",
       "count        300.000000\n",
       "mean           0.728392\n",
       "std            0.157755\n",
       "min            0.125357\n",
       "25%            0.651273\n",
       "50%            0.763761\n",
       "75%            0.836235\n",
       "max            0.958796"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluations_q3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e6581-7a91-46a9-88de-44e80bb4c293",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q4. Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca11f19e-ea21-4cac-9b77-b84246a2e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /home/hhammad/miniconda3/envs/llm_zoomcamp/lib/python3.10/site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f0ab61c-320d-4c0e-a609-d6099c472c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_llm': \"Yes, all sessions are recorded, so if you miss one, you won't miss anything. You can catch up on the content later. Additionally, you can submit your questions in advance for office hours, and those sessions are also recorded.\",\n",
       " 'answer_orig': 'Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.',\n",
       " 'document': '5170565b',\n",
       " 'question': 'Are sessions recorded if I miss one?',\n",
       " 'course': 'machine-learning-zoomcamp'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = results_gpt4omini_list[10]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c474d45-53cd-4492-899d-b3d11e7b7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0df686d4-9516-46fd-ac78-d96987719c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e49a6-ec10-4577-87e1-9687d43a977b",
   "metadata": {},
   "source": [
    "#### What's the F score for rouge-1?\n",
    "\n",
    " 'f': 0.45454544954545456},"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd856c-bdb9-4f79-bd27-9af888111a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q5. Average rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645d89b-96d5-48a0-8832-c90e8ce4f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_F_score = scores['rouge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22aa25e3-9b78-4a30-8619-f1b84d64d40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454544954545456"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4542df4f-3341-47d6-a466-10c098ac4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fscores = [scores[k]['f'] for k in scores.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a284b7c1-fd5c-4dfb-a437-091fcb6bd749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45454544954545456, 0.21621621121621637, 0.393939388939394]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "242aa8c2-499d-43ae-84ce-0bf9d2c5f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f_score = sum(fscores)/len(fscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5b77217-b5ee-47bf-a61d-294473848878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35490034990035496"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018bac3-f5a0-4ab1-a45c-7049c4a0355b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Q6. Average rouge score for all the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2721102-b115-457e-a111-b9bf3ea96b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84501235851b44ca80c876bd822ed190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge_scores = []\n",
    "\n",
    "\n",
    "for record in tqdm(results_gpt4omini_list):\n",
    "   #v_llm = embedding_model.encode(record['answer_llm'])\n",
    "    #v_orig = embedding_model.encode(record['answer_orig'])\n",
    "\n",
    "    rouge_score = rouge_scorer.get_scores(record['answer_llm'], record['answer_orig'])[0]\n",
    "    rouge_scores.append(rouge_score)\n",
    "    #dp = v_llm.dot(v_orig)\n",
    "    #n_v_llm= normalizing(v_llm)\n",
    "    #n_v_orig= normalizing(v_orig) \n",
    "    #dp2 = n_v_llm.dot(n_v_orig)\n",
    "    #evaluations_q3.append(dp2) #dotproduct = compute_similarity(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41e067e6-d87b-4cda-9e55-25e04cb87ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,\n",
       " [{'rouge-1': {'r': 0.061224489795918366,\n",
       "    'p': 0.21428571428571427,\n",
       "    'f': 0.09523809178130524},\n",
       "   'rouge-2': {'r': 0.017543859649122806,\n",
       "    'p': 0.07142857142857142,\n",
       "    'f': 0.028169010918468917},\n",
       "   'rouge-l': {'r': 0.061224489795918366,\n",
       "    'p': 0.21428571428571427,\n",
       "    'f': 0.09523809178130524}},\n",
       "  {'rouge-1': {'r': 0.08163265306122448,\n",
       "    'p': 0.26666666666666666,\n",
       "    'f': 0.12499999641113292},\n",
       "   'rouge-2': {'r': 0.03508771929824561,\n",
       "    'p': 0.13333333333333333,\n",
       "    'f': 0.05555555225694465},\n",
       "   'rouge-l': {'r': 0.061224489795918366, 'p': 0.2, 'f': 0.09374999641113295}},\n",
       "  {'rouge-1': {'r': 0.32653061224489793,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.41558441095631643},\n",
       "   'rouge-2': {'r': 0.14035087719298245,\n",
       "    'p': 0.24242424242424243,\n",
       "    'f': 0.17777777313333343},\n",
       "   'rouge-l': {'r': 0.30612244897959184,\n",
       "    'p': 0.5357142857142857,\n",
       "    'f': 0.3896103849822905}},\n",
       "  {'rouge-1': {'r': 0.16326530612244897, 'p': 0.32, 'f': 0.2162162117421476},\n",
       "   'rouge-2': {'r': 0.03508771929824561,\n",
       "    'p': 0.07142857142857142,\n",
       "    'f': 0.047058819111419105},\n",
       "   'rouge-l': {'r': 0.14285714285714285, 'p': 0.28, 'f': 0.18918918471512064}},\n",
       "  {'rouge-1': {'r': 0.2653061224489796,\n",
       "    'p': 0.09701492537313433,\n",
       "    'f': 0.14207649881095297},\n",
       "   'rouge-2': {'r': 0.07017543859649122,\n",
       "    'p': 0.0223463687150838,\n",
       "    'f': 0.03389830142092829},\n",
       "   'rouge-l': {'r': 0.22448979591836735,\n",
       "    'p': 0.08208955223880597,\n",
       "    'f': 0.12021857531368524}},\n",
       "  {'rouge-1': {'r': 0.2894736842105263,\n",
       "    'p': 0.8461538461538461,\n",
       "    'f': 0.43137254522106894},\n",
       "   'rouge-2': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841},\n",
       "   'rouge-l': {'r': 0.2894736842105263,\n",
       "    'p': 0.8461538461538461,\n",
       "    'f': 0.43137254522106894}},\n",
       "  {'rouge-1': {'r': 0.34210526315789475, 'p': 0.52, 'f': 0.41269840791131274},\n",
       "   'rouge-2': {'r': 0.14, 'p': 0.25, 'f': 0.17948717488494426},\n",
       "   'rouge-l': {'r': 0.2894736842105263, 'p': 0.44, 'f': 0.34920634441924925}},\n",
       "  {'rouge-1': {'r': 0.18421052631578946, 'p': 0.875, 'f': 0.30434782321361065},\n",
       "   'rouge-2': {'r': 0.08, 'p': 0.5714285714285714, 'f': 0.1403508750384734},\n",
       "   'rouge-l': {'r': 0.15789473684210525, 'p': 0.75, 'f': 0.26086956234404535}},\n",
       "  {'rouge-1': {'r': 0.39473684210526316, 'p': 0.75, 'f': 0.5172413747919143},\n",
       "   'rouge-2': {'r': 0.24, 'p': 0.5, 'f': 0.3243243199415632},\n",
       "   'rouge-l': {'r': 0.3157894736842105, 'p': 0.6, 'f': 0.4137930989298454}},\n",
       "  {'rouge-1': {'r': 0.2894736842105263,\n",
       "    'p': 0.4230769230769231,\n",
       "    'f': 0.34374999517578125},\n",
       "   'rouge-2': {'r': 0.18, 'p': 0.2647058823529412, 'f': 0.21428570946712028},\n",
       "   'rouge-l': {'r': 0.2894736842105263,\n",
       "    'p': 0.4230769230769231,\n",
       "    'f': 0.34374999517578125}},\n",
       "  {'rouge-1': {'r': 0.45454545454545453,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.45454544954545456},\n",
       "   'rouge-2': {'r': 0.21621621621621623,\n",
       "    'p': 0.21621621621621623,\n",
       "    'f': 0.21621621121621637},\n",
       "   'rouge-l': {'r': 0.3939393939393939,\n",
       "    'p': 0.3939393939393939,\n",
       "    'f': 0.393939388939394}},\n",
       "  {'rouge-1': {'r': 0.6060606060606061,\n",
       "    'p': 0.6060606060606061,\n",
       "    'f': 0.6060606010606061},\n",
       "   'rouge-2': {'r': 0.43243243243243246,\n",
       "    'p': 0.41025641025641024,\n",
       "    'f': 0.42105262658241},\n",
       "   'rouge-l': {'r': 0.5757575757575758,\n",
       "    'p': 0.5757575757575758,\n",
       "    'f': 0.5757575707575758}},\n",
       "  {'rouge-1': {'r': 0.7878787878787878,\n",
       "    'p': 0.7222222222222222,\n",
       "    'f': 0.753623183415249},\n",
       "   'rouge-2': {'r': 0.6486486486486487,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.6075949317288897},\n",
       "   'rouge-l': {'r': 0.7575757575757576,\n",
       "    'p': 0.6944444444444444,\n",
       "    'f': 0.7246376761688721}},\n",
       "  {'rouge-1': {'r': 0.8181818181818182,\n",
       "    'p': 0.6585365853658537,\n",
       "    'f': 0.7297297247881666},\n",
       "   'rouge-2': {'r': 0.6486486486486487,\n",
       "    'p': 0.5106382978723404,\n",
       "    'f': 0.5714285664994332},\n",
       "   'rouge-l': {'r': 0.7878787878787878,\n",
       "    'p': 0.6341463414634146,\n",
       "    'f': 0.7027026977611397}},\n",
       "  {'rouge-1': {'r': 0.7575757575757576, 'p': 0.625, 'f': 0.6849315018952901},\n",
       "   'rouge-2': {'r': 0.5405405405405406,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.4878048730963712},\n",
       "   'rouge-l': {'r': 0.6666666666666666, 'p': 0.55, 'f': 0.6027397210733721}},\n",
       "  {'rouge-1': {'r': 0.4772727272727273,\n",
       "    'p': 0.4772727272727273,\n",
       "    'f': 0.4772727222727273},\n",
       "   'rouge-2': {'r': 0.2692307692307692,\n",
       "    'p': 0.2641509433962264,\n",
       "    'f': 0.2666666616671202},\n",
       "   'rouge-l': {'r': 0.4318181818181818,\n",
       "    'p': 0.4318181818181818,\n",
       "    'f': 0.43181817681818185}},\n",
       "  {'rouge-1': {'r': 0.4090909090909091,\n",
       "    'p': 0.5806451612903226,\n",
       "    'f': 0.47999999515022235},\n",
       "   'rouge-2': {'r': 0.23076923076923078,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.28235293642629766},\n",
       "   'rouge-l': {'r': 0.38636363636363635,\n",
       "    'p': 0.5483870967741935,\n",
       "    'f': 0.45333332848355556}},\n",
       "  {'rouge-1': {'r': 0.3181818181818182, 'p': 0.5, 'f': 0.3888888841358025},\n",
       "   'rouge-2': {'r': 0.09615384615384616,\n",
       "    'p': 0.15625,\n",
       "    'f': 0.11904761433106596},\n",
       "   'rouge-l': {'r': 0.29545454545454547,\n",
       "    'p': 0.4642857142857143,\n",
       "    'f': 0.36111110635802474}},\n",
       "  {'rouge-1': {'r': 0.2727272727272727, 'p': 0.3, 'f': 0.2857142807256236},\n",
       "   'rouge-2': {'r': 0.057692307692307696,\n",
       "    'p': 0.075,\n",
       "    'f': 0.06521738638941436},\n",
       "   'rouge-l': {'r': 0.2727272727272727, 'p': 0.3, 'f': 0.2857142807256236}},\n",
       "  {'rouge-1': {'r': 0.3409090909090909,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.37499999505},\n",
       "   'rouge-2': {'r': 0.19230769230769232,\n",
       "    'p': 0.2631578947368421,\n",
       "    'f': 0.22222221734320996},\n",
       "   'rouge-l': {'r': 0.3409090909090909,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.37499999505}},\n",
       "  {'rouge-1': {'r': 0.1746031746031746,\n",
       "    'p': 0.4230769230769231,\n",
       "    'f': 0.24719100710011369},\n",
       "   'rouge-2': {'r': 0.05714285714285714,\n",
       "    'p': 0.14814814814814814,\n",
       "    'f': 0.08247422278669378},\n",
       "   'rouge-l': {'r': 0.15873015873015872,\n",
       "    'p': 0.38461538461538464,\n",
       "    'f': 0.22471909698775408}},\n",
       "  {'rouge-1': {'r': 0.1746031746031746,\n",
       "    'p': 0.5789473684210527,\n",
       "    'f': 0.2682926793664485},\n",
       "   'rouge-2': {'r': 0.08571428571428572,\n",
       "    'p': 0.3157894736842105,\n",
       "    'f': 0.13483145731599552},\n",
       "   'rouge-l': {'r': 0.1746031746031746,\n",
       "    'p': 0.5789473684210527,\n",
       "    'f': 0.2682926793664485}},\n",
       "  {'rouge-1': {'r': 0.42857142857142855, 'p': 0.675, 'f': 0.5242718399095109},\n",
       "   'rouge-2': {'r': 0.2714285714285714,\n",
       "    'p': 0.41304347826086957,\n",
       "    'f': 0.3275862021105827},\n",
       "   'rouge-l': {'r': 0.3968253968253968, 'p': 0.625, 'f': 0.4854368884532002}},\n",
       "  {'rouge-1': {'r': 0.1111111111111111,\n",
       "    'p': 0.6363636363636364,\n",
       "    'f': 0.18918918665814466},\n",
       "   'rouge-2': {'r': 0.04285714285714286, 'p': 0.3, 'f': 0.07499999781250007},\n",
       "   'rouge-l': {'r': 0.09523809523809523,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.16216215963111763}},\n",
       "  {'rouge-1': {'r': 0.4126984126984127, 'p': 0.65, 'f': 0.5048543641813555},\n",
       "   'rouge-2': {'r': 0.21428571428571427,\n",
       "    'p': 0.36585365853658536,\n",
       "    'f': 0.27027026561155754},\n",
       "   'rouge-l': {'r': 0.3968253968253968, 'p': 0.625, 'f': 0.4854368884532002}},\n",
       "  {'rouge-1': {'r': 0.6206896551724138,\n",
       "    'p': 0.6206896551724138,\n",
       "    'f': 0.6206896501724138},\n",
       "   'rouge-2': {'r': 0.391304347826087, 'p': 0.36, 'f': 0.3749999950086806},\n",
       "   'rouge-l': {'r': 0.603448275862069,\n",
       "    'p': 0.603448275862069,\n",
       "    'f': 0.603448270862069}},\n",
       "  {'rouge-1': {'r': 0.2413793103448276,\n",
       "    'p': 0.5384615384615384,\n",
       "    'f': 0.3333333290589569},\n",
       "   'rouge-2': {'r': 0.13043478260869565, 'p': 0.3, 'f': 0.18181817759412314},\n",
       "   'rouge-l': {'r': 0.2413793103448276,\n",
       "    'p': 0.5384615384615384,\n",
       "    'f': 0.3333333290589569}},\n",
       "  {'rouge-1': {'r': 0.3275862068965517,\n",
       "    'p': 0.5588235294117647,\n",
       "    'f': 0.4130434736011342},\n",
       "   'rouge-2': {'r': 0.17391304347826086, 'p': 0.3, 'f': 0.22018348159245865},\n",
       "   'rouge-l': {'r': 0.3275862068965517,\n",
       "    'p': 0.5588235294117647,\n",
       "    'f': 0.4130434736011342}},\n",
       "  {'rouge-1': {'r': 0.5517241379310345,\n",
       "    'p': 0.7441860465116279,\n",
       "    'f': 0.633663361446917},\n",
       "   'rouge-2': {'r': 0.391304347826087,\n",
       "    'p': 0.4909090909090909,\n",
       "    'f': 0.43548386603147765},\n",
       "   'rouge-l': {'r': 0.5517241379310345,\n",
       "    'p': 0.7441860465116279,\n",
       "    'f': 0.633663361446917}},\n",
       "  {'rouge-1': {'r': 0.27586206896551724,\n",
       "    'p': 0.5161290322580645,\n",
       "    'f': 0.359550557257922},\n",
       "   'rouge-2': {'r': 0.15942028985507245,\n",
       "    'p': 0.3142857142857143,\n",
       "    'f': 0.21153845707285512},\n",
       "   'rouge-l': {'r': 0.25862068965517243,\n",
       "    'p': 0.4838709677419355,\n",
       "    'f': 0.3370786471455625}},\n",
       "  {'rouge-1': {'r': 0.55, 'p': 0.3793103448275862, 'f': 0.4489795870054144},\n",
       "   'rouge-2': {'r': 0.25, 'p': 0.17857142857142858, 'f': 0.20833332847222233},\n",
       "   'rouge-l': {'r': 0.55, 'p': 0.3793103448275862, 'f': 0.4489795870054144}},\n",
       "  {'rouge-1': {'r': 0.6, 'p': 0.46153846153846156, 'f': 0.5217391255198489},\n",
       "   'rouge-2': {'r': 0.3, 'p': 0.2222222222222222, 'f': 0.2553191440470802},\n",
       "   'rouge-l': {'r': 0.6, 'p': 0.46153846153846156, 'f': 0.5217391255198489}},\n",
       "  {'rouge-1': {'r': 0.45, 'p': 0.375, 'f': 0.4090909041322315},\n",
       "   'rouge-2': {'r': 0.15, 'p': 0.13043478260869565, 'f': 0.13953487874526788},\n",
       "   'rouge-l': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596}},\n",
       "  {'rouge-1': {'r': 0.45, 'p': 0.18, 'f': 0.25714285306122453},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.07017543859649122, 'f': 0.10389610005059889},\n",
       "   'rouge-l': {'r': 0.45, 'p': 0.18, 'f': 0.25714285306122453}},\n",
       "  {'rouge-1': {'r': 0.15, 'p': 0.1875, 'f': 0.16666666172839517},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.15, 'p': 0.1875, 'f': 0.16666666172839517}},\n",
       "  {'rouge-1': {'r': 0.13793103448275862,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.19999999601250007},\n",
       "   'rouge-2': {'r': 0.10344827586206896, 'p': 0.3, 'f': 0.15384615003287322},\n",
       "   'rouge-l': {'r': 0.13793103448275862,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.19999999601250007}},\n",
       "  {'rouge-1': {'r': 0.20689655172413793,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.29999999601250005},\n",
       "   'rouge-2': {'r': 0.06896551724137931, 'p': 0.2, 'f': 0.10256409875082198},\n",
       "   'rouge-l': {'r': 0.13793103448275862,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.19999999601250007}},\n",
       "  {'rouge-1': {'r': 0.3103448275862069,\n",
       "    'p': 0.6923076923076923,\n",
       "    'f': 0.42857142429705225},\n",
       "   'rouge-2': {'r': 0.20689655172413793,\n",
       "    'p': 0.46153846153846156,\n",
       "    'f': 0.28571428143990935},\n",
       "   'rouge-l': {'r': 0.3103448275862069,\n",
       "    'p': 0.6923076923076923,\n",
       "    'f': 0.42857142429705225}},\n",
       "  {'rouge-1': {'r': 0.1724137931034483,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.23255813514332083},\n",
       "   'rouge-2': {'r': 0.06896551724137931,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.09523809096371902},\n",
       "   'rouge-l': {'r': 0.1724137931034483,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.23255813514332083}},\n",
       "  {'rouge-1': {'r': 0.7586206896551724,\n",
       "    'p': 0.6666666666666666,\n",
       "    'f': 0.7096774143756504},\n",
       "   'rouge-2': {'r': 0.6206896551724138,\n",
       "    'p': 0.5142857142857142,\n",
       "    'f': 0.5624999950439454},\n",
       "   'rouge-l': {'r': 0.6551724137931034,\n",
       "    'p': 0.5757575757575758,\n",
       "    'f': 0.6129032208272632}},\n",
       "  {'rouge-1': {'r': 0.47058823529411764, 'p': 0.5, 'f': 0.4848484798530762},\n",
       "   'rouge-2': {'r': 0.3157894736842105,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.3243243193279767},\n",
       "   'rouge-l': {'r': 0.47058823529411764, 'p': 0.5, 'f': 0.4848484798530762}},\n",
       "  {'rouge-1': {'r': 0.35294117647058826,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.3934426180166622},\n",
       "   'rouge-2': {'r': 0.18421052631578946,\n",
       "    'p': 0.23333333333333334,\n",
       "    'f': 0.20588234801038074},\n",
       "   'rouge-l': {'r': 0.35294117647058826,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.3934426180166622}},\n",
       "  {'rouge-1': {'r': 0.47058823529411764,\n",
       "    'p': 0.5925925925925926,\n",
       "    'f': 0.5245901590002687},\n",
       "   'rouge-2': {'r': 0.34210526315789475,\n",
       "    'p': 0.41935483870967744,\n",
       "    'f': 0.3768115892543584},\n",
       "   'rouge-l': {'r': 0.47058823529411764,\n",
       "    'p': 0.5925925925925926,\n",
       "    'f': 0.5245901590002687}},\n",
       "  {'rouge-1': {'r': 0.35294117647058826,\n",
       "    'p': 0.3870967741935484,\n",
       "    'f': 0.3692307642414202},\n",
       "   'rouge-2': {'r': 0.18421052631578946,\n",
       "    'p': 0.19444444444444445,\n",
       "    'f': 0.18918918419284161},\n",
       "   'rouge-l': {'r': 0.3235294117647059,\n",
       "    'p': 0.3548387096774194,\n",
       "    'f': 0.3384615334721894}},\n",
       "  {'rouge-1': {'r': 0.14705882352941177,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.22222221852839513},\n",
       "   'rouge-2': {'r': 0.05263157894736842,\n",
       "    'p': 0.18181818181818182,\n",
       "    'f': 0.08163264957934209},\n",
       "   'rouge-l': {'r': 0.14705882352941177,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.22222221852839513}},\n",
       "  {'rouge-1': {'r': 0.2857142857142857,\n",
       "    'p': 0.06896551724137931,\n",
       "    'f': 0.11111110797839514},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.2857142857142857,\n",
       "    'p': 0.06896551724137931,\n",
       "    'f': 0.11111110797839514}},\n",
       "  {'rouge-1': {'r': 0.42857142857142855,\n",
       "    'p': 0.0967741935483871,\n",
       "    'f': 0.15789473383656513},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.42857142857142855,\n",
       "    'p': 0.0967741935483871,\n",
       "    'f': 0.15789473383656513}},\n",
       "  {'rouge-1': {'r': 0.2857142857142857,\n",
       "    'p': 0.06060606060606061,\n",
       "    'f': 0.09999999711250009},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.2857142857142857,\n",
       "    'p': 0.06060606060606061,\n",
       "    'f': 0.09999999711250009}},\n",
       "  {'rouge-1': {'r': 0.14285714285714285,\n",
       "    'p': 0.03333333333333333,\n",
       "    'f': 0.054054050986121434},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.14285714285714285,\n",
       "    'p': 0.03333333333333333,\n",
       "    'f': 0.054054050986121434}},\n",
       "  {'rouge-1': {'r': 0.2857142857142857,\n",
       "    'p': 0.05555555555555555,\n",
       "    'f': 0.09302325308815583},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.2857142857142857,\n",
       "    'p': 0.05555555555555555,\n",
       "    'f': 0.09302325308815583}},\n",
       "  {'rouge-1': {'r': 0.4520547945205479,\n",
       "    'p': 0.44594594594594594,\n",
       "    'f': 0.44897958683696615},\n",
       "   'rouge-2': {'r': 0.21212121212121213,\n",
       "    'p': 0.21649484536082475,\n",
       "    'f': 0.214285709286235},\n",
       "   'rouge-l': {'r': 0.4246575342465753,\n",
       "    'p': 0.4189189189189189,\n",
       "    'f': 0.4217687024832246}},\n",
       "  {'rouge-1': {'r': 0.4794520547945205, 'p': 0.5, 'f': 0.48951048451269014},\n",
       "   'rouge-2': {'r': 0.21212121212121213,\n",
       "    'p': 0.22105263157894736,\n",
       "    'f': 0.2164948403629505},\n",
       "   'rouge-l': {'r': 0.4246575342465753,\n",
       "    'p': 0.44285714285714284,\n",
       "    'f': 0.4335664285686342}},\n",
       "  {'rouge-1': {'r': 0.2328767123287671,\n",
       "    'p': 0.2463768115942029,\n",
       "    'f': 0.23943661472227742},\n",
       "   'rouge-2': {'r': 0.08080808080808081,\n",
       "    'p': 0.09302325581395349,\n",
       "    'f': 0.08648648151117633},\n",
       "   'rouge-l': {'r': 0.2191780821917808,\n",
       "    'p': 0.2318840579710145,\n",
       "    'f': 0.2253521076800239}},\n",
       "  {'rouge-1': {'r': 0.3150684931506849,\n",
       "    'p': 0.696969696969697,\n",
       "    'f': 0.4339622598629406},\n",
       "   'rouge-2': {'r': 0.18181818181818182,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.2553191447532821},\n",
       "   'rouge-l': {'r': 0.2876712328767123,\n",
       "    'p': 0.6363636363636364,\n",
       "    'f': 0.3962264108063368}},\n",
       "  {'rouge-1': {'r': 0.3561643835616438,\n",
       "    'p': 0.49056603773584906,\n",
       "    'f': 0.41269840782438905},\n",
       "   'rouge-2': {'r': 0.12121212121212122,\n",
       "    'p': 0.17142857142857143,\n",
       "    'f': 0.14201182946675553},\n",
       "   'rouge-l': {'r': 0.3013698630136986,\n",
       "    'p': 0.41509433962264153,\n",
       "    'f': 0.3492063443323256}},\n",
       "  {'rouge-1': {'r': 0.6097560975609756,\n",
       "    'p': 0.7142857142857143,\n",
       "    'f': 0.6578947318732687},\n",
       "   'rouge-2': {'r': 0.46938775510204084,\n",
       "    'p': 0.5476190476190477,\n",
       "    'f': 0.5054945005240913},\n",
       "   'rouge-l': {'r': 0.5609756097560976,\n",
       "    'p': 0.6571428571428571,\n",
       "    'f': 0.6052631529259005}},\n",
       "  {'rouge-1': {'r': 0.3170731707317073, 'p': 0.8125, 'f': 0.45614034683902743},\n",
       "   'rouge-2': {'r': 0.16326530612244897,\n",
       "    'p': 0.5333333333333333,\n",
       "    'f': 0.24999999641113285},\n",
       "   'rouge-l': {'r': 0.3170731707317073,\n",
       "    'p': 0.8125,\n",
       "    'f': 0.45614034683902743}},\n",
       "  {'rouge-1': {'r': 0.5365853658536586,\n",
       "    'p': 0.5641025641025641,\n",
       "    'f': 0.5499999950031251},\n",
       "   'rouge-2': {'r': 0.32653061224489793,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.3298969022170263},\n",
       "   'rouge-l': {'r': 0.4878048780487805,\n",
       "    'p': 0.5128205128205128,\n",
       "    'f': 0.4999999950031251}},\n",
       "  {'rouge-1': {'r': 0.2926829268292683,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.3529411716825259},\n",
       "   'rouge-2': {'r': 0.10204081632653061,\n",
       "    'p': 0.16129032258064516,\n",
       "    'f': 0.1249999952531252},\n",
       "   'rouge-l': {'r': 0.24390243902439024,\n",
       "    'p': 0.37037037037037035,\n",
       "    'f': 0.29411764227076126}},\n",
       "  {'rouge-1': {'r': 0.2682926829268293,\n",
       "    'p': 0.39285714285714285,\n",
       "    'f': 0.31884057488762874},\n",
       "   'rouge-2': {'r': 0.08163265306122448, 'p': 0.125, 'f': 0.09876542731900649},\n",
       "   'rouge-l': {'r': 0.17073170731707318, 'p': 0.25, 'f': 0.2028985459021215}},\n",
       "  {'rouge-1': {'r': 0.1, 'p': 0.03225806451612903, 'f': 0.04878048411659754},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.1, 'p': 0.03225806451612903, 'f': 0.04878048411659754}},\n",
       "  {'rouge-1': {'r': 0.6, 'p': 0.12244897959183673, 'f': 0.2033898276931916},\n",
       "   'rouge-2': {'r': 0.4444444444444444,\n",
       "    'p': 0.06779661016949153,\n",
       "    'f': 0.11764705652681663},\n",
       "   'rouge-l': {'r': 0.6, 'p': 0.12244897959183673, 'f': 0.2033898276931916}},\n",
       "  {'rouge-1': {'r': 0.6, 'p': 0.12, 'f': 0.19999999722222223},\n",
       "   'rouge-2': {'r': 0.3333333333333333,\n",
       "    'p': 0.05084745762711865,\n",
       "    'f': 0.08823529182093431},\n",
       "   'rouge-l': {'r': 0.6, 'p': 0.12, 'f': 0.19999999722222223}},\n",
       "  {'rouge-1': {'r': 0.2, 'p': 0.07142857142857142, 'f': 0.10526315401662063},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.2, 'p': 0.07142857142857142, 'f': 0.10526315401662063}},\n",
       "  {'rouge-1': {'r': 0.1, 'p': 0.009345794392523364, 'f': 0.017094015530718237},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.1,\n",
       "    'p': 0.009345794392523364,\n",
       "    'f': 0.017094015530718237}},\n",
       "  {'rouge-1': {'r': 0.6029411764705882,\n",
       "    'p': 0.6612903225806451,\n",
       "    'f': 0.6307692257798817},\n",
       "   'rouge-2': {'r': 0.5161290322580645,\n",
       "    'p': 0.6153846153846154,\n",
       "    'f': 0.5614035038104033},\n",
       "   'rouge-l': {'r': 0.6029411764705882,\n",
       "    'p': 0.6612903225806451,\n",
       "    'f': 0.6307692257798817}},\n",
       "  {'rouge-1': {'r': 0.5588235294117647,\n",
       "    'p': 0.5757575757575758,\n",
       "    'f': 0.5671641741055915},\n",
       "   'rouge-2': {'r': 0.43010752688172044,\n",
       "    'p': 0.47058823529411764,\n",
       "    'f': 0.4494381972572908},\n",
       "   'rouge-l': {'r': 0.5588235294117647,\n",
       "    'p': 0.5757575757575758,\n",
       "    'f': 0.5671641741055915}},\n",
       "  {'rouge-1': {'r': 0.16176470588235295, 'p': 0.5, 'f': 0.24444444075061733},\n",
       "   'rouge-2': {'r': 0.07526881720430108,\n",
       "    'p': 0.2916666666666667,\n",
       "    'f': 0.11965811639710727},\n",
       "   'rouge-l': {'r': 0.14705882352941177,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.22222221852839513}},\n",
       "  {'rouge-1': {'r': 0.23529411764705882,\n",
       "    'p': 0.7619047619047619,\n",
       "    'f': 0.35955055819214743},\n",
       "   'rouge-2': {'r': 0.12903225806451613,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.19999999651250003},\n",
       "   'rouge-l': {'r': 0.20588235294117646,\n",
       "    'p': 0.6666666666666666,\n",
       "    'f': 0.31460673796742833}},\n",
       "  {'rouge-1': {'r': 0.6029411764705882,\n",
       "    'p': 0.6612903225806451,\n",
       "    'f': 0.6307692257798817},\n",
       "   'rouge-2': {'r': 0.4946236559139785,\n",
       "    'p': 0.6052631578947368,\n",
       "    'f': 0.5443786932754456},\n",
       "   'rouge-l': {'r': 0.6029411764705882,\n",
       "    'p': 0.6612903225806451,\n",
       "    'f': 0.6307692257798817}},\n",
       "  {'rouge-1': {'r': 0.5192307692307693,\n",
       "    'p': 0.7941176470588235,\n",
       "    'f': 0.6279069719632235},\n",
       "   'rouge-2': {'r': 0.4142857142857143,\n",
       "    'p': 0.6744186046511628,\n",
       "    'f': 0.5132743315686429},\n",
       "   'rouge-l': {'r': 0.4807692307692308,\n",
       "    'p': 0.7352941176470589,\n",
       "    'f': 0.5813953440562467}},\n",
       "  {'rouge-1': {'r': 0.46153846153846156,\n",
       "    'p': 0.7272727272727273,\n",
       "    'f': 0.5647058776027682},\n",
       "   'rouge-2': {'r': 0.32857142857142857, 'p': 0.575, 'f': 0.4181818135537191},\n",
       "   'rouge-l': {'r': 0.46153846153846156,\n",
       "    'p': 0.7272727272727273,\n",
       "    'f': 0.5647058776027682}},\n",
       "  {'rouge-1': {'r': 0.36538461538461536,\n",
       "    'p': 0.9047619047619048,\n",
       "    'f': 0.5205479411071496},\n",
       "   'rouge-2': {'r': 0.24285714285714285,\n",
       "    'p': 0.7727272727272727,\n",
       "    'f': 0.369565213752363},\n",
       "   'rouge-l': {'r': 0.36538461538461536,\n",
       "    'p': 0.9047619047619048,\n",
       "    'f': 0.5205479411071496}},\n",
       "  {'rouge-1': {'r': 0.28846153846153844,\n",
       "    'p': 0.6818181818181818,\n",
       "    'f': 0.4054054012271731},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.43478260869565216,\n",
       "    'f': 0.21505375971788648},\n",
       "   'rouge-l': {'r': 0.28846153846153844,\n",
       "    'p': 0.6818181818181818,\n",
       "    'f': 0.4054054012271731}},\n",
       "  {'rouge-1': {'r': 0.5576923076923077,\n",
       "    'p': 0.7073170731707317,\n",
       "    'f': 0.6236559090484449},\n",
       "   'rouge-2': {'r': 0.44285714285714284, 'p': 0.62, 'f': 0.5166666618055558},\n",
       "   'rouge-l': {'r': 0.5384615384615384,\n",
       "    'p': 0.6829268292682927,\n",
       "    'f': 0.6021505327043589}},\n",
       "  {'rouge-1': {'r': 0.36585365853658536,\n",
       "    'p': 0.8333333333333334,\n",
       "    'f': 0.5084745720310256},\n",
       "   'rouge-2': {'r': 0.19148936170212766, 'p': 0.5, 'f': 0.2769230729183432},\n",
       "   'rouge-l': {'r': 0.34146341463414637,\n",
       "    'p': 0.7777777777777778,\n",
       "    'f': 0.47457626694627986}},\n",
       "  {'rouge-1': {'r': 0.6585365853658537,\n",
       "    'p': 0.7297297297297297,\n",
       "    'f': 0.6923076873208417},\n",
       "   'rouge-2': {'r': 0.425531914893617,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.4395604345658737},\n",
       "   'rouge-l': {'r': 0.6341463414634146,\n",
       "    'p': 0.7027027027027027,\n",
       "    'f': 0.666666661679816}},\n",
       "  {'rouge-1': {'r': 0.3170731707317073,\n",
       "    'p': 0.5909090909090909,\n",
       "    'f': 0.41269840815318726},\n",
       "   'rouge-2': {'r': 0.10638297872340426,\n",
       "    'p': 0.22727272727272727,\n",
       "    'f': 0.14492753188825888},\n",
       "   'rouge-l': {'r': 0.2682926829268293, 'p': 0.5, 'f': 0.34920634466112377}},\n",
       "  {'rouge-1': {'r': 0.5853658536585366,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.5783132480127741},\n",
       "   'rouge-2': {'r': 0.2978723404255319,\n",
       "    'p': 0.2916666666666667,\n",
       "    'f': 0.2947368371058172},\n",
       "   'rouge-l': {'r': 0.5609756097560976,\n",
       "    'p': 0.5476190476190477,\n",
       "    'f': 0.5542168624706055}},\n",
       "  {'rouge-1': {'r': 0.21951219512195122,\n",
       "    'p': 0.5294117647058824,\n",
       "    'f': 0.3103448234423306},\n",
       "   'rouge-2': {'r': 0.0851063829787234,\n",
       "    'p': 0.2222222222222222,\n",
       "    'f': 0.12307691907218948},\n",
       "   'rouge-l': {'r': 0.1951219512195122,\n",
       "    'p': 0.47058823529411764,\n",
       "    'f': 0.27586206482164094}},\n",
       "  {'rouge-1': {'r': 0.7, 'p': 0.5185185185185185, 'f': 0.5957446759619739},\n",
       "   'rouge-2': {'r': 0.5, 'p': 0.36666666666666664, 'f': 0.42307691819526627},\n",
       "   'rouge-l': {'r': 0.7, 'p': 0.5185185185185185, 'f': 0.5957446759619739}},\n",
       "  {'rouge-1': {'r': 0.7, 'p': 0.6666666666666666, 'f': 0.682926824271267},\n",
       "   'rouge-2': {'r': 0.5909090909090909, 'p': 0.52, 'f': 0.5531914843820736},\n",
       "   'rouge-l': {'r': 0.7, 'p': 0.6666666666666666, 'f': 0.682926824271267}},\n",
       "  {'rouge-1': {'r': 0.65, 'p': 0.4482758620689655, 'f': 0.530612240066639},\n",
       "   'rouge-2': {'r': 0.5909090909090909,\n",
       "    'p': 0.35135135135135137,\n",
       "    'f': 0.4406779614248779},\n",
       "   'rouge-l': {'r': 0.65, 'p': 0.4482758620689655, 'f': 0.530612240066639}},\n",
       "  {'rouge-1': {'r': 0.85, 'p': 0.85, 'f': 0.849999995},\n",
       "   'rouge-2': {'r': 0.7727272727272727,\n",
       "    'p': 0.7083333333333334,\n",
       "    'f': 0.7391304297920607},\n",
       "   'rouge-l': {'r': 0.85, 'p': 0.85, 'f': 0.849999995}},\n",
       "  {'rouge-1': {'r': 0.55, 'p': 0.3548387096774194, 'f': 0.4313725442522107},\n",
       "   'rouge-2': {'r': 0.36363636363636365,\n",
       "    'p': 0.2222222222222222,\n",
       "    'f': 0.2758620642568371},\n",
       "   'rouge-l': {'r': 0.5, 'p': 0.3225806451612903, 'f': 0.39215685797770095}},\n",
       "  {'rouge-1': {'r': 0.4426229508196721,\n",
       "    'p': 0.7105263157894737,\n",
       "    'f': 0.5454545407244159},\n",
       "   'rouge-2': {'r': 0.3472222222222222,\n",
       "    'p': 0.5681818181818182,\n",
       "    'f': 0.43103447804994055},\n",
       "   'rouge-l': {'r': 0.4426229508196721,\n",
       "    'p': 0.7105263157894737,\n",
       "    'f': 0.5454545407244159}},\n",
       "  {'rouge-1': {'r': 0.5737704918032787,\n",
       "    'p': 0.5932203389830508,\n",
       "    'f': 0.5833333283347223},\n",
       "   'rouge-2': {'r': 0.4027777777777778,\n",
       "    'p': 0.4027777777777778,\n",
       "    'f': 0.4027777727777778},\n",
       "   'rouge-l': {'r': 0.5737704918032787,\n",
       "    'p': 0.5932203389830508,\n",
       "    'f': 0.5833333283347223}},\n",
       "  {'rouge-1': {'r': 0.5901639344262295,\n",
       "    'p': 0.6206896551724138,\n",
       "    'f': 0.6050420118099005},\n",
       "   'rouge-2': {'r': 0.3472222222222222, 'p': 0.3125, 'f': 0.3289473634349031},\n",
       "   'rouge-l': {'r': 0.5573770491803278,\n",
       "    'p': 0.5862068965517241,\n",
       "    'f': 0.5714285664317493}},\n",
       "  {'rouge-1': {'r': 0.22950819672131148,\n",
       "    'p': 0.6363636363636364,\n",
       "    'f': 0.3373493936942953},\n",
       "   'rouge-2': {'r': 0.08333333333333333,\n",
       "    'p': 0.2608695652173913,\n",
       "    'f': 0.12631578580387823},\n",
       "   'rouge-l': {'r': 0.19672131147540983,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.28915662260995795}},\n",
       "  {'rouge-1': {'r': 0.11475409836065574, 'p': 1.0, 'f': 0.2058823510942907},\n",
       "   'rouge-2': {'r': 0.08333333333333333, 'p': 1.0, 'f': 0.15384615242603553},\n",
       "   'rouge-l': {'r': 0.11475409836065574, 'p': 1.0, 'f': 0.2058823510942907}},\n",
       "  {'rouge-1': {'r': 0.35714285714285715,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.3571428521428572},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.1875, 'f': 0.19354838210197725},\n",
       "   'rouge-l': {'r': 0.35714285714285715,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.3571428521428572}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.4117647058823529, 'f': 0.4516128982726327},\n",
       "   'rouge-2': {'r': 0.4, 'p': 0.3333333333333333, 'f': 0.36363635867768596},\n",
       "   'rouge-l': {'r': 0.5, 'p': 0.4117647058823529, 'f': 0.4516128982726327}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.5, 'f': 0.4999999950000001},\n",
       "   'rouge-2': {'r': 0.3333333333333333,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.3333333283333334},\n",
       "   'rouge-l': {'r': 0.42857142857142855,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.4285714235714286}},\n",
       "  {'rouge-1': {'r': 0.42857142857142855, 'p': 0.5, 'f': 0.4615384565680473},\n",
       "   'rouge-2': {'r': 0.3333333333333333,\n",
       "    'p': 0.38461538461538464,\n",
       "    'f': 0.35714285216836733},\n",
       "   'rouge-l': {'r': 0.42857142857142855, 'p': 0.5, 'f': 0.4615384565680473}},\n",
       "  {'rouge-1': {'r': 0.6428571428571429, 'p': 0.5, 'f': 0.5624999950781251},\n",
       "   'rouge-2': {'r': 0.4, 'p': 0.3157894736842105, 'f': 0.35294117153979243},\n",
       "   'rouge-l': {'r': 0.5714285714285714,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.4999999950781251}},\n",
       "  {'rouge-1': {'r': 0.6206896551724138,\n",
       "    'p': 0.5142857142857142,\n",
       "    'f': 0.5624999950439454},\n",
       "   'rouge-2': {'r': 0.36363636363636365,\n",
       "    'p': 0.26666666666666666,\n",
       "    'f': 0.3076923028106509},\n",
       "   'rouge-l': {'r': 0.5862068965517241,\n",
       "    'p': 0.4857142857142857,\n",
       "    'f': 0.5312499950439454}},\n",
       "  {'rouge-1': {'r': 0.6206896551724138, 'p': 0.6, 'f': 0.6101694865268602},\n",
       "   'rouge-2': {'r': 0.5151515151515151,\n",
       "    'p': 0.4146341463414634,\n",
       "    'f': 0.4594594545178963},\n",
       "   'rouge-l': {'r': 0.5862068965517241,\n",
       "    'p': 0.5666666666666667,\n",
       "    'f': 0.5762711814421144}},\n",
       "  {'rouge-1': {'r': 0.20689655172413793, 'p': 0.5, 'f': 0.2926829226888757},\n",
       "   'rouge-2': {'r': 0.06060606060606061,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.0869565176843102},\n",
       "   'rouge-l': {'r': 0.20689655172413793, 'p': 0.5, 'f': 0.2926829226888757}},\n",
       "  {'rouge-1': {'r': 0.3793103448275862,\n",
       "    'p': 0.5789473684210527,\n",
       "    'f': 0.45833332855034725},\n",
       "   'rouge-2': {'r': 0.09090909090909091,\n",
       "    'p': 0.14285714285714285,\n",
       "    'f': 0.11111110635802489},\n",
       "   'rouge-l': {'r': 0.2413793103448276,\n",
       "    'p': 0.3684210526315789,\n",
       "    'f': 0.2916666618836806}},\n",
       "  {'rouge-1': {'r': 0.5862068965517241,\n",
       "    'p': 0.7083333333333334,\n",
       "    'f': 0.6415094290067641},\n",
       "   'rouge-2': {'r': 0.5454545454545454, 'p': 0.5625, 'f': 0.5538461488473373},\n",
       "   'rouge-l': {'r': 0.5862068965517241,\n",
       "    'p': 0.7083333333333334,\n",
       "    'f': 0.6415094290067641}},\n",
       "  {'rouge-1': {'r': 0.8541666666666666,\n",
       "    'p': 0.7592592592592593,\n",
       "    'f': 0.803921563644752},\n",
       "   'rouge-2': {'r': 0.6229508196721312,\n",
       "    'p': 0.5428571428571428,\n",
       "    'f': 0.5801526667793252},\n",
       "   'rouge-l': {'r': 0.8125, 'p': 0.7222222222222222, 'f': 0.7647058773702423}},\n",
       "  {'rouge-1': {'r': 0.25, 'p': 0.6, 'f': 0.3529411723183391},\n",
       "   'rouge-2': {'r': 0.09836065573770492,\n",
       "    'p': 0.2608695652173913,\n",
       "    'f': 0.1428571388803856},\n",
       "   'rouge-l': {'r': 0.22916666666666666, 'p': 0.55, 'f': 0.32352940761245674}},\n",
       "  {'rouge-1': {'r': 0.22916666666666666,\n",
       "    'p': 0.4782608695652174,\n",
       "    'f': 0.3098591505494942},\n",
       "   'rouge-2': {'r': 0.14754098360655737,\n",
       "    'p': 0.32142857142857145,\n",
       "    'f': 0.20224718669864927},\n",
       "   'rouge-l': {'r': 0.20833333333333334,\n",
       "    'p': 0.43478260869565216,\n",
       "    'f': 0.28169013646498714}},\n",
       "  {'rouge-1': {'r': 0.4583333333333333,\n",
       "    'p': 0.5945945945945946,\n",
       "    'f': 0.5176470539072665},\n",
       "   'rouge-2': {'r': 0.22950819672131148,\n",
       "    'p': 0.34146341463414637,\n",
       "    'f': 0.27450979911380247},\n",
       "   'rouge-l': {'r': 0.375, 'p': 0.4864864864864865, 'f': 0.423529406848443}},\n",
       "  {'rouge-1': {'r': 0.125, 'p': 0.5454545454545454, 'f': 0.20338982747486356},\n",
       "   'rouge-2': {'r': 0.03278688524590164,\n",
       "    'p': 0.18181818181818182,\n",
       "    'f': 0.05555555296682112},\n",
       "   'rouge-l': {'r': 0.125, 'p': 0.5454545454545454, 'f': 0.20338982747486356}},\n",
       "  {'rouge-1': {'r': 0.16842105263157894,\n",
       "    'p': 0.9411764705882353,\n",
       "    'f': 0.2857142831393495},\n",
       "   'rouge-2': {'r': 0.1076923076923077,\n",
       "    'p': 0.8235294117647058,\n",
       "    'f': 0.19047618843074646},\n",
       "   'rouge-l': {'r': 0.16842105263157894,\n",
       "    'p': 0.9411764705882353,\n",
       "    'f': 0.2857142831393495}},\n",
       "  {'rouge-1': {'r': 0.42105263157894735,\n",
       "    'p': 0.5555555555555556,\n",
       "    'f': 0.47904191126250495},\n",
       "   'rouge-2': {'r': 0.15384615384615385,\n",
       "    'p': 0.21505376344086022,\n",
       "    'f': 0.1793721924470632},\n",
       "   'rouge-l': {'r': 0.3368421052631579,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.38323352802897204}},\n",
       "  {'rouge-1': {'r': 0.21052631578947367,\n",
       "    'p': 0.6896551724137931,\n",
       "    'f': 0.3225806415777836},\n",
       "   'rouge-2': {'r': 0.046153846153846156,\n",
       "    'p': 0.1875,\n",
       "    'f': 0.07407407090382578},\n",
       "   'rouge-l': {'r': 0.18947368421052632,\n",
       "    'p': 0.6206896551724138,\n",
       "    'f': 0.29032257706165454}},\n",
       "  {'rouge-1': {'r': 0.23157894736842105,\n",
       "    'p': 0.8148148148148148,\n",
       "    'f': 0.36065573425826397},\n",
       "   'rouge-2': {'r': 0.07692307692307693,\n",
       "    'p': 0.2702702702702703,\n",
       "    'f': 0.11976047559252762},\n",
       "   'rouge-l': {'r': 0.22105263157894736,\n",
       "    'p': 0.7777777777777778,\n",
       "    'f': 0.34426229163531313}},\n",
       "  {'rouge-1': {'r': 0.22105263157894736,\n",
       "    'p': 0.6774193548387096,\n",
       "    'f': 0.33333332962333084},\n",
       "   'rouge-2': {'r': 0.038461538461538464,\n",
       "    'p': 0.125,\n",
       "    'f': 0.058823525813149015},\n",
       "   'rouge-l': {'r': 0.18947368421052632,\n",
       "    'p': 0.5806451612903226,\n",
       "    'f': 0.2857142820042832}},\n",
       "  {'rouge-1': {'r': 0.34146341463414637,\n",
       "    'p': 0.2916666666666667,\n",
       "    'f': 0.3146067366039642},\n",
       "   'rouge-2': {'r': 0.14814814814814814,\n",
       "    'p': 0.12307692307692308,\n",
       "    'f': 0.1344537765553282},\n",
       "   'rouge-l': {'r': 0.3170731707317073,\n",
       "    'p': 0.2708333333333333,\n",
       "    'f': 0.2921348264916046}},\n",
       "  {'rouge-1': {'r': 0.6829268292682927,\n",
       "    'p': 0.7368421052631579,\n",
       "    'f': 0.7088607545008814},\n",
       "   'rouge-2': {'r': 0.5185185185185185,\n",
       "    'p': 0.509090909090909,\n",
       "    'f': 0.5137614628903292},\n",
       "   'rouge-l': {'r': 0.6341463414634146,\n",
       "    'p': 0.6842105263157895,\n",
       "    'f': 0.6582278431084763}},\n",
       "  {'rouge-1': {'r': 0.7317073170731707,\n",
       "    'p': 0.7692307692307693,\n",
       "    'f': 0.749999995003125},\n",
       "   'rouge-2': {'r': 0.7037037037037037,\n",
       "    'p': 0.7037037037037037,\n",
       "    'f': 0.7037036987037039},\n",
       "   'rouge-l': {'r': 0.7073170731707317,\n",
       "    'p': 0.7435897435897436,\n",
       "    'f': 0.7249999950031251}},\n",
       "  {'rouge-1': {'r': 0.7073170731707317, 'p': 0.725, 'f': 0.7160493777168117},\n",
       "   'rouge-2': {'r': 0.5, 'p': 0.4909090909090909, 'f': 0.4954128390371181},\n",
       "   'rouge-l': {'r': 0.6829268292682927, 'p': 0.7, 'f': 0.6913580196921202}},\n",
       "  {'rouge-1': {'r': 0.6585365853658537,\n",
       "    'p': 0.8181818181818182,\n",
       "    'f': 0.7297297247881666},\n",
       "   'rouge-2': {'r': 0.5185185185185185,\n",
       "    'p': 0.5490196078431373,\n",
       "    'f': 0.5333333283374151},\n",
       "   'rouge-l': {'r': 0.6585365853658537,\n",
       "    'p': 0.8181818181818182,\n",
       "    'f': 0.7297297247881666}},\n",
       "  {'rouge-1': {'r': 0.12162162162162163,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.18947368076675905},\n",
       "   'rouge-2': {'r': 0.023809523809523808,\n",
       "    'p': 0.08695652173913043,\n",
       "    'f': 0.03738317419512652},\n",
       "   'rouge-l': {'r': 0.10810810810810811,\n",
       "    'p': 0.38095238095238093,\n",
       "    'f': 0.1684210491878117}},\n",
       "  {'rouge-1': {'r': 0.3918918918918919,\n",
       "    'p': 0.5686274509803921,\n",
       "    'f': 0.46399999516928003},\n",
       "   'rouge-2': {'r': 0.23809523809523808,\n",
       "    'p': 0.3448275862068966,\n",
       "    'f': 0.28169013601269594},\n",
       "   'rouge-l': {'r': 0.35135135135135137,\n",
       "    'p': 0.5098039215686274,\n",
       "    'f': 0.41599999516928005}},\n",
       "  {'rouge-1': {'r': 0.13513513513513514,\n",
       "    'p': 0.3225806451612903,\n",
       "    'f': 0.19047618631473934},\n",
       "   'rouge-2': {'r': 0.03571428571428571,\n",
       "    'p': 0.08333333333333333,\n",
       "    'f': 0.049999995800000356},\n",
       "   'rouge-l': {'r': 0.13513513513513514,\n",
       "    'p': 0.3225806451612903,\n",
       "    'f': 0.19047618631473934}},\n",
       "  {'rouge-1': {'r': 0.24324324324324326,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.3364485938649664},\n",
       "   'rouge-2': {'r': 0.08333333333333333,\n",
       "    'p': 0.18421052631578946,\n",
       "    'f': 0.11475409407148632},\n",
       "   'rouge-l': {'r': 0.21621621621621623,\n",
       "    'p': 0.48484848484848486,\n",
       "    'f': 0.299065416294873}},\n",
       "  {'rouge-1': {'r': 0.4594594594594595,\n",
       "    'p': 0.38202247191011235,\n",
       "    'f': 0.4171779091527721},\n",
       "   'rouge-2': {'r': 0.30952380952380953,\n",
       "    'p': 0.22608695652173913,\n",
       "    'f': 0.26130652778465197},\n",
       "   'rouge-l': {'r': 0.44594594594594594,\n",
       "    'p': 0.3707865168539326,\n",
       "    'f': 0.40490797050246535}},\n",
       "  {'rouge-1': {'r': 0.2088607594936709,\n",
       "    'p': 0.6111111111111112,\n",
       "    'f': 0.3113207509202564},\n",
       "   'rouge-2': {'r': 0.08571428571428572,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.13636363310950422},\n",
       "   'rouge-l': {'r': 0.189873417721519,\n",
       "    'p': 0.5555555555555556,\n",
       "    'f': 0.2830188641278035}},\n",
       "  {'rouge-1': {'r': 0.17088607594936708, 'p': 0.54, 'f': 0.2596153809633876},\n",
       "   'rouge-2': {'r': 0.07346938775510205,\n",
       "    'p': 0.2535211267605634,\n",
       "    'f': 0.11392404714889454},\n",
       "   'rouge-l': {'r': 0.15822784810126583, 'p': 0.5, 'f': 0.2403846117326184}},\n",
       "  {'rouge-1': {'r': 0.2848101265822785,\n",
       "    'p': 0.6617647058823529,\n",
       "    'f': 0.39823008428851125},\n",
       "   'rouge-2': {'r': 0.1673469387755102,\n",
       "    'p': 0.40594059405940597,\n",
       "    'f': 0.23699421551922892},\n",
       "   'rouge-l': {'r': 0.27848101265822783,\n",
       "    'p': 0.6470588235294118,\n",
       "    'f': 0.38938052676638735}},\n",
       "  {'rouge-1': {'r': 0.25949367088607594,\n",
       "    'p': 0.7884615384615384,\n",
       "    'f': 0.3904761867501135},\n",
       "   'rouge-2': {'r': 0.12653061224489795,\n",
       "    'p': 0.4626865671641791,\n",
       "    'f': 0.19871794534537315},\n",
       "   'rouge-l': {'r': 0.24050632911392406,\n",
       "    'p': 0.7307692307692307,\n",
       "    'f': 0.3619047581786848}},\n",
       "  {'rouge-1': {'r': 0.35443037974683544,\n",
       "    'p': 0.7567567567567568,\n",
       "    'f': 0.48275861634512496},\n",
       "   'rouge-2': {'r': 0.23265306122448978,\n",
       "    'p': 0.59375,\n",
       "    'f': 0.33431084639450986},\n",
       "   'rouge-l': {'r': 0.34177215189873417,\n",
       "    'p': 0.7297297297297297,\n",
       "    'f': 0.4655172370347801}},\n",
       "  {'rouge-1': {'r': 0.3684210526315789,\n",
       "    'p': 0.2692307692307692,\n",
       "    'f': 0.3111111062320988},\n",
       "   'rouge-2': {'r': 0.14705882352941177,\n",
       "    'p': 0.09345794392523364,\n",
       "    'f': 0.11428570953404102},\n",
       "   'rouge-l': {'r': 0.3684210526315789,\n",
       "    'p': 0.2692307692307692,\n",
       "    'f': 0.3111111062320988}},\n",
       "  {'rouge-1': {'r': 0.22807017543859648,\n",
       "    'p': 0.2653061224489796,\n",
       "    'f': 0.2452830138964045},\n",
       "   'rouge-2': {'r': 0.07352941176470588,\n",
       "    'p': 0.08333333333333333,\n",
       "    'f': 0.07812499501953157},\n",
       "   'rouge-l': {'r': 0.21052631578947367,\n",
       "    'p': 0.24489795918367346,\n",
       "    'f': 0.22641508936810262}},\n",
       "  {'rouge-1': {'r': 0.2807017543859649,\n",
       "    'p': 0.2077922077922078,\n",
       "    'f': 0.23880596526063722},\n",
       "   'rouge-2': {'r': 0.058823529411764705,\n",
       "    'p': 0.04040404040404041,\n",
       "    'f': 0.04790418678905711},\n",
       "   'rouge-l': {'r': 0.2631578947368421,\n",
       "    'p': 0.19480519480519481,\n",
       "    'f': 0.22388059212630887}},\n",
       "  {'rouge-1': {'r': 0.14035087719298245,\n",
       "    'p': 0.6153846153846154,\n",
       "    'f': 0.2285714255469388},\n",
       "   'rouge-2': {'r': 0.029411764705882353,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.04938271335467169},\n",
       "   'rouge-l': {'r': 0.14035087719298245,\n",
       "    'p': 0.6153846153846154,\n",
       "    'f': 0.2285714255469388}},\n",
       "  {'rouge-1': {'r': 0.40350877192982454,\n",
       "    'p': 0.4339622641509434,\n",
       "    'f': 0.4181818131884298},\n",
       "   'rouge-2': {'r': 0.16176470588235295,\n",
       "    'p': 0.1506849315068493,\n",
       "    'f': 0.1560283638006138},\n",
       "   'rouge-l': {'r': 0.40350877192982454,\n",
       "    'p': 0.4339622641509434,\n",
       "    'f': 0.4181818131884298}},\n",
       "  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       "  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       "  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       "  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       "  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}},\n",
       "  {'rouge-1': {'r': 0.38202247191011235,\n",
       "    'p': 0.576271186440678,\n",
       "    'f': 0.4594594546649014},\n",
       "   'rouge-2': {'r': 0.18421052631578946,\n",
       "    'p': 0.2916666666666667,\n",
       "    'f': 0.2258064468678461},\n",
       "   'rouge-l': {'r': 0.34831460674157305,\n",
       "    'p': 0.5254237288135594,\n",
       "    'f': 0.4189189141243609}},\n",
       "  {'rouge-1': {'r': 0.16853932584269662,\n",
       "    'p': 0.7142857142857143,\n",
       "    'f': 0.2727272696380165},\n",
       "   'rouge-2': {'r': 0.05263157894736842, 'p': 0.3, 'f': 0.08955223626642911},\n",
       "   'rouge-l': {'r': 0.14606741573033707,\n",
       "    'p': 0.6190476190476191,\n",
       "    'f': 0.23636363327438017}},\n",
       "  {'rouge-1': {'r': 0.30337078651685395,\n",
       "    'p': 0.3698630136986301,\n",
       "    'f': 0.33333332838210644},\n",
       "   'rouge-2': {'r': 0.13157894736842105,\n",
       "    'p': 0.16853932584269662,\n",
       "    'f': 0.1477832463073602},\n",
       "   'rouge-l': {'r': 0.2696629213483146,\n",
       "    'p': 0.3287671232876712,\n",
       "    'f': 0.2962962913450694}},\n",
       "  {'rouge-1': {'r': 0.12359550561797752,\n",
       "    'p': 0.7857142857142857,\n",
       "    'f': 0.21359223066075975},\n",
       "   'rouge-2': {'r': 0.06140350877192982,\n",
       "    'p': 0.5384615384615384,\n",
       "    'f': 0.1102362186347573},\n",
       "   'rouge-l': {'r': 0.12359550561797752,\n",
       "    'p': 0.7857142857142857,\n",
       "    'f': 0.21359223066075975}},\n",
       "  {'rouge-1': {'r': 0.19101123595505617,\n",
       "    'p': 0.6538461538461539,\n",
       "    'f': 0.2956521704136106},\n",
       "   'rouge-2': {'r': 0.017543859649122806,\n",
       "    'p': 0.06451612903225806,\n",
       "    'f': 0.027586203534839884},\n",
       "   'rouge-l': {'r': 0.14606741573033707, 'p': 0.5, 'f': 0.22608695302230625}},\n",
       "  {'rouge-1': {'r': 0.36363636363636365,\n",
       "    'p': 0.425531914893617,\n",
       "    'f': 0.39215685777585546},\n",
       "   'rouge-2': {'r': 0.136986301369863,\n",
       "    'p': 0.16666666666666666,\n",
       "    'f': 0.15037593489739404},\n",
       "   'rouge-l': {'r': 0.32727272727272727,\n",
       "    'p': 0.3829787234042553,\n",
       "    'f': 0.3529411715013457}},\n",
       "  {'rouge-1': {'r': 0.4727272727272727,\n",
       "    'p': 0.4482758620689655,\n",
       "    'f': 0.46017698615396674},\n",
       "   'rouge-2': {'r': 0.2602739726027397, 'p': 0.2375, 'f': 0.2483660080823616},\n",
       "   'rouge-l': {'r': 0.45454545454545453,\n",
       "    'p': 0.43103448275862066,\n",
       "    'f': 0.44247787110971887}},\n",
       "  {'rouge-1': {'r': 0.41818181818181815,\n",
       "    'p': 0.5348837209302325,\n",
       "    'f': 0.46938775017700957},\n",
       "   'rouge-2': {'r': 0.1917808219178082,\n",
       "    'p': 0.24561403508771928,\n",
       "    'f': 0.21538461046035515},\n",
       "   'rouge-l': {'r': 0.41818181818181815,\n",
       "    'p': 0.5348837209302325,\n",
       "    'f': 0.46938775017700957}},\n",
       "  {'rouge-1': {'r': 0.3090909090909091,\n",
       "    'p': 0.3469387755102041,\n",
       "    'f': 0.326923071939719},\n",
       "   'rouge-2': {'r': 0.1232876712328767,\n",
       "    'p': 0.140625,\n",
       "    'f': 0.13138685633544694},\n",
       "   'rouge-l': {'r': 0.2909090909090909,\n",
       "    'p': 0.32653061224489793,\n",
       "    'f': 0.3076923027089497}},\n",
       "  {'rouge-1': {'r': 0.41818181818181815,\n",
       "    'p': 0.2839506172839506,\n",
       "    'f': 0.33823528930038926},\n",
       "   'rouge-2': {'r': 0.1917808219178082,\n",
       "    'p': 0.12173913043478261,\n",
       "    'f': 0.14893616546231342},\n",
       "   'rouge-l': {'r': 0.4, 'p': 0.2716049382716049, 'f': 0.32352940694744814}},\n",
       "  {'rouge-1': {'r': 0.43137254901960786,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.47311827461671874},\n",
       "   'rouge-2': {'r': 0.22033898305084745,\n",
       "    'p': 0.2653061224489796,\n",
       "    'f': 0.2407407357836078},\n",
       "   'rouge-l': {'r': 0.4117647058823529, 'p': 0.5, 'f': 0.4516128982726327}},\n",
       "  {'rouge-1': {'r': 0.2549019607843137,\n",
       "    'p': 0.4482758620689655,\n",
       "    'f': 0.324999995378125},\n",
       "   'rouge-2': {'r': 0.1016949152542373, 'p': 0.1875, 'f': 0.13186812730829625},\n",
       "   'rouge-l': {'r': 0.2549019607843137,\n",
       "    'p': 0.4482758620689655,\n",
       "    'f': 0.324999995378125}},\n",
       "  {'rouge-1': {'r': 0.17647058823529413,\n",
       "    'p': 0.2727272727272727,\n",
       "    'f': 0.2142857095153062},\n",
       "   'rouge-2': {'r': 0.03389830508474576,\n",
       "    'p': 0.05263157894736842,\n",
       "    'f': 0.0412371086364125},\n",
       "   'rouge-l': {'r': 0.13725490196078433,\n",
       "    'p': 0.21212121212121213,\n",
       "    'f': 0.16666666189625867}},\n",
       "  {'rouge-1': {'r': 0.09803921568627451,\n",
       "    'p': 0.38461538461538464,\n",
       "    'f': 0.1562499967626954},\n",
       "   'rouge-2': {'r': 0.01694915254237288,\n",
       "    'p': 0.07692307692307693,\n",
       "    'f': 0.027777774818673155},\n",
       "   'rouge-l': {'r': 0.0784313725490196,\n",
       "    'p': 0.3076923076923077,\n",
       "    'f': 0.1249999967626954}},\n",
       "  {'rouge-1': {'r': 0.09803921568627451,\n",
       "    'p': 0.14705882352941177,\n",
       "    'f': 0.1176470540235296},\n",
       "   'rouge-2': {'r': 0.01694915254237288,\n",
       "    'p': 0.023255813953488372,\n",
       "    'f': 0.01960783826028572},\n",
       "   'rouge-l': {'r': 0.09803921568627451,\n",
       "    'p': 0.14705882352941177,\n",
       "    'f': 0.1176470540235296}},\n",
       "  {'rouge-1': {'r': 0.39361702127659576,\n",
       "    'p': 0.5211267605633803,\n",
       "    'f': 0.4484848435820019},\n",
       "   'rouge-2': {'r': 0.19166666666666668,\n",
       "    'p': 0.25842696629213485,\n",
       "    'f': 0.2200956888899065},\n",
       "   'rouge-l': {'r': 0.3723404255319149,\n",
       "    'p': 0.49295774647887325,\n",
       "    'f': 0.42424241933957757}},\n",
       "  {'rouge-1': {'r': 0.30851063829787234,\n",
       "    'p': 0.5272727272727272,\n",
       "    'f': 0.38926174030899513},\n",
       "   'rouge-2': {'r': 0.10833333333333334, 'p': 0.2, 'f': 0.1405405359824691},\n",
       "   'rouge-l': {'r': 0.26595744680851063,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.33557046514120986}},\n",
       "  {'rouge-1': {'r': 0.43617021276595747,\n",
       "    'p': 0.6307692307692307,\n",
       "    'f': 0.5157232656065821},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.2823529411764706, 'f': 0.23414633660916132},\n",
       "   'rouge-l': {'r': 0.3829787234042553,\n",
       "    'p': 0.5538461538461539,\n",
       "    'f': 0.4528301838455758}},\n",
       "  {'rouge-1': {'r': 0.30851063829787234,\n",
       "    'p': 0.5178571428571429,\n",
       "    'f': 0.3866666619875556},\n",
       "   'rouge-2': {'r': 0.11666666666666667,\n",
       "    'p': 0.18666666666666668,\n",
       "    'f': 0.14358973885601592},\n",
       "   'rouge-l': {'r': 0.23404255319148937,\n",
       "    'p': 0.39285714285714285,\n",
       "    'f': 0.2933333286542223}},\n",
       "  {'rouge-1': {'r': 0.2872340425531915,\n",
       "    'p': 0.5869565217391305,\n",
       "    'f': 0.3857142813020409},\n",
       "   'rouge-2': {'r': 0.10833333333333334,\n",
       "    'p': 0.24528301886792453,\n",
       "    'f': 0.1502890130909821},\n",
       "   'rouge-l': {'r': 0.2553191489361702,\n",
       "    'p': 0.5217391304347826,\n",
       "    'f': 0.3428571384448979}},\n",
       "  {'rouge-1': {'r': 0.5238095238095238,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.5238095188095239},\n",
       "   'rouge-2': {'r': 0.3958333333333333,\n",
       "    'p': 0.3584905660377358,\n",
       "    'f': 0.37623761877462997},\n",
       "   'rouge-l': {'r': 0.47619047619047616,\n",
       "    'p': 0.47619047619047616,\n",
       "    'f': 0.4761904711904762}},\n",
       "  {'rouge-1': {'r': 0.40476190476190477,\n",
       "    'p': 0.4857142857142857,\n",
       "    'f': 0.44155843659976396},\n",
       "   'rouge-2': {'r': 0.2708333333333333, 'p': 0.325, 'f': 0.2954545404958678},\n",
       "   'rouge-l': {'r': 0.40476190476190477,\n",
       "    'p': 0.4857142857142857,\n",
       "    'f': 0.44155843659976396}},\n",
       "  {'rouge-1': {'r': 0.38095238095238093, 'p': 0.5, 'f': 0.43243242752373995},\n",
       "   'rouge-2': {'r': 0.2916666666666667,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.31111110613333337},\n",
       "   'rouge-l': {'r': 0.35714285714285715,\n",
       "    'p': 0.46875,\n",
       "    'f': 0.405405400496713}},\n",
       "  {'rouge-1': {'r': 0.7619047619047619,\n",
       "    'p': 0.5079365079365079,\n",
       "    'f': 0.6095238047238095},\n",
       "   'rouge-2': {'r': 0.5208333333333334,\n",
       "    'p': 0.2976190476190476,\n",
       "    'f': 0.37878787415977966},\n",
       "   'rouge-l': {'r': 0.7380952380952381,\n",
       "    'p': 0.49206349206349204,\n",
       "    'f': 0.5904761856761905}},\n",
       "  {'rouge-1': {'r': 0.5952380952380952,\n",
       "    'p': 0.5319148936170213,\n",
       "    'f': 0.5617977478247695},\n",
       "   'rouge-2': {'r': 0.4166666666666667,\n",
       "    'p': 0.3076923076923077,\n",
       "    'f': 0.3539822959981205},\n",
       "   'rouge-l': {'r': 0.5476190476190477,\n",
       "    'p': 0.48936170212765956,\n",
       "    'f': 0.5168539276000507}},\n",
       "  {'rouge-1': {'r': 0.6071428571428571,\n",
       "    'p': 0.576271186440678,\n",
       "    'f': 0.5913043428294897},\n",
       "   'rouge-2': {'r': 0.4142857142857143,\n",
       "    'p': 0.43283582089552236,\n",
       "    'f': 0.4233576592359743},\n",
       "   'rouge-l': {'r': 0.5714285714285714,\n",
       "    'p': 0.5423728813559322,\n",
       "    'f': 0.5565217341338374}},\n",
       "  {'rouge-1': {'r': 0.4107142857142857,\n",
       "    'p': 0.5476190476190477,\n",
       "    'f': 0.4693877502040817},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.3111111111111111, 'f': 0.2434782561058602},\n",
       "   'rouge-l': {'r': 0.39285714285714285,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.44897958693877554}},\n",
       "  {'rouge-1': {'r': 0.23214285714285715,\n",
       "    'p': 0.48148148148148145,\n",
       "    'f': 0.3132530076585862},\n",
       "   'rouge-2': {'r': 0.1, 'p': 0.2692307692307692, 'f': 0.14583332938368065},\n",
       "   'rouge-l': {'r': 0.21428571428571427,\n",
       "    'p': 0.4444444444444444,\n",
       "    'f': 0.2891566221164175}},\n",
       "  {'rouge-1': {'r': 0.32142857142857145,\n",
       "    'p': 0.34615384615384615,\n",
       "    'f': 0.3333333283401921},\n",
       "   'rouge-2': {'r': 0.1, 'p': 0.1, 'f': 0.09999999500000027},\n",
       "   'rouge-l': {'r': 0.32142857142857145,\n",
       "    'p': 0.34615384615384615,\n",
       "    'f': 0.3333333283401921}},\n",
       "  {'rouge-1': {'r': 0.2857142857142857,\n",
       "    'p': 0.41025641025641024,\n",
       "    'f': 0.33684210042326873},\n",
       "   'rouge-2': {'r': 0.12857142857142856, 'p': 0.2, 'f': 0.15652173436672984},\n",
       "   'rouge-l': {'r': 0.2857142857142857,\n",
       "    'p': 0.41025641025641024,\n",
       "    'f': 0.33684210042326873}},\n",
       "  {'rouge-1': {'r': 0.379746835443038,\n",
       "    'p': 0.6666666666666666,\n",
       "    'f': 0.48387096311784605},\n",
       "   'rouge-2': {'r': 0.24489795918367346,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.30967741470468263},\n",
       "   'rouge-l': {'r': 0.34177215189873417, 'p': 0.6, 'f': 0.4354838663436525}},\n",
       "  {'rouge-1': {'r': 0.35443037974683544,\n",
       "    'p': 0.6511627906976745,\n",
       "    'f': 0.45901638887798996},\n",
       "   'rouge-2': {'r': 0.23469387755102042,\n",
       "    'p': 0.42592592592592593,\n",
       "    'f': 0.3026315743663436},\n",
       "   'rouge-l': {'r': 0.3291139240506329,\n",
       "    'p': 0.6046511627906976,\n",
       "    'f': 0.42622950363208817}},\n",
       "  {'rouge-1': {'r': 0.27848101265822783,\n",
       "    'p': 0.38596491228070173,\n",
       "    'f': 0.323529406895545},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.20588235294117646,\n",
       "    'f': 0.16867469395848467},\n",
       "   'rouge-l': {'r': 0.26582278481012656,\n",
       "    'p': 0.3684210526315789,\n",
       "    'f': 0.30882352454260387}},\n",
       "  {'rouge-1': {'r': 0.31645569620253167,\n",
       "    'p': 0.5681818181818182,\n",
       "    'f': 0.406504060445502},\n",
       "   'rouge-2': {'r': 0.22448979591836735, 'p': 0.4, 'f': 0.2875816947413389},\n",
       "   'rouge-l': {'r': 0.31645569620253167,\n",
       "    'p': 0.5681818181818182,\n",
       "    'f': 0.406504060445502}},\n",
       "  {'rouge-1': {'r': 0.22784810126582278, 'p': 0.6, 'f': 0.330275225368235},\n",
       "   'rouge-2': {'r': 0.1326530612244898,\n",
       "    'p': 0.3939393939393939,\n",
       "    'f': 0.198473278673737},\n",
       "   'rouge-l': {'r': 0.22784810126582278, 'p': 0.6, 'f': 0.330275225368235}},\n",
       "  {'rouge-1': {'r': 0.14772727272727273,\n",
       "    'p': 0.5909090909090909,\n",
       "    'f': 0.23636363316363643},\n",
       "   'rouge-2': {'r': 0.059322033898305086,\n",
       "    'p': 0.30434782608695654,\n",
       "    'f': 0.09929077741159908},\n",
       "   'rouge-l': {'r': 0.14772727272727273,\n",
       "    'p': 0.5909090909090909,\n",
       "    'f': 0.23636363316363643}},\n",
       "  {'rouge-1': {'r': 0.18181818181818182,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.27586206530321045},\n",
       "   'rouge-2': {'r': 0.0847457627118644,\n",
       "    'p': 0.30303030303030304,\n",
       "    'f': 0.13245032771018825},\n",
       "   'rouge-l': {'r': 0.1590909090909091, 'p': 0.5, 'f': 0.24137930668252086}},\n",
       "  {'rouge-1': {'r': 0.4318181818181818,\n",
       "    'p': 0.6229508196721312,\n",
       "    'f': 0.5100671092581416},\n",
       "   'rouge-2': {'r': 0.2542372881355932,\n",
       "    'p': 0.36585365853658536,\n",
       "    'f': 0.29999999516200004},\n",
       "   'rouge-l': {'r': 0.3977272727272727,\n",
       "    'p': 0.5737704918032787,\n",
       "    'f': 0.4697986528823026}},\n",
       "  {'rouge-1': {'r': 0.11363636363636363,\n",
       "    'p': 0.5263157894736842,\n",
       "    'f': 0.1869158849296882},\n",
       "   'rouge-2': {'r': 0.0423728813559322,\n",
       "    'p': 0.2777777777777778,\n",
       "    'f': 0.07352940946799315},\n",
       "   'rouge-l': {'r': 0.10227272727272728,\n",
       "    'p': 0.47368421052631576,\n",
       "    'f': 0.16822429614464152}},\n",
       "  {'rouge-1': {'r': 0.32954545454545453,\n",
       "    'p': 0.42028985507246375,\n",
       "    'f': 0.36942674666558484},\n",
       "   'rouge-2': {'r': 0.1271186440677966,\n",
       "    'p': 0.18072289156626506,\n",
       "    'f': 0.14925372649488888},\n",
       "   'rouge-l': {'r': 0.32954545454545453,\n",
       "    'p': 0.42028985507246375,\n",
       "    'f': 0.36942674666558484}},\n",
       "  {'rouge-1': {'r': 0.35384615384615387,\n",
       "    'p': 0.6571428571428571,\n",
       "    'f': 0.45999999545000014},\n",
       "   'rouge-2': {'r': 0.24719101123595505,\n",
       "    'p': 0.4489795918367347,\n",
       "    'f': 0.3188405751302248},\n",
       "   'rouge-l': {'r': 0.3384615384615385,\n",
       "    'p': 0.6285714285714286,\n",
       "    'f': 0.43999999545000007}},\n",
       "  {'rouge-1': {'r': 0.16923076923076924, 'p': 0.6875, 'f': 0.2716049351013565},\n",
       "   'rouge-2': {'r': 0.0449438202247191,\n",
       "    'p': 0.23529411764705882,\n",
       "    'f': 0.07547169542007842},\n",
       "   'rouge-l': {'r': 0.15384615384615385,\n",
       "    'p': 0.625,\n",
       "    'f': 0.24691357707666517}},\n",
       "  {'rouge-1': {'r': 0.3384615384615385, 'p': 0.55, 'f': 0.4190476143310658},\n",
       "   'rouge-2': {'r': 0.1797752808988764,\n",
       "    'p': 0.3076923076923077,\n",
       "    'f': 0.22695034995422775},\n",
       "   'rouge-l': {'r': 0.3384615384615385, 'p': 0.55, 'f': 0.4190476143310658}},\n",
       "  {'rouge-1': {'r': 0.2, 'p': 0.65, 'f': 0.30588234934256053},\n",
       "   'rouge-2': {'r': 0.06741573033707865, 'p': 0.3, 'f': 0.11009174012288536},\n",
       "   'rouge-l': {'r': 0.16923076923076924, 'p': 0.55, 'f': 0.2588235258131488}},\n",
       "  {'rouge-1': {'r': 0.16923076923076924, 'p': 0.6875, 'f': 0.2716049351013565},\n",
       "   'rouge-2': {'r': 0.056179775280898875,\n",
       "    'p': 0.29411764705882354,\n",
       "    'f': 0.09433961994838029},\n",
       "   'rouge-l': {'r': 0.15384615384615385,\n",
       "    'p': 0.625,\n",
       "    'f': 0.24691357707666517}},\n",
       "  {'rouge-1': {'r': 0.38461538461538464,\n",
       "    'p': 0.3472222222222222,\n",
       "    'f': 0.36496349866268846},\n",
       "   'rouge-2': {'r': 0.20512820512820512,\n",
       "    'p': 0.1797752808988764,\n",
       "    'f': 0.19161676148875914},\n",
       "   'rouge-l': {'r': 0.35384615384615387,\n",
       "    'p': 0.3194444444444444,\n",
       "    'f': 0.3357664183707177}},\n",
       "  {'rouge-1': {'r': 0.3076923076923077,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.3199999950080001},\n",
       "   'rouge-2': {'r': 0.11538461538461539,\n",
       "    'p': 0.1232876712328767,\n",
       "    'f': 0.11920529301872745},\n",
       "   'rouge-l': {'r': 0.27692307692307694, 'p': 0.3, 'f': 0.2879999950080001}},\n",
       "  {'rouge-1': {'r': 0.4, 'p': 0.32098765432098764, 'f': 0.3561643786216927},\n",
       "   'rouge-2': {'r': 0.20512820512820512,\n",
       "    'p': 0.15841584158415842,\n",
       "    'f': 0.178770944803221},\n",
       "   'rouge-l': {'r': 0.38461538461538464,\n",
       "    'p': 0.30864197530864196,\n",
       "    'f': 0.34246574848470634}},\n",
       "  {'rouge-1': {'r': 0.2, 'p': 0.22807017543859648, 'f': 0.2131147491198604},\n",
       "   'rouge-2': {'r': 0.05128205128205128,\n",
       "    'p': 0.056338028169014086,\n",
       "    'f': 0.05369127017882125},\n",
       "   'rouge-l': {'r': 0.15384615384615385,\n",
       "    'p': 0.17543859649122806,\n",
       "    'f': 0.16393442125100796}},\n",
       "  {'rouge-1': {'r': 0.3384615384615385,\n",
       "    'p': 0.3384615384615385,\n",
       "    'f': 0.33846153346153857},\n",
       "   'rouge-2': {'r': 0.11538461538461539,\n",
       "    'p': 0.1111111111111111,\n",
       "    'f': 0.11320754217159153},\n",
       "   'rouge-l': {'r': 0.27692307692307694,\n",
       "    'p': 0.27692307692307694,\n",
       "    'f': 0.276923071923077}},\n",
       "  {'rouge-1': {'r': 0.36363636363636365,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.3636363586363637},\n",
       "   'rouge-2': {'r': 0.21621621621621623,\n",
       "    'p': 0.20512820512820512,\n",
       "    'f': 0.2105263107929364},\n",
       "   'rouge-l': {'r': 0.36363636363636365,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.3636363586363637}},\n",
       "  {'rouge-1': {'r': 0.5151515151515151,\n",
       "    'p': 0.4146341463414634,\n",
       "    'f': 0.4594594545178963},\n",
       "   'rouge-2': {'r': 0.2702702702702703, 'p': 0.2, 'f': 0.22988505258290406},\n",
       "   'rouge-l': {'r': 0.48484848484848486,\n",
       "    'p': 0.3902439024390244,\n",
       "    'f': 0.43243242749086924}},\n",
       "  {'rouge-1': {'r': 0.36363636363636365,\n",
       "    'p': 0.3157894736842105,\n",
       "    'f': 0.33802816403888125},\n",
       "   'rouge-2': {'r': 0.13513513513513514,\n",
       "    'p': 0.11627906976744186,\n",
       "    'f': 0.1249999950281252},\n",
       "   'rouge-l': {'r': 0.3333333333333333,\n",
       "    'p': 0.2894736842105263,\n",
       "    'f': 0.3098591499543742}},\n",
       "  {'rouge-1': {'r': 0.12121212121212122,\n",
       "    'p': 0.26666666666666666,\n",
       "    'f': 0.16666666236979177},\n",
       "   'rouge-2': {'r': 0.02702702702702703,\n",
       "    'p': 0.07142857142857142,\n",
       "    'f': 0.03921568229142678},\n",
       "   'rouge-l': {'r': 0.09090909090909091, 'p': 0.2, 'f': 0.12499999570312517}},\n",
       "  {'rouge-1': {'r': 0.45454545454545453,\n",
       "    'p': 0.32608695652173914,\n",
       "    'f': 0.379746830578433},\n",
       "   'rouge-2': {'r': 0.24324324324324326,\n",
       "    'p': 0.16363636363636364,\n",
       "    'f': 0.19565216910444247},\n",
       "   'rouge-l': {'r': 0.45454545454545453,\n",
       "    'p': 0.32608695652173914,\n",
       "    'f': 0.379746830578433}},\n",
       "  {'rouge-1': {'r': 0.45, 'p': 0.2857142857142857, 'f': 0.3495145583561127},\n",
       "   'rouge-2': {'r': 0.13043478260869565,\n",
       "    'p': 0.07142857142857142,\n",
       "    'f': 0.09230768773491146},\n",
       "   'rouge-l': {'r': 0.25, 'p': 0.15873015873015872, 'f': 0.19417475253087013}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.36363636363636365, 'f': 0.42105262670360116},\n",
       "   'rouge-2': {'r': 0.21739130434782608,\n",
       "    'p': 0.136986301369863,\n",
       "    'f': 0.1680672221481535},\n",
       "   'rouge-l': {'r': 0.425, 'p': 0.3090909090909091, 'f': 0.35789473196675897}},\n",
       "  {'rouge-1': {'r': 0.15, 'p': 0.35294117647058826, 'f': 0.21052631160357038},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.1, 'p': 0.23529411764705882, 'f': 0.1403508730070792}},\n",
       "  {'rouge-1': {'r': 0.325, 'p': 0.26, 'f': 0.2888888839506174},\n",
       "   'rouge-2': {'r': 0.08695652173913043,\n",
       "    'p': 0.07272727272727272,\n",
       "    'f': 0.07920791583178151},\n",
       "   'rouge-l': {'r': 0.275, 'p': 0.22, 'f': 0.24444443950617298}},\n",
       "  {'rouge-1': {'r': 0.125, 'p': 0.3333333333333333, 'f': 0.18181817785123974},\n",
       "   'rouge-2': {'r': 0.06521739130434782, 'p': 0.2, 'f': 0.0983606520290246},\n",
       "   'rouge-l': {'r': 0.125, 'p': 0.3333333333333333, 'f': 0.18181817785123974}},\n",
       "  {'rouge-1': {'r': 0.48484848484848486,\n",
       "    'p': 0.48484848484848486,\n",
       "    'f': 0.4848484798484849},\n",
       "   'rouge-2': {'r': 0.2571428571428571,\n",
       "    'p': 0.23076923076923078,\n",
       "    'f': 0.24324323825785255},\n",
       "   'rouge-l': {'r': 0.42424242424242425,\n",
       "    'p': 0.42424242424242425,\n",
       "    'f': 0.42424241924242434}},\n",
       "  {'rouge-1': {'r': 0.3333333333333333,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.40740740265432096},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.22727272727272727,\n",
       "    'f': 0.17543859175130824},\n",
       "   'rouge-l': {'r': 0.2727272727272727,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.3333333285802469}},\n",
       "  {'rouge-1': {'r': 0.3333333333333333,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.40740740265432096},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.23809523809523808,\n",
       "    'f': 0.1785714238839287},\n",
       "   'rouge-l': {'r': 0.2727272727272727,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.3333333285802469}},\n",
       "  {'rouge-1': {'r': 0.24242424242424243,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.3076923030547338},\n",
       "   'rouge-2': {'r': 0.11428571428571428,\n",
       "    'p': 0.18181818181818182,\n",
       "    'f': 0.14035087245306266},\n",
       "   'rouge-l': {'r': 0.24242424242424243,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.3076923030547338}},\n",
       "  {'rouge-1': {'r': 0.12121212121212122,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.1818181780681819},\n",
       "   'rouge-2': {'r': 0.05714285714285714,\n",
       "    'p': 0.18181818181818182,\n",
       "    'f': 0.08695651810018919},\n",
       "   'rouge-l': {'r': 0.12121212121212122,\n",
       "    'p': 0.36363636363636365,\n",
       "    'f': 0.1818181780681819}},\n",
       "  {'rouge-1': {'r': 0.2962962962962963,\n",
       "    'p': 0.1702127659574468,\n",
       "    'f': 0.21621621158144638},\n",
       "   'rouge-2': {'r': 0.14705882352941177,\n",
       "    'p': 0.07142857142857142,\n",
       "    'f': 0.09615384175295877},\n",
       "   'rouge-l': {'r': 0.2962962962962963,\n",
       "    'p': 0.1702127659574468,\n",
       "    'f': 0.21621621158144638}},\n",
       "  {'rouge-1': {'r': 0.25925925925925924,\n",
       "    'p': 0.30434782608695654,\n",
       "    'f': 0.27999999503200007},\n",
       "   'rouge-2': {'r': 0.11764705882352941,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.1333333284222224},\n",
       "   'rouge-l': {'r': 0.25925925925925924,\n",
       "    'p': 0.30434782608695654,\n",
       "    'f': 0.27999999503200007}},\n",
       "  {'rouge-1': {'r': 0.2222222222222222, 'p': 0.24, 'f': 0.2307692257766273},\n",
       "   'rouge-2': {'r': 0.14705882352941177,\n",
       "    'p': 0.18518518518518517,\n",
       "    'f': 0.16393442129535088},\n",
       "   'rouge-l': {'r': 0.2222222222222222, 'p': 0.24, 'f': 0.2307692257766273}},\n",
       "  {'rouge-1': {'r': 0.2222222222222222,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.2926829223319453},\n",
       "   'rouge-2': {'r': 0.11764705882352941,\n",
       "    'p': 0.3076923076923077,\n",
       "    'f': 0.17021276195563612},\n",
       "   'rouge-l': {'r': 0.2222222222222222,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.2926829223319453}},\n",
       "  {'rouge-1': {'r': 0.37037037037037035,\n",
       "    'p': 0.30303030303030304,\n",
       "    'f': 0.33333332838333335},\n",
       "   'rouge-2': {'r': 0.20588235294117646,\n",
       "    'p': 0.16666666666666666,\n",
       "    'f': 0.18421052137119123},\n",
       "   'rouge-l': {'r': 0.37037037037037035,\n",
       "    'p': 0.30303030303030304,\n",
       "    'f': 0.33333332838333335}},\n",
       "  {'rouge-1': {'r': 0.4, 'p': 0.4444444444444444, 'f': 0.42105262659279785},\n",
       "   'rouge-2': {'r': 0.1791044776119403,\n",
       "    'p': 0.20689655172413793,\n",
       "    'f': 0.19199999502592013},\n",
       "   'rouge-l': {'r': 0.32, 'p': 0.35555555555555557, 'f': 0.33684210027700834}},\n",
       "  {'rouge-1': {'r': 0.24, 'p': 0.23076923076923078, 'f': 0.23529411264898129},\n",
       "   'rouge-2': {'r': 0.04477611940298507,\n",
       "    'p': 0.037037037037037035,\n",
       "    'f': 0.04054053558528184},\n",
       "   'rouge-l': {'r': 0.22, 'p': 0.21153846153846154, 'f': 0.21568626951172637}},\n",
       "  {'rouge-1': {'r': 0.32, 'p': 0.4444444444444444, 'f': 0.37209301838831804},\n",
       "   'rouge-2': {'r': 0.14925373134328357,\n",
       "    'p': 0.2127659574468085,\n",
       "    'f': 0.17543859164512174},\n",
       "   'rouge-l': {'r': 0.3, 'p': 0.4166666666666667, 'f': 0.3488372044348297}},\n",
       "  {'rouge-1': {'r': 0.2, 'p': 0.6666666666666666, 'f': 0.3076923041420118},\n",
       "   'rouge-2': {'r': 0.16417910447761194,\n",
       "    'p': 0.5789473684210527,\n",
       "    'f': 0.25581395004597085},\n",
       "   'rouge-l': {'r': 0.2, 'p': 0.6666666666666666, 'f': 0.3076923041420118}},\n",
       "  {'rouge-1': {'r': 0.06, 'p': 0.21428571428571427, 'f': 0.09374999658203137},\n",
       "   'rouge-2': {'r': 0.029850746268656716,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.049999997278125145},\n",
       "   'rouge-l': {'r': 0.06, 'p': 0.21428571428571427, 'f': 0.09374999658203137}},\n",
       "  {'rouge-1': {'r': 0.42, 'p': 0.5384615384615384, 'f': 0.47191010743592976},\n",
       "   'rouge-2': {'r': 0.2833333333333333,\n",
       "    'p': 0.3148148148148148,\n",
       "    'f': 0.2982456090489382},\n",
       "   'rouge-l': {'r': 0.38, 'p': 0.48717948717948717, 'f': 0.42696628721121077}},\n",
       "  {'rouge-1': {'r': 0.28, 'p': 0.3181818181818182, 'f': 0.2978723354459032},\n",
       "   'rouge-2': {'r': 0.15, 'p': 0.16981132075471697, 'f': 0.15929203041741732},\n",
       "   'rouge-l': {'r': 0.26, 'p': 0.29545454545454547, 'f': 0.27659573970122237}},\n",
       "  {'rouge-1': {'r': 0.34, 'p': 0.4722222222222222, 'f': 0.39534883234180646},\n",
       "   'rouge-2': {'r': 0.16666666666666666,\n",
       "    'p': 0.2222222222222222,\n",
       "    'f': 0.19047618557823143},\n",
       "   'rouge-l': {'r': 0.32, 'p': 0.4444444444444444, 'f': 0.37209301838831804}},\n",
       "  {'rouge-1': {'r': 0.36, 'p': 0.5454545454545454, 'f': 0.43373493496879084},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.2926829268292683, 'f': 0.23762375755318116},\n",
       "   'rouge-l': {'r': 0.36, 'p': 0.5454545454545454, 'f': 0.43373493496879084}},\n",
       "  {'rouge-1': {'r': 0.44, 'p': 0.44, 'f': 0.43999999500000003},\n",
       "   'rouge-2': {'r': 0.18333333333333332,\n",
       "    'p': 0.18032786885245902,\n",
       "    'f': 0.18181817681852347},\n",
       "   'rouge-l': {'r': 0.36, 'p': 0.36, 'f': 0.359999995}},\n",
       "  {'rouge-1': {'r': 0.3584905660377358,\n",
       "    'p': 0.6129032258064516,\n",
       "    'f': 0.4523809477239229},\n",
       "   'rouge-2': {'r': 0.17543859649122806,\n",
       "    'p': 0.2857142857142857,\n",
       "    'f': 0.217391299633743},\n",
       "   'rouge-l': {'r': 0.33962264150943394,\n",
       "    'p': 0.5806451612903226,\n",
       "    'f': 0.4285714239143991}},\n",
       "  {'rouge-1': {'r': 0.41509433962264153,\n",
       "    'p': 0.4074074074074074,\n",
       "    'f': 0.4112149482714648},\n",
       "   'rouge-2': {'r': 0.24561403508771928,\n",
       "    'p': 0.2153846153846154,\n",
       "    'f': 0.22950819174281117},\n",
       "   'rouge-l': {'r': 0.39622641509433965,\n",
       "    'p': 0.3888888888888889,\n",
       "    'f': 0.3925233594864181}},\n",
       "  {'rouge-1': {'r': 0.41509433962264153,\n",
       "    'p': 0.6285714285714286,\n",
       "    'f': 0.4999999952091944},\n",
       "   'rouge-2': {'r': 0.24561403508771928,\n",
       "    'p': 0.3783783783783784,\n",
       "    'f': 0.2978723356518787},\n",
       "   'rouge-l': {'r': 0.39622641509433965, 'p': 0.6, 'f': 0.47727272248192154}},\n",
       "  {'rouge-1': {'r': 0.37735849056603776,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.39603959897264984},\n",
       "   'rouge-2': {'r': 0.21052631578947367,\n",
       "    'p': 0.21818181818181817,\n",
       "    'f': 0.21428570928730878},\n",
       "   'rouge-l': {'r': 0.37735849056603776,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.39603959897264984}},\n",
       "  {'rouge-1': {'r': 0.1509433962264151,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.2222222183371914},\n",
       "   'rouge-2': {'r': 0.05263157894736842,\n",
       "    'p': 0.16666666666666666,\n",
       "    'f': 0.07999999635200017},\n",
       "   'rouge-l': {'r': 0.1509433962264151,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.2222222183371914}},\n",
       "  {'rouge-1': {'r': 0.42105263157894735,\n",
       "    'p': 0.25806451612903225,\n",
       "    'f': 0.319999995288},\n",
       "   'rouge-2': {'r': 0.3, 'p': 0.18181818181818182, 'f': 0.22641508964044152},\n",
       "   'rouge-l': {'r': 0.42105263157894735,\n",
       "    'p': 0.25806451612903225,\n",
       "    'f': 0.319999995288}},\n",
       "  {'rouge-1': {'r': 0.5263157894736842,\n",
       "    'p': 0.38461538461538464,\n",
       "    'f': 0.4444444395654321},\n",
       "   'rouge-2': {'r': 0.35, 'p': 0.2413793103448276, 'f': 0.28571428088296547},\n",
       "   'rouge-l': {'r': 0.47368421052631576,\n",
       "    'p': 0.34615384615384615,\n",
       "    'f': 0.39999999512098766}},\n",
       "  {'rouge-1': {'r': 0.3684210526315789,\n",
       "    'p': 0.18421052631578946,\n",
       "    'f': 0.24561403064327492},\n",
       "   'rouge-2': {'r': 0.15, 'p': 0.06666666666666667, 'f': 0.09230768804733748},\n",
       "   'rouge-l': {'r': 0.3157894736842105,\n",
       "    'p': 0.15789473684210525,\n",
       "    'f': 0.21052631134502933}},\n",
       "  {'rouge-1': {'r': 0.3157894736842105, 'p': 0.5, 'f': 0.3870967694484912},\n",
       "   'rouge-2': {'r': 0.15, 'p': 0.2727272727272727, 'f': 0.19354838251821027},\n",
       "   'rouge-l': {'r': 0.2631578947368421,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.32258064041623313}},\n",
       "  {'rouge-1': {'r': 0.5263157894736842,\n",
       "    'p': 0.2127659574468085,\n",
       "    'f': 0.3030302989302112},\n",
       "   'rouge-2': {'r': 0.35, 'p': 0.1044776119402985, 'f': 0.16091953668912679},\n",
       "   'rouge-l': {'r': 0.5263157894736842,\n",
       "    'p': 0.2127659574468085,\n",
       "    'f': 0.3030302989302112}},\n",
       "  {'rouge-1': {'r': 0.10638297872340426,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.16393442269282457},\n",
       "   'rouge-2': {'r': 0.015625,\n",
       "    'p': 0.07692307692307693,\n",
       "    'f': 0.025974023167482173},\n",
       "   'rouge-l': {'r': 0.10638297872340426,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.16393442269282457}},\n",
       "  {'rouge-1': {'r': 0.5106382978723404,\n",
       "    'p': 0.7741935483870968,\n",
       "    'f': 0.6153846105950033},\n",
       "   'rouge-2': {'r': 0.3125, 'p': 0.5714285714285714, 'f': 0.40404039946944187},\n",
       "   'rouge-l': {'r': 0.5106382978723404,\n",
       "    'p': 0.7741935483870968,\n",
       "    'f': 0.6153846105950033}},\n",
       "  {'rouge-1': {'r': 0.1702127659574468, 'p': 0.8, 'f': 0.280701751492767},\n",
       "   'rouge-2': {'r': 0.0625, 'p': 0.4444444444444444, 'f': 0.10958903893413402},\n",
       "   'rouge-l': {'r': 0.1702127659574468, 'p': 0.8, 'f': 0.280701751492767}},\n",
       "  {'rouge-1': {'r': 0.1276595744680851,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.19672130793872616},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.10638297872340426,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.16393442269282457}},\n",
       "  {'rouge-1': {'r': 0.0851063829787234, 'p': 0.4, 'f': 0.1403508742997846},\n",
       "   'rouge-2': {'r': 0.03125,\n",
       "    'p': 0.2222222222222222,\n",
       "    'f': 0.054794518386188866},\n",
       "   'rouge-l': {'r': 0.0851063829787234, 'p': 0.4, 'f': 0.1403508742997846}},\n",
       "  {'rouge-1': {'r': 0.42105263157894735,\n",
       "    'p': 0.5333333333333333,\n",
       "    'f': 0.4705882303633218},\n",
       "   'rouge-2': {'r': 0.25, 'p': 0.29411764705882354, 'f': 0.270270265303141},\n",
       "   'rouge-l': {'r': 0.42105263157894735,\n",
       "    'p': 0.5333333333333333,\n",
       "    'f': 0.4705882303633218}},\n",
       "  {'rouge-1': {'r': 0.3684210526315789,\n",
       "    'p': 0.5833333333333334,\n",
       "    'f': 0.45161289848074926},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.3076923076923077, 'f': 0.24242423764921955},\n",
       "   'rouge-l': {'r': 0.3684210526315789,\n",
       "    'p': 0.5833333333333334,\n",
       "    'f': 0.45161289848074926}},\n",
       "  {'rouge-1': {'r': 0.2894736842105263, 'p': 0.5, 'f': 0.36666666202222226},\n",
       "   'rouge-2': {'r': 0.175, 'p': 0.2916666666666667, 'f': 0.21874999531250008},\n",
       "   'rouge-l': {'r': 0.2631578947368421,\n",
       "    'p': 0.45454545454545453,\n",
       "    'f': 0.33333332868888893}},\n",
       "  {'rouge-1': {'r': 0.15789473684210525,\n",
       "    'p': 0.42857142857142855,\n",
       "    'f': 0.23076922683431958},\n",
       "   'rouge-2': {'r': 0.075, 'p': 0.23076923076923078, 'f': 0.11320754346742624},\n",
       "   'rouge-l': {'r': 0.13157894736842105,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.19230768837278112}},\n",
       "  {'rouge-1': {'r': 0.3157894736842105,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.40677965643205977},\n",
       "   'rouge-2': {'r': 0.2, 'p': 0.34782608695652173, 'f': 0.2539682493323256},\n",
       "   'rouge-l': {'r': 0.2894736842105263,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.37288135134731404}},\n",
       "  {'rouge-1': {'r': 0.896551724137931,\n",
       "    'p': 0.7027027027027027,\n",
       "    'f': 0.7878787829522499},\n",
       "   'rouge-2': {'r': 0.8055555555555556,\n",
       "    'p': 0.6444444444444445,\n",
       "    'f': 0.7160493777777778},\n",
       "   'rouge-l': {'r': 0.896551724137931,\n",
       "    'p': 0.7027027027027027,\n",
       "    'f': 0.7878787829522499}},\n",
       "  {'rouge-1': {'r': 0.6896551724137931,\n",
       "    'p': 0.6060606060606061,\n",
       "    'f': 0.6451612853433925},\n",
       "   'rouge-2': {'r': 0.4166666666666667,\n",
       "    'p': 0.39473684210526316,\n",
       "    'f': 0.4054054004090578},\n",
       "   'rouge-l': {'r': 0.6206896551724138,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.5806451563111343}},\n",
       "  {'rouge-1': {'r': 0.4827586206896552,\n",
       "    'p': 0.42424242424242425,\n",
       "    'f': 0.45161289824661816},\n",
       "   'rouge-2': {'r': 0.1388888888888889,\n",
       "    'p': 0.13157894736842105,\n",
       "    'f': 0.1351351301387876},\n",
       "   'rouge-l': {'r': 0.4482758620689655,\n",
       "    'p': 0.3939393939393939,\n",
       "    'f': 0.4193548337304891}},\n",
       "  {'rouge-1': {'r': 0.5172413793103449,\n",
       "    'p': 0.7894736842105263,\n",
       "    'f': 0.6249999952170139},\n",
       "   'rouge-2': {'r': 0.5833333333333334, 'p': 0.75, 'f': 0.656249995078125},\n",
       "   'rouge-l': {'r': 0.5172413793103449,\n",
       "    'p': 0.7894736842105263,\n",
       "    'f': 0.6249999952170139}},\n",
       "  {'rouge-1': {'r': 0.13793103448275862,\n",
       "    'p': 0.07017543859649122,\n",
       "    'f': 0.09302325134396992},\n",
       "   'rouge-2': {'r': 0.027777777777777776,\n",
       "    'p': 0.013513513513513514,\n",
       "    'f': 0.018181813778513464},\n",
       "   'rouge-l': {'r': 0.13793103448275862,\n",
       "    'p': 0.07017543859649122,\n",
       "    'f': 0.09302325134396992}},\n",
       "  {'rouge-1': {'r': 0.37142857142857144,\n",
       "    'p': 0.5416666666666666,\n",
       "    'f': 0.4406779612754956},\n",
       "   'rouge-2': {'r': 0.21686746987951808,\n",
       "    'p': 0.3103448275862069,\n",
       "    'f': 0.25531914409335554},\n",
       "   'rouge-l': {'r': 0.37142857142857144,\n",
       "    'p': 0.5416666666666666,\n",
       "    'f': 0.4406779612754956}},\n",
       "  {'rouge-1': {'r': 0.44285714285714284,\n",
       "    'p': 0.484375,\n",
       "    'f': 0.4626865621742036},\n",
       "   'rouge-2': {'r': 0.3253012048192771, 'p': 0.3, 'f': 0.3121387233318855},\n",
       "   'rouge-l': {'r': 0.42857142857142855,\n",
       "    'p': 0.46875,\n",
       "    'f': 0.4477611890398752}},\n",
       "  {'rouge-1': {'r': 0.37142857142857144,\n",
       "    'p': 0.6842105263157895,\n",
       "    'f': 0.4814814769204391},\n",
       "   'rouge-2': {'r': 0.24096385542168675,\n",
       "    'p': 0.43478260869565216,\n",
       "    'f': 0.3100775147911784},\n",
       "   'rouge-l': {'r': 0.32857142857142857,\n",
       "    'p': 0.6052631578947368,\n",
       "    'f': 0.4259259213648834}},\n",
       "  {'rouge-1': {'r': 0.3142857142857143,\n",
       "    'p': 0.43137254901960786,\n",
       "    'f': 0.36363635875964767},\n",
       "   'rouge-2': {'r': 0.1686746987951807,\n",
       "    'p': 0.22580645161290322,\n",
       "    'f': 0.19310344338073732},\n",
       "   'rouge-l': {'r': 0.3, 'p': 0.4117647058823529, 'f': 0.3471074331398129}},\n",
       "  {'rouge-1': {'r': 0.1, 'p': 0.4375, 'f': 0.16279069464575452},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.1, 'p': 0.4375, 'f': 0.16279069464575452}},\n",
       "  {'rouge-1': {'r': 0.5882352941176471,\n",
       "    'p': 0.5660377358490566,\n",
       "    'f': 0.5769230719249261},\n",
       "   'rouge-2': {'r': 0.36666666666666664,\n",
       "    'p': 0.2972972972972973,\n",
       "    'f': 0.32835820400980176},\n",
       "   'rouge-l': {'r': 0.5882352941176471,\n",
       "    'p': 0.5660377358490566,\n",
       "    'f': 0.5769230719249261}},\n",
       "  {'rouge-1': {'r': 0.45098039215686275,\n",
       "    'p': 0.38333333333333336,\n",
       "    'f': 0.4144144094472852},\n",
       "   'rouge-2': {'r': 0.25, 'p': 0.18518518518518517, 'f': 0.21276595255771855},\n",
       "   'rouge-l': {'r': 0.3333333333333333,\n",
       "    'p': 0.2833333333333333,\n",
       "    'f': 0.306306301339177}},\n",
       "  {'rouge-1': {'r': 0.3333333333333333, 'p': 0.53125, 'f': 0.4096385494788794},\n",
       "   'rouge-2': {'r': 0.15, 'p': 0.2647058823529412, 'f': 0.1914893570846538},\n",
       "   'rouge-l': {'r': 0.27450980392156865,\n",
       "    'p': 0.4375,\n",
       "    'f': 0.3373493928523734}},\n",
       "  {'rouge-1': {'r': 0.39215686274509803, 'p': 0.625, 'f': 0.4819277061053855},\n",
       "   'rouge-2': {'r': 0.25, 'p': 0.35714285714285715, 'f': 0.2941176422145329},\n",
       "   'rouge-l': {'r': 0.39215686274509803, 'p': 0.625, 'f': 0.4819277061053855}},\n",
       "  {'rouge-1': {'r': 0.35294117647058826, 'p': 0.5625, 'f': 0.4337349350210481},\n",
       "   'rouge-2': {'r': 0.21666666666666667,\n",
       "    'p': 0.30952380952380953,\n",
       "    'f': 0.25490195594002313},\n",
       "   'rouge-l': {'r': 0.35294117647058826,\n",
       "    'p': 0.5625,\n",
       "    'f': 0.4337349350210481}},\n",
       "  {'rouge-1': {'r': 0.13114754098360656,\n",
       "    'p': 0.47058823529411764,\n",
       "    'f': 0.2051282017192637},\n",
       "   'rouge-2': {'r': 0.05555555555555555, 'p': 0.2, 'f': 0.08695651833648407},\n",
       "   'rouge-l': {'r': 0.11475409836065574,\n",
       "    'p': 0.4117647058823529,\n",
       "    'f': 0.17948717607823805}},\n",
       "  {'rouge-1': {'r': 0.13114754098360656, 'p': 0.5, 'f': 0.2077922044999157},\n",
       "   'rouge-2': {'r': 0.05555555555555555, 'p': 0.25, 'f': 0.09090908793388439},\n",
       "   'rouge-l': {'r': 0.13114754098360656, 'p': 0.5, 'f': 0.2077922044999157}},\n",
       "  {'rouge-1': {'r': 0.09836065573770492, 'p': 0.375, 'f': 0.1558441525518638},\n",
       "   'rouge-2': {'r': 0.027777777777777776,\n",
       "    'p': 0.1111111111111111,\n",
       "    'f': 0.044444441244444674},\n",
       "   'rouge-l': {'r': 0.08196721311475409,\n",
       "    'p': 0.3125,\n",
       "    'f': 0.12987012657783784}},\n",
       "  {'rouge-1': {'r': 0.16393442622950818,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.22471909681100874},\n",
       "   'rouge-2': {'r': 0.09722222222222222,\n",
       "    'p': 0.21875,\n",
       "    'f': 0.13461538035502973},\n",
       "   'rouge-l': {'r': 0.16393442622950818,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.22471909681100874}},\n",
       "  {'rouge-1': {'r': 0.09836065573770492, 'p': 0.4, 'f': 0.15789473367382278},\n",
       "   'rouge-2': {'r': 0.013888888888888888,\n",
       "    'p': 0.058823529411764705,\n",
       "    'f': 0.022471907021841105},\n",
       "   'rouge-l': {'r': 0.08196721311475409,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.13157894420013858}},\n",
       "  {'rouge-1': {'r': 0.8, 'p': 0.5970149253731343, 'f': 0.683760678866243},\n",
       "   'rouge-2': {'r': 0.7611940298507462,\n",
       "    'p': 0.5425531914893617,\n",
       "    'f': 0.6335403678114271},\n",
       "   'rouge-l': {'r': 0.8, 'p': 0.5970149253731343, 'f': 0.683760678866243}},\n",
       "  {'rouge-1': {'r': 0.48, 'p': 0.4444444444444444, 'f': 0.461538456545858},\n",
       "   'rouge-2': {'r': 0.34328358208955223,\n",
       "    'p': 0.2987012987012987,\n",
       "    'f': 0.31944443946855716},\n",
       "   'rouge-l': {'r': 0.48, 'p': 0.4444444444444444, 'f': 0.461538456545858}},\n",
       "  {'rouge-1': {'r': 0.28, 'p': 0.30434782608695654, 'f': 0.2916666616753473},\n",
       "   'rouge-2': {'r': 0.19402985074626866,\n",
       "    'p': 0.23214285714285715,\n",
       "    'f': 0.21138210886112777},\n",
       "   'rouge-l': {'r': 0.28, 'p': 0.30434782608695654, 'f': 0.2916666616753473}},\n",
       "  {'rouge-1': {'r': 0.12, 'p': 0.14634146341463414, 'f': 0.13186812691703917},\n",
       "   'rouge-2': {'r': 0.029850746268656716,\n",
       "    'p': 0.043478260869565216,\n",
       "    'f': 0.035398225261180076},\n",
       "   'rouge-l': {'r': 0.12, 'p': 0.14634146341463414, 'f': 0.13186812691703917}},\n",
       "  {'rouge-1': {'r': 0.14, 'p': 0.1320754716981132, 'f': 0.13592232510132926},\n",
       "   'rouge-2': {'r': 0.04477611940298507,\n",
       "    'p': 0.043478260869565216,\n",
       "    'f': 0.04411764205990541},\n",
       "   'rouge-l': {'r': 0.14, 'p': 0.1320754716981132, 'f': 0.13592232510132926}},\n",
       "  {'rouge-1': {'r': 0.5102040816326531,\n",
       "    'p': 0.28735632183908044,\n",
       "    'f': 0.3676470542138841},\n",
       "   'rouge-2': {'r': 0.2033898305084746,\n",
       "    'p': 0.09917355371900827,\n",
       "    'f': 0.13333332892654337},\n",
       "   'rouge-l': {'r': 0.46938775510204084,\n",
       "    'p': 0.26436781609195403,\n",
       "    'f': 0.33823528950800175}},\n",
       "  {'rouge-1': {'r': 0.16326530612244897,\n",
       "    'p': 0.6666666666666666,\n",
       "    'f': 0.2622950788067724},\n",
       "   'rouge-2': {'r': 0.06779661016949153,\n",
       "    'p': 0.3333333333333333,\n",
       "    'f': 0.11267605352906178},\n",
       "   'rouge-l': {'r': 0.10204081632653061,\n",
       "    'p': 0.4166666666666667,\n",
       "    'f': 0.16393442306906753}},\n",
       "  {'rouge-1': {'r': 0.32653061224489793,\n",
       "    'p': 0.47058823529411764,\n",
       "    'f': 0.3855421638380026},\n",
       "   'rouge-2': {'r': 0.1016949152542373,\n",
       "    'p': 0.15789473684210525,\n",
       "    'f': 0.12371133544053584},\n",
       "   'rouge-l': {'r': 0.2653061224489796,\n",
       "    'p': 0.38235294117647056,\n",
       "    'f': 0.3132530072114966}},\n",
       "  {'rouge-1': {'r': 0.3469387755102041,\n",
       "    'p': 0.4146341463414634,\n",
       "    'f': 0.377777772817284},\n",
       "   'rouge-2': {'r': 0.15254237288135594,\n",
       "    'p': 0.19148936170212766,\n",
       "    'f': 0.16981131581879688},\n",
       "   'rouge-l': {'r': 0.30612244897959184,\n",
       "    'p': 0.36585365853658536,\n",
       "    'f': 0.3333333283728396}},\n",
       "  {'rouge-1': {'r': 0.12244897959183673,\n",
       "    'p': 0.46153846153846156,\n",
       "    'f': 0.19354838378251824},\n",
       "   'rouge-2': {'r': 0.05084745762711865, 'p': 0.25, 'f': 0.08450703944455476},\n",
       "   'rouge-l': {'r': 0.12244897959183673,\n",
       "    'p': 0.46153846153846156,\n",
       "    'f': 0.19354838378251824}},\n",
       "  {'rouge-1': {'r': 0.26666666666666666,\n",
       "    'p': 0.0975609756097561,\n",
       "    'f': 0.14285713893494908},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.2, 'p': 0.07317073170731707, 'f': 0.1071428532206634}},\n",
       "  {'rouge-1': {'r': 0.7333333333333333,\n",
       "    'p': 0.7333333333333333,\n",
       "    'f': 0.7333333283333333},\n",
       "   'rouge-2': {'r': 0.5714285714285714,\n",
       "    'p': 0.5714285714285714,\n",
       "    'f': 0.5714285664285715},\n",
       "   'rouge-l': {'r': 0.7333333333333333,\n",
       "    'p': 0.7333333333333333,\n",
       "    'f': 0.7333333283333333}},\n",
       "  {'rouge-1': {'r': 0.3333333333333333,\n",
       "    'p': 0.21739130434782608,\n",
       "    'f': 0.2631578899584488},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.07692307692307693,\n",
       "    'f': 0.09999999545000023},\n",
       "   'rouge-l': {'r': 0.26666666666666666,\n",
       "    'p': 0.17391304347826086,\n",
       "    'f': 0.21052631101108046}},\n",
       "  {'rouge-1': {'r': 0.3333333333333333,\n",
       "    'p': 0.2631578947368421,\n",
       "    'f': 0.2941176421280277},\n",
       "   'rouge-2': {'r': 0.07142857142857142, 'p': 0.05, 'f': 0.05882352456747445},\n",
       "   'rouge-l': {'r': 0.3333333333333333,\n",
       "    'p': 0.2631578947368421,\n",
       "    'f': 0.2941176421280277}},\n",
       "  {'rouge-1': {'r': 0.4, 'p': 0.14634146341463414, 'f': 0.21428571036352043},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.044444444444444446,\n",
       "    'f': 0.0677966065498422},\n",
       "   'rouge-l': {'r': 0.3333333333333333,\n",
       "    'p': 0.12195121951219512,\n",
       "    'f': 0.17857142464923478}},\n",
       "  {'rouge-1': {'r': 0.4, 'p': 0.5172413793103449, 'f': 0.4511278146305614},\n",
       "   'rouge-2': {'r': 0.16666666666666666, 'p': 0.24, 'f': 0.19672130663800066},\n",
       "   'rouge-l': {'r': 0.38666666666666666, 'p': 0.5, 'f': 0.43609022064559894}},\n",
       "  {'rouge-1': {'r': 0.4533333333333333,\n",
       "    'p': 0.5964912280701754,\n",
       "    'f': 0.5151515102444904},\n",
       "   'rouge-2': {'r': 0.2777777777777778,\n",
       "    'p': 0.35714285714285715,\n",
       "    'f': 0.3124999950781251},\n",
       "   'rouge-l': {'r': 0.44, 'p': 0.5789473684210527, 'f': 0.49999999509297527}},\n",
       "  {'rouge-1': {'r': 0.21333333333333335,\n",
       "    'p': 0.42105263157894735,\n",
       "    'f': 0.28318583624402854},\n",
       "   'rouge-2': {'r': 0.06481481481481481,\n",
       "    'p': 0.12280701754385964,\n",
       "    'f': 0.08484848032617104},\n",
       "   'rouge-l': {'r': 0.2, 'p': 0.39473684210526316, 'f': 0.2654867211997808}},\n",
       "  {'rouge-1': {'r': 0.16, 'p': 0.48, 'f': 0.23999999625000001},\n",
       "   'rouge-2': {'r': 0.037037037037037035,\n",
       "    'p': 0.14814814814814814,\n",
       "    'f': 0.059259256059259434},\n",
       "   'rouge-l': {'r': 0.14666666666666667, 'p': 0.44, 'f': 0.21999999625000002}},\n",
       "  {'rouge-1': {'r': 0.21333333333333335,\n",
       "    'p': 0.25396825396825395,\n",
       "    'f': 0.23188405300882178},\n",
       "   'rouge-2': {'r': 0.046296296296296294,\n",
       "    'p': 0.0684931506849315,\n",
       "    'f': 0.05524861397149091},\n",
       "   'rouge-l': {'r': 0.17333333333333334,\n",
       "    'p': 0.20634920634920634,\n",
       "    'f': 0.1884057921392566}},\n",
       "  {'rouge-1': {'r': 0.5192307692307693, 'p': 0.5, 'f': 0.5094339572659311},\n",
       "   'rouge-2': {'r': 0.31746031746031744,\n",
       "    'p': 0.30303030303030304,\n",
       "    'f': 0.3100775143825492},\n",
       "   'rouge-l': {'r': 0.4807692307692308,\n",
       "    'p': 0.46296296296296297,\n",
       "    'f': 0.47169810820932717}},\n",
       "  {'rouge-1': {'r': 0.5961538461538461,\n",
       "    'p': 0.5636363636363636,\n",
       "    'f': 0.5794392473403791},\n",
       "   'rouge-2': {'r': 0.3333333333333333,\n",
       "    'p': 0.3181818181818182,\n",
       "    'f': 0.3255813903515414},\n",
       "   'rouge-l': {'r': 0.5576923076923077,\n",
       "    'p': 0.5272727272727272,\n",
       "    'f': 0.5420560697702858}},\n",
       "  {'rouge-1': {'r': 0.38461538461538464,\n",
       "    'p': 0.37037037037037035,\n",
       "    'f': 0.37735848556781776},\n",
       "   'rouge-2': {'r': 0.15873015873015872,\n",
       "    'p': 0.14925373134328357,\n",
       "    'f': 0.15384614885088774},\n",
       "   'rouge-l': {'r': 0.36538461538461536,\n",
       "    'p': 0.35185185185185186,\n",
       "    'f': 0.3584905610395159}},\n",
       "  {'rouge-1': {'r': 0.34615384615384615,\n",
       "    'p': 0.43902439024390244,\n",
       "    'f': 0.3870967692634986},\n",
       "   'rouge-2': {'r': 0.09523809523809523,\n",
       "    'p': 0.11764705882352941,\n",
       "    'f': 0.10526315295013873},\n",
       "   'rouge-l': {'r': 0.28846153846153844,\n",
       "    'p': 0.36585365853658536,\n",
       "    'f': 0.32258064023124067}},\n",
       "  {'rouge-1': {'r': 0.3269230769230769, 'p': 0.34, 'f': 0.3333333283352557},\n",
       "   'rouge-2': {'r': 0.14285714285714285,\n",
       "    'p': 0.13846153846153847,\n",
       "    'f': 0.1406249950012209},\n",
       "   'rouge-l': {'r': 0.2692307692307692, 'p': 0.28, 'f': 0.27450979892349103}},\n",
       "  {'rouge-1': {'r': 0.5416666666666666,\n",
       "    'p': 0.5416666666666666,\n",
       "    'f': 0.5416666616666667},\n",
       "   'rouge-2': {'r': 0.20833333333333334,\n",
       "    'p': 0.20833333333333334,\n",
       "    'f': 0.20833332833333348},\n",
       "   'rouge-l': {'r': 0.4583333333333333,\n",
       "    'p': 0.4583333333333333,\n",
       "    'f': 0.45833332833333335}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.41379310344827586, 'f': 0.45283018372374517},\n",
       "   'rouge-2': {'r': 0.16666666666666666, 'p': 0.125, 'f': 0.14285713795918387},\n",
       "   'rouge-l': {'r': 0.5, 'p': 0.41379310344827586, 'f': 0.45283018372374517}},\n",
       "  {'rouge-1': {'r': 0.5833333333333334,\n",
       "    'p': 0.4117647058823529,\n",
       "    'f': 0.4827586158382878},\n",
       "   'rouge-2': {'r': 0.25, 'p': 0.15789473684210525, 'f': 0.19354838235171706},\n",
       "   'rouge-l': {'r': 0.5, 'p': 0.35294117647058826, 'f': 0.41379309859690844}},\n",
       "  {'rouge-1': {'r': 0.4166666666666667, 'p': 0.4, 'f': 0.408163260308205},\n",
       "   'rouge-2': {'r': 0.125, 'p': 0.11538461538461539, 'f': 0.11999999500800021},\n",
       "   'rouge-l': {'r': 0.3333333333333333, 'p': 0.32, 'f': 0.32653060724698046}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.1791044776119403, 'f': 0.26373625985267485},\n",
       "   'rouge-2': {'r': 0.20833333333333334,\n",
       "    'p': 0.058823529411764705,\n",
       "    'f': 0.0917431158320008},\n",
       "   'rouge-l': {'r': 0.3333333333333333,\n",
       "    'p': 0.11940298507462686,\n",
       "    'f': 0.17582417194058697}},\n",
       "  {'rouge-1': {'r': 0.5952380952380952,\n",
       "    'p': 0.5681818181818182,\n",
       "    'f': 0.5813953438399135},\n",
       "   'rouge-2': {'r': 0.3333333333333333,\n",
       "    'p': 0.3090909090909091,\n",
       "    'f': 0.3207547119882521},\n",
       "   'rouge-l': {'r': 0.5952380952380952,\n",
       "    'p': 0.5681818181818182,\n",
       "    'f': 0.5813953438399135}},\n",
       "  {'rouge-1': {'r': 0.23809523809523808,\n",
       "    'p': 0.43478260869565216,\n",
       "    'f': 0.30769230311952667},\n",
       "   'rouge-2': {'r': 0.09803921568627451,\n",
       "    'p': 0.18518518518518517,\n",
       "    'f': 0.12820512367850115},\n",
       "   'rouge-l': {'r': 0.23809523809523808,\n",
       "    'p': 0.43478260869565216,\n",
       "    'f': 0.30769230311952667}},\n",
       "  {'rouge-1': {'r': 0.23809523809523808,\n",
       "    'p': 0.24390243902439024,\n",
       "    'f': 0.24096385042241264},\n",
       "   'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0},\n",
       "   'rouge-l': {'r': 0.19047619047619047,\n",
       "    'p': 0.1951219512195122,\n",
       "    'f': 0.19277107933807533}},\n",
       "  {'rouge-1': {'r': 0.5476190476190477,\n",
       "    'p': 0.4791666666666667,\n",
       "    'f': 0.5111111061333333},\n",
       "   'rouge-2': {'r': 0.23529411764705882, 'p': 0.2, 'f': 0.21621621124908705},\n",
       "   'rouge-l': {'r': 0.5, 'p': 0.4375, 'f': 0.46666666168888893}},\n",
       "  {'rouge-1': {'r': 0.09523809523809523,\n",
       "    'p': 0.2857142857142857,\n",
       "    'f': 0.14285713910714296},\n",
       "   'rouge-2': {'r': 0.0392156862745098,\n",
       "    'p': 0.15384615384615385,\n",
       "    'f': 0.062499996762695485},\n",
       "   'rouge-l': {'r': 0.09523809523809523,\n",
       "    'p': 0.2857142857142857,\n",
       "    'f': 0.14285713910714296}},\n",
       "  {'rouge-1': {'r': 0.6470588235294118,\n",
       "    'p': 0.43137254901960786,\n",
       "    'f': 0.5176470540235294},\n",
       "   'rouge-2': {'r': 0.47368421052631576,\n",
       "    'p': 0.2903225806451613,\n",
       "    'f': 0.35999999528800003},\n",
       "   'rouge-l': {'r': 0.6176470588235294,\n",
       "    'p': 0.4117647058823529,\n",
       "    'f': 0.49411764225882365}},\n",
       "  {'rouge-1': {'r': 0.17647058823529413,\n",
       "    'p': 0.35294117647058826,\n",
       "    'f': 0.23529411320261448},\n",
       "   'rouge-2': {'r': 0.10526315789473684,\n",
       "    'p': 0.21052631578947367,\n",
       "    'f': 0.14035087274853814},\n",
       "   'rouge-l': {'r': 0.17647058823529413,\n",
       "    'p': 0.35294117647058826,\n",
       "    'f': 0.23529411320261448}},\n",
       "  {'rouge-1': {'r': 0.5, 'p': 0.3953488372093023, 'f': 0.4415584366267499},\n",
       "   'rouge-2': {'r': 0.2631578947368421,\n",
       "    'p': 0.20408163265306123,\n",
       "    'f': 0.22988505255119576},\n",
       "   'rouge-l': {'r': 0.47058823529411764,\n",
       "    'p': 0.37209302325581395,\n",
       "    'f': 0.41558441065272395}},\n",
       "  {'rouge-1': {'r': 0.6470588235294118,\n",
       "    'p': 0.3793103448275862,\n",
       "    'f': 0.47826086490548214},\n",
       "   'rouge-2': {'r': 0.47368421052631576, 'p': 0.25, 'f': 0.3272727227504133},\n",
       "   'rouge-l': {'r': 0.6176470588235294,\n",
       "    'p': 0.3620689655172414,\n",
       "    'f': 0.4565217344706995}},\n",
       "  {'rouge-1': {'r': 0.38235294117647056, 'p': 0.25, 'f': 0.3023255766143862},\n",
       "   'rouge-2': {'r': 0.18421052631578946,\n",
       "    'p': 0.1111111111111111,\n",
       "    'f': 0.1386138566924813},\n",
       "   'rouge-l': {'r': 0.35294117647058826,\n",
       "    'p': 0.23076923076923078,\n",
       "    'f': 0.2790697626608979}},\n",
       "  {'rouge-1': {'r': 0.6428571428571429,\n",
       "    'p': 0.6666666666666666,\n",
       "    'f': 0.6545454495471076},\n",
       "   'rouge-2': {'r': 0.559322033898305,\n",
       "    'p': 0.5238095238095238,\n",
       "    'f': 0.540983601562752},\n",
       "   'rouge-l': {'r': 0.6071428571428571,\n",
       "    'p': 0.6296296296296297,\n",
       "    'f': 0.6181818131834711}},\n",
       "  {'rouge-1': {'r': 0.6428571428571429,\n",
       "    'p': 0.5454545454545454,\n",
       "    'f': 0.5901639294598228},\n",
       "   'rouge-2': {'r': 0.5423728813559322, 'p': 0.4, 'f': 0.4604316497903836},\n",
       "   'rouge-l': {'r': 0.6071428571428571,\n",
       "    'p': 0.5151515151515151,\n",
       "    'f': 0.557377044213921}},\n",
       "  {'rouge-1': {'r': 0.6607142857142857,\n",
       "    'p': 0.6491228070175439,\n",
       "    'f': 0.6548672516375598},\n",
       "   'rouge-2': {'r': 0.5932203389830508,\n",
       "    'p': 0.5384615384615384,\n",
       "    'f': 0.5645161240439647},\n",
       "   'rouge-l': {'r': 0.6428571428571429,\n",
       "    'p': 0.631578947368421,\n",
       "    'f': 0.637168136593312}},\n",
       "  {'rouge-1': {'r': 0.2857142857142857,\n",
       "    'p': 0.32653061224489793,\n",
       "    'f': 0.304761899784127},\n",
       "   'rouge-2': {'r': 0.13559322033898305,\n",
       "    'p': 0.12903225806451613,\n",
       "    'f': 0.13223139996175146},\n",
       "   'rouge-l': {'r': 0.2857142857142857,\n",
       "    'p': 0.32653061224489793,\n",
       "    'f': 0.304761899784127}},\n",
       "  {'rouge-1': {'r': 0.125, 'p': 0.3181818181818182, 'f': 0.17948717543721246},\n",
       "   'rouge-2': {'r': 0.01694915254237288,\n",
       "    'p': 0.038461538461538464,\n",
       "    'f': 0.023529407518339866},\n",
       "   'rouge-l': {'r': 0.10714285714285714,\n",
       "    'p': 0.2727272727272727,\n",
       "    'f': 0.1538461497961868}}])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rouge_scores), rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07540c10-9c07-4107-ab6e-1b2b27595c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_dict = {}\n",
    "for i in range(len(rouge_scores)):\n",
    "    rouge_scores_dict[i]= rouge_scores[i]\n",
    "\n",
    "type(rouge_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a1bd968-1f6f-435b-a9e8-414167aee4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.061224489795918366,\n",
       "  'p': 0.21428571428571427,\n",
       "  'f': 0.09523809178130524},\n",
       " 'rouge-2': {'r': 0.017543859649122806,\n",
       "  'p': 0.07142857142857142,\n",
       "  'f': 0.028169010918468917},\n",
       " 'rouge-l': {'r': 0.061224489795918366,\n",
       "  'p': 0.21428571428571427,\n",
       "  'f': 0.09523809178130524}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84355a9f-d96c-471a-8d9d-12b4618a2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rouge_scores_df = pd.DataFrame.from_dict(rouge_scores_dict, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc9c29e9-f879-4dfd-935c-117304124839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge-1</th>\n",
       "      <th>rouge-2</th>\n",
       "      <th>rouge-l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'r': 0.061224489795918366, 'p': 0.21428571428...</td>\n",
       "      <td>{'r': 0.017543859649122806, 'p': 0.07142857142...</td>\n",
       "      <td>{'r': 0.061224489795918366, 'p': 0.21428571428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'r': 0.08163265306122448, 'p': 0.266666666666...</td>\n",
       "      <td>{'r': 0.03508771929824561, 'p': 0.133333333333...</td>\n",
       "      <td>{'r': 0.061224489795918366, 'p': 0.2, 'f': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'r': 0.32653061224489793, 'p': 0.571428571428...</td>\n",
       "      <td>{'r': 0.14035087719298245, 'p': 0.242424242424...</td>\n",
       "      <td>{'r': 0.30612244897959184, 'p': 0.535714285714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'r': 0.16326530612244897, 'p': 0.32, 'f': 0.2...</td>\n",
       "      <td>{'r': 0.03508771929824561, 'p': 0.071428571428...</td>\n",
       "      <td>{'r': 0.14285714285714285, 'p': 0.28, 'f': 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'r': 0.2653061224489796, 'p': 0.0970149253731...</td>\n",
       "      <td>{'r': 0.07017543859649122, 'p': 0.022346368715...</td>\n",
       "      <td>{'r': 0.22448979591836735, 'p': 0.082089552238...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>{'r': 0.6428571428571429, 'p': 0.6666666666666...</td>\n",
       "      <td>{'r': 0.559322033898305, 'p': 0.52380952380952...</td>\n",
       "      <td>{'r': 0.6071428571428571, 'p': 0.6296296296296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>{'r': 0.6428571428571429, 'p': 0.5454545454545...</td>\n",
       "      <td>{'r': 0.5423728813559322, 'p': 0.4, 'f': 0.460...</td>\n",
       "      <td>{'r': 0.6071428571428571, 'p': 0.5151515151515...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>{'r': 0.6607142857142857, 'p': 0.6491228070175...</td>\n",
       "      <td>{'r': 0.5932203389830508, 'p': 0.5384615384615...</td>\n",
       "      <td>{'r': 0.6428571428571429, 'p': 0.6315789473684...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>{'r': 0.2857142857142857, 'p': 0.3265306122448...</td>\n",
       "      <td>{'r': 0.13559322033898305, 'p': 0.129032258064...</td>\n",
       "      <td>{'r': 0.2857142857142857, 'p': 0.3265306122448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>{'r': 0.125, 'p': 0.3181818181818182, 'f': 0.1...</td>\n",
       "      <td>{'r': 0.01694915254237288, 'p': 0.038461538461...</td>\n",
       "      <td>{'r': 0.10714285714285714, 'p': 0.272727272727...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               rouge-1  \\\n",
       "0    {'r': 0.061224489795918366, 'p': 0.21428571428...   \n",
       "1    {'r': 0.08163265306122448, 'p': 0.266666666666...   \n",
       "2    {'r': 0.32653061224489793, 'p': 0.571428571428...   \n",
       "3    {'r': 0.16326530612244897, 'p': 0.32, 'f': 0.2...   \n",
       "4    {'r': 0.2653061224489796, 'p': 0.0970149253731...   \n",
       "..                                                 ...   \n",
       "295  {'r': 0.6428571428571429, 'p': 0.6666666666666...   \n",
       "296  {'r': 0.6428571428571429, 'p': 0.5454545454545...   \n",
       "297  {'r': 0.6607142857142857, 'p': 0.6491228070175...   \n",
       "298  {'r': 0.2857142857142857, 'p': 0.3265306122448...   \n",
       "299  {'r': 0.125, 'p': 0.3181818181818182, 'f': 0.1...   \n",
       "\n",
       "                                               rouge-2  \\\n",
       "0    {'r': 0.017543859649122806, 'p': 0.07142857142...   \n",
       "1    {'r': 0.03508771929824561, 'p': 0.133333333333...   \n",
       "2    {'r': 0.14035087719298245, 'p': 0.242424242424...   \n",
       "3    {'r': 0.03508771929824561, 'p': 0.071428571428...   \n",
       "4    {'r': 0.07017543859649122, 'p': 0.022346368715...   \n",
       "..                                                 ...   \n",
       "295  {'r': 0.559322033898305, 'p': 0.52380952380952...   \n",
       "296  {'r': 0.5423728813559322, 'p': 0.4, 'f': 0.460...   \n",
       "297  {'r': 0.5932203389830508, 'p': 0.5384615384615...   \n",
       "298  {'r': 0.13559322033898305, 'p': 0.129032258064...   \n",
       "299  {'r': 0.01694915254237288, 'p': 0.038461538461...   \n",
       "\n",
       "                                               rouge-l  \n",
       "0    {'r': 0.061224489795918366, 'p': 0.21428571428...  \n",
       "1    {'r': 0.061224489795918366, 'p': 0.2, 'f': 0.0...  \n",
       "2    {'r': 0.30612244897959184, 'p': 0.535714285714...  \n",
       "3    {'r': 0.14285714285714285, 'p': 0.28, 'f': 0.1...  \n",
       "4    {'r': 0.22448979591836735, 'p': 0.082089552238...  \n",
       "..                                                 ...  \n",
       "295  {'r': 0.6071428571428571, 'p': 0.6296296296296...  \n",
       "296  {'r': 0.6071428571428571, 'p': 0.5151515151515...  \n",
       "297  {'r': 0.6428571428571429, 'p': 0.6315789473684...  \n",
       "298  {'r': 0.2857142857142857, 'p': 0.3265306122448...  \n",
       "299  {'r': 0.10714285714285714, 'p': 0.272727272727...  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80c56a73-d3d4-43ea-a9d4-e827f201c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge2 = rouge_scores_df['rouge-2'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "339499fe-96f8-4010-bd5f-0bda0da6ce27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r</th>\n",
       "      <th>p</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.198613</td>\n",
       "      <td>0.258626</td>\n",
       "      <td>0.206965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.164964</td>\n",
       "      <td>0.174559</td>\n",
       "      <td>0.153550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.074632</td>\n",
       "      <td>0.138093</td>\n",
       "      <td>0.097809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.159075</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.178671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.260995</td>\n",
       "      <td>0.335366</td>\n",
       "      <td>0.286181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                r           p           f\n",
       "count  300.000000  300.000000  300.000000\n",
       "mean     0.198613    0.258626    0.206965\n",
       "std      0.164964    0.174559    0.153550\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      0.074632    0.138093    0.097809\n",
       "50%      0.159075    0.230769    0.178671\n",
       "75%      0.260995    0.335366    0.286181\n",
       "max      0.805556    1.000000    0.739130"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91a999e2-c600-4462-9e34-19d15eaeee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19861258009846788, 0.25862646516998544, 0.20696501983423318)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_r = rouge2['r'].mean()\n",
    "average_p = rouge2['p'].mean()\n",
    "average_f = rouge2['f'].mean()\n",
    "average_r, average_p, average_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00c15c-b902-4221-931f-6d3e8ae00055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
